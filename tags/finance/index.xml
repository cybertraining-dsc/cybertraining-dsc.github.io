<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cybertraining – finance</title><link>/tags/finance/</link><description>Recent content in finance on Cybertraining</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Mon, 15 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/finance/index.xml" rel="self" type="application/rss+xml"/><item><title>Report: Big Data in E-Commerce</title><link>/report/fa20-523-329/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-329/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-329/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-329/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-329/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-329/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Wanru Li, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-329/">fa20-523-329&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-329/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The topic of my report is big data in e-commerce. E-commerce is a big part of todays society. During the shopping online, the recommend commodities are fitter and fitter for my liking and willingness to buy. This is the merit of big data. Big data use my purchase history and browsing history to analyze my liking and recommend the goods for me.&lt;/p>
&lt;p>In our everyday lives, e-commerce is now a critical element. It redefines trading practices worldwide. Over the years the growth of eCommerce has been profound. As we move forward, we are learning how to grow eCommerce in this era and how to run an eCommerce company. The dominant mode of trading was brick-and-mortar until the rise of eCommerce. Brick and mortar firms have at least one physical location in supermarket stores. Goods must be bought and sold by active and physical contacts between the buyer and the seller. Brick and mortar trading continues, but eCommerce is increasingly replacing. Many brick and mortar retailers in an evolutionary manner turn themselves into eCommerce stores. This includes an online presence and bringing key company practices online.&lt;/p>
&lt;p>The eCommerce market is increasingly developing as the Internet becomes more available in various areas of the world. Traditional retail companies are migrating to the eCommerce space. Expand their appeal to customers and remain competitive as well. It is clear that the eCommerce shops provide great experiences for customers. An improved flexibility of the Internet, faster purchases, plenty of goods and customized deals, the lack of physical presence restrictions and interaction make it attractive for customers to buy online. E-commerce has many advantages for you, whether you are a company or a customer. Learn all about powering eCommerce sites such as Shopify and Big Commerce online shops.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-search-and-analysis">4. Search and Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-references">6. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> e-commerce, big data, data analysis&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>E-commerce is already changed by big data a lot. As we can see in the lecture slides, the retail store was closed rapidly in the past three years. It can be seen that e-commerce has begun to take shape and has been accepted by customers. E-commerce can make shopping more convenient for customers, and enable companies to better discover current trends and customers' favorite categories for better development.&lt;/p>
&lt;p>For customers, they can find the item they want easier than find it in a retail store. Maybe the customer doesn&amp;rsquo;t know what he wants, doesn&amp;rsquo;t know his brand, only knows its style, but that&amp;rsquo;s enough to search for the item on e-commerce. There are also some e-commerce services that offer photo search, which makes shopping easier. Shopping in e-commerce usually keeps a record of the purchase. In this way, you don&amp;rsquo;t have to go to a retail store to buy some products repeatedly. Instead, you can directly find the products in the record and place orders, which saves a lot of time.&lt;/p>
&lt;p>For companies, there are more changes. They can analyze customers preferences and purchasing power based on their browsing data, shopping cart data, and purchasing data. Large enough to predict the future business trend, small enough to better see the customer&amp;rsquo;s evaluation of the product.&lt;/p>
&lt;p>E-commerce companies have access to a lot of data, which makes it easy for them to analyze product trends and customer preferences. As talent says, &amp;ldquo;Retail websites track the number of clicks per page, the average number of products people add to their shopping carts before checking out, and the average length of time between a homepage visit and a purchase. If customers are signed up for a rewards or subscription program, companies can analyze demographic, age, style, size, and socioeconomic information. Predictive analytics can help companies develop new strategies to prevent shopping cart abandonment, lessen time to purchase, and cater to budding trends. Likewise, e-commerce companies use this data to accurately predict inventory needs with changes in seasonality or the economy&amp;rdquo; &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. There is an example of the Lenovo, to enhance the customer experience and stand out from the competition, Lenovo needs to understand customers' needs, preferences, and purchasing behaviors. By collecting data sets from various touchpoints, Lenovo USES real-time predictive analytics to improve customer experience and increase revenue per retail segment by 11 percent &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Meeting customer needs is not just an immediate problem. E-commerce depends on having the right inventory in the future. Big data can help the company to be prepared for the emerging trend, in the slow or potential prosperity and development of the year, or around major activity plan marketing activities. E-commerce companies will compile large data sets. By evaluating the data of a few years ago, electronics retailers can plan accordingly inventory, inventory to predict peak, simplify the overall business operations, and predict demand. E-commerce sites, for example, can do it in the shopping rush hour in social media significantly depreciate sales promotion, to eliminate redundant products. In order to optimize pricing decisions, e-commerce sites can also provide a special discount. Through big data analysis and machine learning, learn when to offer discounts, how long they should last, and what discount prices are offered more accurately (para 8).&lt;/p>
&lt;p>E-commerce is bound to dominate the retail market in the future because it can help retailers better analyze and predict future trends, which retailers cannot resist. At the same time, e-commerce provides better ways for customers to shop. With better analysis, retail companies will be able to provide better service to customers, so e-commerce will be more and more accepted and popular in the future.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>As Artur Olechowski wrote, &amp;ldquo;According to the IDC, the digital universe of data will grow by 61% to reach a smashing 175 zettabytes worldwide by 2025. There’s no denying that a large chunk of the digital world belongs to e-commerce, which takes advantage of customer social media activity, web browser history, geolocation, and data about abandoned online shopping carts. Most e-commerce businesses are able to collect and process data at scale today. Many of them leverage data analytics to understand their customers’ purchasing behaviors, follow the changing market trends, gain insights that allow them to become more proactive, deliver more personalized experiences to customers. The global Big Data in the e-commerce industry is expected to grow at a CAGR of 13.27% between 2019 and 2028. But what exactly is Big Data? And how can e-commerce businesses capture this powerful technology trend to their advantage? In this article, we take a closer look at the key trends in the usage of Big Data technologies by e-commerce companies and offer you some tips to help you get started in this game-changing field&amp;rdquo; &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The most common and widely used application of big data is in e-commerce. Nowadays, the application of big data in e-commerce is relatively mature. As Artur Olechowski wrote, &amp;ldquo;As businesses scale up, they also collect an increasing amount of data. They need to get interested in data and its processing; this is just inevitable. That’s why a data-driven e-commerce company should regularly measure and improve upon: shopper analysis, customer service personalization, customer experience, the security of online payment processing, targeted advertising&amp;rdquo; &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>There are also some disadvantages of the big data, or to say more need to do after getting the data. Artur Olechowski wrote, &amp;ldquo;Understand the problem of security — Big Data tools gather a lot of data about every single customer who visits your site. This is a lot of sensitive information. If your security is compromised, you could lose your reputation. That’s why before adopting the data technology, make sure to hire a cybersecurity expert to keep all of your data private and secure&amp;rdquo;&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> . Security is always a big problem with big data. This is one of the components will be analyzed in my report. He also wrote, &amp;ldquo;Lack of analytics will become a bigger problem — Big Data is all about gathering information, but to make use of it, your system should also be able to process it. High-quality Big Data solutions can do that and then visualize insights in a simple manner. That’s how you can make this valuable information useful to everyone, from managers to customer service reps&amp;rdquo; &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The analysis is also an important part of the big data. Only collecting data cannot help e-commerce anything. Security and analytics will be talked about in my report.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>QUARTERLY RETAIL E-COMMERCE SALES 2 nd QUARTER 2020:&lt;a href="https://www.census.gov/retail/mrts/www/data/pdf/ec_current.pdf">https://www.census.gov/retail/mrts/www/data/pdf/ec_current.pdf&lt;/a>&lt;/p>
&lt;p>For the dataset, the source website provided in the project requirements will be used, if there needs more information, data and information on the web will be searched for. As a result of recent COVID-19 incidents, many organizations work in a small capacity or have entirely ceased activities. The Census Bureau has tracked and analyzed the response and data quality in this manner.&lt;/p>
&lt;p>Monthly Retail Trade from Census will be analyzed. The Census Bureau of the Department of Commerce today reported that the forecast of U.S. retail e-commerce revenue for the second quarter of 2020 adjusted for seasonal fluctuations, but not for price adjustments, was $211.5 billion, a rise of 31.8 per cent (plus or minus 1.2 per cent) from the first quarter of 2020. Total retail revenues were projected at $1,311.0 billion for the second quarter of 2020, a decline of 3.9 percent (plus or minus 0.4 percent) from the first quarter of 2020. The e-commerce forecast for the second quarter of 2020 increased (para1).&lt;/p>
&lt;p>Retail e-commerce sales are estimated from the same sample used for the Monthly Retail Trade Survey (MRTS) to estimate preliminary and final U.S. retail sales. Advance U.S. online transactions are calculated from a subsample of the MRTS survey that is not of appropriate magnitude to calculate improvements in retail e-commerce transactions.&lt;/p>
&lt;p>A stratified basic random sampling procedure is used to pick approximately 10,800 retailers, except food services, whose transactions are then weighted and benchmarked to reflect the entire universe of over two million retailers. The MRTS sample is focused on probability and represents all employer firms engaged in retail activities as described in the North American Industry Classification System (NAICS). Coverage covers all vendors whether or not they are active in e-commerce. Internet travel agents, financial brokers and distributors, and ticket sales companies are not listed as retail and are not included with either the gross retail or retail e‐commerce sales figures. Non employees are reflected in the projections by benchmarking of previous annual survey estimates that contain non employer revenue based on administrative data. E-commerce revenues are included in the gross monthly sales figures.&lt;/p>
&lt;p>The MRTS sample is revised on a continuous basis to account for new retail employees (including those selling over the Internet), company deaths and other shifts in the retail business environment. Firms are asked to report e-commerce revenue on a monthly basis separately. For each month of the year, data for non-responsive sampling units shall be calculated from reacting sampling units falling under the same class of sector and sales size segment or on the basis of the company&amp;rsquo;s historical results. Responding firms account for approximately 67% of the e-commerce sales estimate and approximately 72% of the U.S. retail sales estimate for any quarter.&lt;/p>
&lt;p>Estimates are obtained by summing the weighted sales (either reported or charged) for each month of the quarter. The monthly figures are benchmarked against previous annual survey estimates. Quartal projections are determined summing up the monthly benchmarked figures. The forecast for the last quarter is a provisional forecast. The calculation is also open to revision. Data consumers who make their own projections using data from this study can only apply to the Census Bureau as the source of input data.&lt;/p>
&lt;p>This article publishes forecasts optimized for seasonal variation and variations in holiday and trade days, but not for adjustments in rates. As feedback for the X‐13ARIMA‐SEATS programme, we have used the updated figures of quarterly figures of e-commerce revenue for the fourth quarter 1999 up to the present quarter. For revenue, we estimated the quarterly adjusted figures for each year with an additional modified monthly revenue forecast. Seasonal estimate adjustment is an approximation based on current and previous experiences.&lt;/p>
&lt;p>The estimates containing sample errors and non-sample errors in this article are based on a survey.&lt;/p>
&lt;p>The difference between the prediction and the results of the full population listing under the same sample conditions is the sampling error. This mistake happens when a national poll only tests a sub-set of the total population. Estimated sampling variance measurements are standard errors and variance coefficients, as stated in Table 2 of this article.&lt;/p>
&lt;h2 id="4-search-and-analysis">4. Search and Analysis&lt;/h2>
&lt;p>This year the pandemic accelerated growth in ecommerce in the US, with online revenues projected to hit just 2022. The top 10 ecommerce retailers will strengthen their hold on the retail market with our Q3 American retail prediction.&lt;/p>
&lt;p>This year, revenues of US eCommerce are projected to hit $794.50 billion, up 32.4% annually. This is even more than the 18.0% predicted in our Q2, since customers are now ignoring shops and opting to buy online in the wake of the pandemic.&lt;/p>
&lt;p>In &amp;ldquo;US Ecommerce Growth Jumps to More than 30%, Accelerating Online Shopping Shift by Nearly 2 Years&amp;rdquo;, it says, &amp;ldquo;&amp;lsquo;We’ve seen ecommerce accelerate in ways that didn’t seem possible last spring, given the extent of the economic crisis,&amp;rsquo; said Andrew Lipsman, eMarketer principal analyst at Insider Intelligence. &amp;lsquo;While much of the shift has been led by essential categories like grocery, there has been surprising strength in discretionary categories like consumer electronics and home furnishings that benefited from pandemic-driven lifestyle needs&amp;rsquo;&amp;rdquo; &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This year, ecommerce revenues are projected to hit 14.4 percent and 19.2 percent of all US retail spending by 2024. Without purchases of petrol and cars, ecommerce penetration leaps to 20.6% (classes sold almost entirely offline).&lt;/p>
&lt;p>In &amp;ldquo;US Ecommerce Growth Jumps to More than 30%, Accelerating Online Shopping Shift by Nearly 2 Years&amp;rdquo;, it writes, &amp;ldquo;&amp;lsquo;There will be some lasting impacts from the pandemic that will fundamentally change how people shop,&amp;rsquo; said Cindy Liu, eMarketer senior forecasting analyst at Insider Intelligence. &amp;lsquo;For one, many stores, particularly department stores, may close permanently. Secondly, we believe consumer shopping behaviors will permanently change. Many consumers have either shopped online for the first time or shopped in new categories (i.e., groceries). Both the increase in new users and frequency of purchasing will have a lasting impact on retail&amp;rsquo;&amp;rdquo; &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Online commerce will be so high that this year, at $4,711 trillion, this will more than compensate for the 3,2 percent fall in brick and mortar expenses. Complete US retail revenue will also remain relatively flat.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>More users are benefiting from the majority of online resources, including eCommerce, as internet penetration and connectivity improve. In everyday life e-commerce has become a mainstream, with fundamental advantages. The e-commerce market is projected to reverse double digit growth in net accounts from anywhere around the world. However, e-commerce can expand enormously as digital payment options are growing in these areas. About 22% of the world&amp;rsquo;s stores are now online. By 2021, e-Commerce online revenues are projected to hit $5 trillion.&lt;/p>
&lt;p>Fru Kerik says, &amp;ldquo;The most popular eCommerce businesses worldwide are Amazon, Alibaba, eBay, and Walmart. These eCommerce giants have redefined the retail industry irrespective of location. They accumulate revenues that exceed billions of dollars yearly. As internet accessibility increases, these estimates would skyrocket. At the time of this writing, Amazon is present in 58 countries, Alibaba in 15, Walmart in 27, MercadoLibre in 18&amp;rdquo; &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>E-Commerce firms have also contributed to the rise of e-Commerce through methodological findings. E-Commerce firms follow customer expectations and make important discoveries about the business-to-consumer model. These insights are then incorporated in market models, ensuring smooth future revenue increase globally.&lt;/p>
&lt;h2 id="6-references">6. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&amp;ldquo;7 Ways Big Data Will Change E-Commerce Business In 2019 | Talend&amp;rdquo;. Talend Real-Time Open Source Data Integration Software, 2020. &lt;a href="https://www.talend.com/resources/big-data-ecommerce/">https://www.talend.com/resources/big-data-ecommerce/&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Olechowski, Artur. &amp;ldquo;Big Data in E-Commerce: Key Trends and Tips for Beginners: Codete Blog.&amp;rdquo; Codete Blog - We Share Knowledge for IT Professionals, CODETE, 8 Sept 2020. &lt;a href="https://codete.com/blog/big-data-in-ecommerce/">https://codete.com/blog/big-data-in-ecommerce/&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>&amp;ldquo;US Ecommerce Growth Jumps to More than 30%, Accelerating Online Shopping Shift by Nearly 2 Years.&amp;rdquo; EMarketer, 12 Oct. 2020. &lt;a href="https://www.emarketer.com/content/us-ecommerce-growth-jumps-more-than-30-accelerating-online-shopping-shift-by-nearly-2-years">https://www.emarketer.com/content/us-ecommerce-growth-jumps-more-than-30-accelerating-online-shopping-shift-by-nearly-2-years&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Kerick, Fru. &amp;ldquo;The Growth of Ecommerce.&amp;rdquo; Medium, The Startup, 1 Jan. 2020. &lt;a href="https://medium.com/swlh/the-growth-of-ecommerce-2220cf2851f3#:~:text=What%20Exactly%20is%20E%2Dcommerce,%2C%20apparel%2C%20software%2C%20furniture">https://medium.com/swlh/the-growth-of-ecommerce-2220cf2851f3#:~:text=What%20Exactly%20is%20E%2Dcommerce,%2C%20apparel%2C%20software%2C%20furniture&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data Analytics in Brazilian E-Commerce</title><link>/report/fa20-523-330/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-330/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-330/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-330/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Oluwatobi Bolarin, &lt;a href="mailto:bolarint@iu.edu">bolarint@iu.edu&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-330/">fa20-523-330&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-330/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As the world begins to utilize online service and stores at greater capacity it becomes a greater priority to increase the efficiency of the various processes that are required for online stores to work effectively. By analyzing the data the comes from online purchases, a better understanding can be formed about what is needed and where as well as the quantity. This data should also allow for us to better predict what orders will be needed at future times so shortages can be avoided.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-analysis">5. Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> economics, Brazil, money, shipping, Amazon, density&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Brazil has the largest E-commerce market in Latin America. &amp;ldquo;It has been estimated that the country accounts for over one third of the region&amp;rsquo;s ecommerce market&amp;rdquo;&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. However, the growth of the potential e-commerce giant has problems that could potentially stunt its long-term growth. The concentration of this effort is to determine the areas the money is spent; however, this topic should expand to other countries and nations to determine locations to stores specific products. After amazon, this can be applied to many online stores or stores that have so form of digital commerce. Due to the nature of spending, this method could also be used to determine what regions of a country are in need of what items when the residents are in poverty. The amount of money that is spent can also be a strong indicator about what trends are possible and what type of goods people are willing to spend their income on. Part of the reason why this is important is due is combating shortages. The COVID pandemic showed that working global supply chains to their maximum at all times isn’t just a bad idea, it is detrimental to the citizens of their respective countries and by extension the world. Supply chains that are constantly working to their maximum capacity lives no room for emergency supplies that could be utilized in live saving applications. Examples would be masks, hospital beds, protective equipment, etc.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>The study of economics is understanding the best way how to deal with scarcity. Earning a bachelor’s degree in economics didn’t feel satisfying enough due to my love for technology. Determine how resources are allocated is one of the most important decision that can be made in our modern world regardless of our region, race, occupations, or economical class.&lt;/p>
&lt;p>Working in restaurants for 7 years working through undergrad the opportunity to learn numerous social and management skills. However, there would always be never ending problems present thing themselves through the day. One of the most painful ones would consistently be the fact that we would run out of things. Surplus were hardly considered acceptable and when there was a shortage the workers would pay the price. This would be understandable in places that just opened for less than a year, however in restaurants that had been in business for a while it was inexcusable. People can reason that, it was a busy day so we didn’t know that it would sell out, that a product went bad that wasn’t counted before, or someone made an ordering error. However, none of those excuses are valid in a place that has been running for a while because they should have an estimate about how much they should be growing and how much extra product they should by to ensure the fact that they don’t run out.&lt;/p>
&lt;p>This is a similar issue that supply chains around the world had, except, with one problem you would have to deal with numerous angry customers and the other it was a matter of lives. This matter should increase the emphasis that we should have more relax supply chains in our world then over-worked ones. It seems that it is in worlds best interest that we start formatting our data in a more readable way for ever one to see a understand so we can do a better job of managing where we place our resources. Constantly determining what is being done well and what needs to be working on and improved. This analysis will attempt to better illustrate a better picture to determine more practical ways to increase e-commerce growth in Brazil and possible by extension this method can be superimposed on other regions of the world.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>After exploring a vast amount of data available, it was best to choose the following two datasets in order to analyze Amazon sales data in brazil to get an unbiased look at what sells on average.&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://www.kaggle.com/olistbr/brazilian-ecommerce?select=olist_products_dataset.csv">Amazon Sales Data&lt;/a>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>The four datasets that will speficialy be using in this file will be the &amp;ldquo;olist_customers_dataset.csv&amp;rdquo;, &amp;ldquo;olist_order_items_dataset.csv&amp;rdquo;, &amp;ldquo;olist_orders_dataset.csv&amp;rdquo;, and &amp;ldquo;olist_products_dataset.csv&amp;rdquo;. Both of the datasets are needed for this project because to obtain the location data from customers the zip code of the customer is needed for the olist_customers_dataset.csv dataset and the name of the order and specifically the category that it is in is held in the olist_order_items_dataset.csv dataset.
For this project work is done with a dataset of 100,000 that has various amazon orders ranging from basic items to more expensive and complex things. The only limiter is what was being sold on amazon at that point in time. The data set size is only 120.3 MB and markdown will be used to show the findings and the method got to them.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/datasetImage.png" alt="Dataset Image Map">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Database Map&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The data that is used can be derived from the database map using in figure 1. The data in each data set that is necessary for this project would be the olist_customers_dataset.csv for what location the customer got their goods delivered to. This can be done with both the city that they live in and the zip code. Both should be used to see if there is going to be a notable difference between what people in entire cities will order verses specific zip codes. Then the order can be found in olist_orders_dataset.csv by matching the customer ID in the dataset olist_customers_dataset.csv. After the specific order is found we can find the specific item that was bought with the dataset olist_order_items_dataset.csv. The chain completed by using the product ID from the olist_order_items_dataset.csv and matching it with the product ID in the olist_products_dataset.csv.&lt;/p>
&lt;p>The reason why it needs to be done this way is because the information that is necessary to answer the question is too vast if people are to deal with specific items so it would be more important and helpful if we found the category of items that is necessary instead. In the end, this project deals with four different datasets in the same database that helps us connect the location of where the order is needed and being sent to the type of object that is bought.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>To correctly articulate the scope of what the dataset is measuring and the region that it is as well as when the data was collected for the project. It is also needed to come up with a viable solution for any (if there is) missing data that exists in the data set. Looking into ways that that the different data sheets interact with one another to find any patterns or factors that could exist that aren't otherwise easily seen is needed as well.&lt;/p>
&lt;p>To determine the product categories that were sold in each region it is mandator that the relevant data frames were merged together through panda. When that is done it is optional to remove the data that isn’t necessary. The reason it is a necessity to merge the data sets together is because the way that it is currently organized is not helpful to the analysis that is trying to be done. The olist_order_customer_dataset contains only information about the customer ordering the objects. The Customer Dataset has no information on the item that they ordered or what. The only relative data that it has for this analysis is the Zip Code of the consumer, the City of the consumer and the State of the consumer. With all of this information we can make any size analysis base on an area as small as people in the same Zip Code to be people as large as a region by putting together multiple cities together. This concept can be utilized with any area data, so it is helpful here as well. However, the customer_id is needed to link the olist_order_customer_dataset with the olist_orders_dataset.&lt;/p>
&lt;p>Although the olist_orders_dataset doesn’t have any information that directly makes it important for this analysis it does have the order_id that is linked to customer_id. Thus, the Order that the Customer made can be linked back to the customer and from the customer the location that it was ordered to. Then with the order_id, we can link the olist_order_items_dataset. Interesting things can be determined from this information.&lt;/p>
&lt;p>For example, we can determine the average freight_value for any area in our data size regardless of how big. It can also be used compare that to other areas or regions to determine how much each large shipment of items should be ensured by. The same thing can also be done with the price and that can help determine economic status, although without knowing the specific type of item this type of analysis may not be accurate and, by extension, not beneficial. However, if any really meaningful analysis that will come out of this the olist_products_data set is a one of the most important sets of data that we need for this analysis. To that end, the product_id from olist_order_items_dataset needs to be linked with the product_id in the olist_products_dataset. With that the area that the products go to are linked with what the products are.&lt;/p>
&lt;p>By linking the products_id to the olist_products_dataset an analysis can be done with about specific product information and the region that it is going to. The most important for this analysis, however, would be the product_category_name. This is important because the product category that is ordered can now be linked with the region that it is going to. Down to an area as small as a zip code. Ultimately, this could potential help determine how much what should be store and where to best help the customers and increase 2-day completion rates without over burdening the work force any more then necessary. From this large data frame, it can now be determining what type of products are ordered this most in what regions. If this information is utilized correctly and efficiently it can greatly reduce the stress of a supply chain. By increase the supply chain’s efficiency without increasing the load we can create slack that will be able to minimize the stress a supply chain suffers when demand is higher than usual for any reason.&lt;/p>
&lt;h2 id="5-analysis">5. Analysis&lt;/h2>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure1.png" alt="ZIP from Python">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> First set of preliminary data showing zicode density&lt;/p>
&lt;p>For figure 2, this is a histogram of the various zip codes in Brazil. The problem with this chart is the fact that it doesn’t really tell you much. It doesn’t properly illustrate the density each area nor do a good job of showing exactly how many orders were made in each area. All this graph could show is the number of customers that reside in each area, but it doesn’t do that well either. What would have worked better for this would have been to first of all understand the fact that there is a lot of data that should have been broken up from the beginning. The main problem with this graph, as well as many others in the series, is the fact that it is trying to show a lot with far too little. The problem would be &amp;ldquo;solved&amp;rdquo; in a sense if, instead of showing all of the zip codes, only finding the zip codes in a part of Brazil. That would enable people to be able to see multiple different useful graphs. One graph could be made to show the number of customers per region and then a few others could be regional showing the Zip codes, states, and/or cities for each region. This would produce graphs that could inform the reader more about how many customers are living in each area.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure3.png" alt="Heatmap Data">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Shows Heat map of all the data&lt;/p>
&lt;p>Figure 3 is a heatmap showing all of the data. This type of graph really doesn’t make sense with the parameters that it was set with. It would have been more helpful to show, like in figure 2, the relationship between the areas and the amount of people that were in the areas. It would also be good to use it for the visualization of areas and the number of products for each area as well as how much is made revenue in each area. The problem with the current format is the fact that it is trying to incorporate strings as well as various numbers that just make all the data harder to understand.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure2.png" alt="Sales Per City">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Shows Sales per State&lt;/p>
&lt;p>Figure 4 is a histogram that shows the number of sales for each state in Brazil. Although this graph is an improvement in terms of relying relevant information it doesn’t do a good enough job. Because this graph is a histogram and not a bar graph it doesn’t show the number of sales in individual states, but rather shows the sales numbers of groups of states. The reason why that isn’t optimal is because it gives the illusion of a trend by helping viewers assume that the states group together are group for a reason, when the only reason is for the groupings is the number of sales that is in each area not the area that the products were delivered.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure4.png" alt="Products">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Histogram for different products in Brazil&lt;/p>
&lt;p>Figure 5 also has similar problems in terms of visualization. One problem is the fact that it mimics at lot of the problems that figure 4 had, like how things are group. However, the second problem is the worst one; the fact that the labels can’t be read. The reason why this occurs is due to the large amount of data that includes 73 different categories of products as well the fact that the graph is also a histogram. This problem is a slightly more complex on to fix just because of the amount of data, no matter how it would be addressed it would be too large. The most beneficial method would be to show the top 6 products and one more bar of the remainder of the sections and that may be able to better demonstrate the different levels of products in a more impactful way. Couple with this would be a list that shows the amount for all the categories as well as the percent that the categories fill.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-330/raw/main/project/images/figure5.png" alt="SP State Products">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Histogram for products sold In SP State.&lt;/p>
&lt;p>Figure 6 has similar problems to Figure 4 and 5 however, it would show more information if than the others because the scope is much smaller. One of the most glaring problems is the exact same as figure 5, which is having too much on the X-axis. The majority of the problems can be fixed by reducing the scope of that data that we are trying to look at as well as not visualizing big sets and just listing them out in an organized fashion.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>E-Commerce will only continue to grow world around the world, not just in Brazil and the United States. With the constant exponential growth and possibility for expansions being able to identity and eventually predict when and where warehouse should be places as well as where they should be place will help not only in day-to-day functions but in times of duress. The analysis and methods done had good intentions but did not achieve the desired result.&lt;/p>
&lt;p>The problems with the methodology and the analysis are the fact that the scope of the data that each graph was trying to visualize was just far too large and not organized inherently to accommodate the smaller scopes of data that was needed in order to perform a helpful analysis that could bring anything meaningful to observe. This in itself is a lesson about how to handle big data.&lt;/p>
&lt;p>With accurate and precise data analysis it can be said that we can improve the logistical capability of all shipping company and companies that ship things. With improve logistics most items can be move more efficiently and consistently to customers with items that are order frequently. With the extra efficiency, if allocated correctly, could be used as a preemptive measure to allow for emergency supplies to always be able to be distributed on the supply chain regardless of the circumstance.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Navarro, José Gabriel. &amp;ldquo;Topic: E-Commerce in Brazil.&amp;rdquo; Statista, &amp;lt;www.statista.com/topics/4697/e-commerce-in-brazil/&amp;gt;. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Olist. &amp;ldquo;Brazilian E-Commerce Public Dataset by Olist.&amp;rdquo; Kaggle, 29 Nov. 2018, &amp;lt;www.kaggle.com/olistbr/brazilian-ecommerce&amp;gt;. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analyzing the Relationship of Cryptocurrencies with Foriegn Exchange Rates and Global Stock Market Indices</title><link>/report/fa20-523-332/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-332/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-332/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-332/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Krish Hemant Mhatre&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-332/">fa20-523-332&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-332/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="mailto:krishmhatre@icloud.com">krishmhatre@icloud.com&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The project involves analyzing the relationships of various cryptocurrencies with Foreign Exchange Rates and Stock Market Indices. Apart from analyzing the relationships, the objective of the project is also to estimate the trend of the cryptocurrencies based on Foreign Exchange Rates and Stock Market Indices. We will be using historical data of 6 different cryptocurrencies, 25 Stock Market Indices and 22 Foreign Exchange Rates for this project. The project will use various machine learning tools for analysis. The project also uses a fully connected deep neural network for prediction and estimation. Apart from analysis and prediction of prices of cryptocurrencies, the project also involves building its own database and giving access to the database using a prototype API. The historical data and recent predictions can be accessed through the public API.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-resources">2. Resources&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-analysis">4. Analysis&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-principal-component-analysis">4.1 Principal Component Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-tsne-analysis">4.2 TSNE Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-weighted-features-analysis">4.3 Weighted Features Analysis&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-neural-network">5. Neural Network&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-data-preprocessing">5.1 Data Preprocessing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-model">5.2 Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-training">5.3 Training&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-prediction">5.4 Prediction&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-deployment">6. Deployment&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-daily-update">6.1 Daily Update&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-rest-service">6.2 REST Service&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgement">8. Acknowledgement&lt;/a>&lt;/li>
&lt;li>&lt;a href="#references">References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> cryptocurrency, stocks, foreign exchange rates.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The latest type of investment in the finance world and one of the latest global medium of exchange is Cryptocurrency. The total market capitalizations of all cryptocurrencies added up to $237.1 Billion as of 2019&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, making it one of the fastest growing industries in the world. Cryptocurrency systems do not require a central authority as its state is maintained through distributed consensus&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Therefore, determining the factors affecting the prices of cryptocurrencies becomes extremely difficult. There are several factors affecting the prices of cryptocurrency like transaction cost, reward system, hash rate, coins circulation, forks, popularity of cryptomarket, speculations, stock markets, exchange rates, gold price, interest rate, legalization and restriction&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. This project involves studying and analysing the relationships of various cryptocurrencies with Foreign Exchange Rates and Stock Market Indices. Furthermore, the project also involves predicting the cryptocurrency price based on stock market indices and foreign exchange rates of the previous day. The project also involves development of a public API to access the database of the historical data and the predictions.&lt;/p>
&lt;h2 id="2-resources">2. Resources&lt;/h2>
&lt;p>&lt;strong>Table 2.1:&lt;/strong> Resources&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>No.&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Name&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Version&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Type&lt;/strong>&lt;/th>
&lt;th style="text-align:right">&lt;strong>Notes&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">1.&lt;/td>
&lt;td style="text-align:center">Python&lt;/td>
&lt;td style="text-align:center">3.6.9&lt;/td>
&lt;td style="text-align:center">Programming language&lt;/td>
&lt;td style="text-align:right">Python is a high-level interpreted programming language.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">MongoDB&lt;/td>
&lt;td style="text-align:center">4.4.2&lt;/td>
&lt;td style="text-align:center">Database&lt;/td>
&lt;td style="text-align:right">MongoDB is a NoSQL Database program that uses JSON-like documents.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">Heroku&lt;/td>
&lt;td style="text-align:center">0.1.4&lt;/td>
&lt;td style="text-align:center">Cloud Platform&lt;/td>
&lt;td style="text-align:right">Heroku is a cloud platform used for deploying applications. It uses a Git server to handle application repositories.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">Gunicorn&lt;/td>
&lt;td style="text-align:center">20.0.4&lt;/td>
&lt;td style="text-align:center">Server Gateway Interface&lt;/td>
&lt;td style="text-align:right">Gunicorn is a python web server gateway interface . It is mainly used in the project for running python applications on Heroku.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">Tensorflow&lt;/td>
&lt;td style="text-align:center">2.3.1&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Tensorflow is an open-source machine learning library. It is mainly used in the project for training models and predicting results.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">Keras&lt;/td>
&lt;td style="text-align:center">2.4.3&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Keras is an open-source python library used for interfacing with artificial neural networks. It is an interface for the Tensorflow library.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">7.&lt;/td>
&lt;td style="text-align:center">Scikit-Learn&lt;/td>
&lt;td style="text-align:center">0.22.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Scikit-learn is an open-source machine learning library featuring various algorithms for classification, regression and clustering problems.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">8.&lt;/td>
&lt;td style="text-align:center">Numpy&lt;/td>
&lt;td style="text-align:center">1.16.0&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Numpy is a python library used for handling and performing various operations on large multi-dimensional arrays.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">9.&lt;/td>
&lt;td style="text-align:center">Scipy&lt;/td>
&lt;td style="text-align:center">1.5.4&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Scipy is a python library used for scientific and technical computing. It is not directly used in the project but serves as a dependency for tensorflow.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">10.&lt;/td>
&lt;td style="text-align:center">Pandas&lt;/td>
&lt;td style="text-align:center">1.1.4&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Pandas is a python library used mainly for large scale data manipulation and analysis.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">11.&lt;/td>
&lt;td style="text-align:center">Matplotlib&lt;/td>
&lt;td style="text-align:center">3.2.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Matplotlib is a python library used for graphing and plotting.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">12.&lt;/td>
&lt;td style="text-align:center">Pickle&lt;/td>
&lt;td style="text-align:center">1.0.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Pickle-mixin is a python library used for saving and loading python variables.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">13.&lt;/td>
&lt;td style="text-align:center">Pymongo&lt;/td>
&lt;td style="text-align:center">3.11.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Pymongo is a python library containing tools for working with MongoDB.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">14.&lt;/td>
&lt;td style="text-align:center">Flask&lt;/td>
&lt;td style="text-align:center">1.1.2&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Flask is a micro web framework for python. It is used in the project for creating the API.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">15.&lt;/td>
&lt;td style="text-align:center">Datetime&lt;/td>
&lt;td style="text-align:center">4.3&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Datetime is a python library used for handling dates as date objects.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">16.&lt;/td>
&lt;td style="text-align:center">Pytz&lt;/td>
&lt;td style="text-align:center">2020.4&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Pytz is a python library used for accurate timezone calculations.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">17&lt;/td>
&lt;td style="text-align:center">Yahoo Financials&lt;/td>
&lt;td style="text-align:center">1.6&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">Yahoo Financials is an unofficial python library used for extracting data from Yahoo Finance website by web scraping.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">18&lt;/td>
&lt;td style="text-align:center">Dns Python&lt;/td>
&lt;td style="text-align:center">2.0.0&lt;/td>
&lt;td style="text-align:center">Python Library&lt;/td>
&lt;td style="text-align:right">DNS python is a necessary dependency of Pymongo.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>The project builds its own dataset by extracting the data from Yahoo Finance website using Yahoo Financial python library &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The data includes cryptocurrency prices, stock market indices and foreign exchange rates from September 30 2015 to December 5 2020. The project uses historical data of 6 cryptocurrencies - Bitcoin (BTC), Ethereum (ETH), Dash (DASH), Litecoin (LTC), Monero (XMR) and Ripple (XRP), 25 stock market indices - S&amp;amp;P 500 (USA), Dow 30 (USA), NASDAQ (USA), Russell 2000 (USA), S&amp;amp;P/TSX (Canada), IBOVESPA (Brazil), IPC MEXICO (Mexico), Nikkei 225 (Japan), HANG SENG INDEX (Hong Kong), SSE (China), Shenzhen Component (China), TSEC (Taiwan), KOSPI (South Korea), STI (Singapore), Jakarta Composite Index (Indonesia), FTSE Bursa Malaysia KLCI (Malaysia), S&amp;amp;P/ASX 200 (Australia), S&amp;amp;P/NZX 50 (New Zealand), S&amp;amp;P BSE (India), FTSE 100 (UK), DAX (Germany), CAC 40 (France), ESTX 50 (Europe), EURONEXT 100 (Europe), BEL 20 (Belgium), and 22 foreign exchange rates - Australian Dollar, Euro, New Zealand Dollar, British Pound, Brazilian Real, Canadian Dollar, Chinese Yuan, Hong Kong Dollar, Indian Rupee, Korean Won, Mexican Peso, South African Rand, Singapore Dollar, Danish Krone, Japanese Yen, Malaysian Ringgit, Norwegian Krone, Swedish Krona, Sri Lankan Rupee, Swiss Franc, New Taiwan Dollar, Thai Baht. This data is, then, posted to MongoDB Database. The three databases are created for each of the data types - Cryptocurrency prices, Stock Market Indices and Foreign Exchange Rates. The three databases each contain one collection for every currency, index and rate respectively. These collections have a uniform structure containing 6 columns - &amp;ldquo;id&amp;rdquo;, &amp;ldquo;formatted_date&amp;rdquo;, &amp;ldquo;low&amp;rdquo;, &amp;ldquo;high&amp;rdquo;, &amp;ldquo;open&amp;rdquo; and &amp;ldquo;close&amp;rdquo;. The tickers used to extract data from Yahoo Finance &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> are stated in Figure 3.1.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/tickers.png" alt="Ticker Information">&lt;/p>
&lt;p>&lt;strong>Figure 3.1:&lt;/strong> Ticker Information&lt;/p>
&lt;p>The data is, then, preprocessed to get only one column per date (&amp;ldquo;close&amp;rdquo; price) and to add missing information by replicating previous day&amp;rsquo;s values, which is used to make a large dataset including the prices of all indices and rates for all the dates within the given range. This data is saved in a different MongoDB Database and collection, both, called nn_data. This collection has 54 columns containing closing prices for each cryptocurrency price, stock market index and foreign exchange rate and the date. The rows represent different dates.&lt;/p>
&lt;p>One additional database is also created - Predictions - which contain the predictions of cryptocurrency prices for each day and it&amp;rsquo;s true value. The collection has 13 columns containing a date column and 2 columns for each cryptocurrency (prediction value and true value). New rows are inserted everyday for all collections except the &amp;ldquo;nn_data&amp;rdquo; collection. Figure 3.2 represents the overview of the MongoDB Cluster. Figure 3.3 shows the structure of the nn_data collection.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/mongodb.png" alt="MongoDB Cluster Overview">&lt;/p>
&lt;p>&lt;strong>Figure 3.2:&lt;/strong> MongoDB Cluster Overview&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/sample_data.png" alt="Short Structure of NN_data Collection">&lt;/p>
&lt;p>&lt;strong>Figure 3.3:&lt;/strong> Short Structure of NN_data Collection&lt;/p>
&lt;h2 id="4-analysis">4. Analysis&lt;/h2>
&lt;h3 id="41-principal-component-analysis">4.1 Principal Component Analysis&lt;/h3>
&lt;p>Principal Component Analysis uses Singular Value Decomposition (SVD) for dimensionality reduction, exploratory data analysis and making predictive models. PCA helps understand a linear relationship in the data&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. In this project, PCA is used for the preliminary analysis to find a pattern between the target and the features. Here we have tried to make some observations by performing PCA on various cryptocurrencies with stocks and forex data. In this analysis, we reduced the dimension of the dataset to 3D, represented in Figure 4.1. The first and second dimension is on x-axis and y-axis respectively whereas the third dimension is used in the color. On observing the scatter plots in Figure 4.1, we can clearly see the patterns formed by various relationships. Therefore, it can be stated that the target and features are related in some way based on the principal component analysis.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/pca.png" alt="Principal Component Analysis">&lt;/p>
&lt;p>&lt;strong>Figure 4.1:&lt;/strong> Principal Component Analysis&lt;/p>
&lt;h3 id="42-tsne-analysis">4.2 TSNE Analysis&lt;/h3>
&lt;p>T-Distributed Stochastic Neighbour Embedding is mainly used for non-linear dimensionality reduction. TSNE uses local relationships between points to create a low-dimensional mapping. TSNE uses Gaussian distribution to create a probability distribution. In this project, TSNE is used to analyze non-linear relationships between cryptocurrencies and the features (stock indices and forex rates), which were not visible in the principal component analysis. It can be observed in Figure 4.2, that there are visible patterns in the data i.e. same colored data points are in some pattern, proving a non linear relationship. The t-SNE plots in Figure 4.2 are not like the typical t-SNE plots i.e. they do not have any clusters. This might be because of the size of the dataset.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/tsne.png" alt="t-SNE Analysis">&lt;/p>
&lt;p>&lt;strong>Figure 4.2:&lt;/strong> t-SNE Analysis&lt;/p>
&lt;h3 id="43-weighted-features-analysis">4.3 Weighted Features Analysis&lt;/h3>
&lt;p>Layers of neural networks have weights assigned to each feature column. These weights are updated continuously while training. Analyzing the weights of the model which is trained for this project, can give us a picture of the important features. To perform such an analysis, the top five feature weights are noted for each layer. The number of times a feature is present in the top five of a layer, is also noted. This is represented in Figure 4.3, where we can observe that the New Zealand Dollar and the Canadian Dollar are repeated most number of times in the top five weights of layers.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/TOP_WEIGTHS.png" alt="No. of repetitions in top five weights">&lt;/p>
&lt;p>&lt;strong>Figure 4.3:&lt;/strong> No. of repetitions in top five weights&lt;/p>
&lt;p>The relationships of these two features - New Zealand Dollar and Canadian Dollar with various cryptocurrencies are, then, analyzed in Figure 4.4 and Figure 4.5. It can be observed that Bitcoin has a direct relationship with these rates. Bitcoin can be observed to increase with an increase in NZD to USD rate and an increase in CAD to USD rate. For the rest of the cryptocurrencies, we can observe that they tend to rise when the NZD to USD rate and the CAD to USD rate are stable and tend to fall when the rates move towards either of the extremes.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/nz_vs_crypto.png" alt="Relationship of NZD with Cryptocurrencies">&lt;/p>
&lt;p>&lt;strong>Figure 4.4:&lt;/strong> Relationship of NZD with Cryptocurrencies&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/cad_vs_crypto.png" alt="Relationship of CAD with Cryptocurrencies">&lt;/p>
&lt;p>&lt;strong>Figure 4.5:&lt;/strong> Relationship of CAD with Cryptocurrencies&lt;/p>
&lt;h2 id="5-neural-network">5. Neural Network&lt;/h2>
&lt;h3 id="51-data-preprocessing">5.1 Data Preprocessing&lt;/h3>
&lt;p>The first step to build a neural network for predicting cryptocurrency prices, is to clean the data. In this step, data from the &amp;ldquo;NN_data&amp;rdquo; collection is imported. Two scalers are used to normalize the data, one for feature columns and other for the target columns. For this purpose, &amp;ldquo;StandardScaler&amp;rdquo; from Scikit-learn library is used. These scalers are made to fit with the data and then saved to a file using pickle-mixin, in order to use it later for predictions. These scalers are then used to normalize the data using mean and standard deviation. This normalized data is shuffled and split into a training set and a test set. This procedure is done by using the &amp;ldquo;train_test_split()&amp;rdquo; function from the Scikit-learn library. The data is split into 94:6 ratio for training and testing respectively. The final data is split into four - X_train, X_test, y_train and y_test and is ready for training the neural network model.&lt;/p>
&lt;h3 id="52-model">5.2 Model&lt;/h3>
&lt;p>For the purpose of predicting the prices of cryptocurrency based on previous day’s stock indices and forex rates, the project uses a fully connected neural network. The solution to this problem could have been perceived in different ways like making it a classification problem by predicting rise or fall in price or by making it a regression problem by either predicting the actual price or by predicting the growth. After trying all these ways of solution, it was concluded that predicting the price regression problem was the best option.&lt;/p>
&lt;p>The final model comprises three layers - one input layer, one hidden layer and one output layer. The first layer uses 8 units with an input dimension of (None, 47) and uses Rectified Linear Unit (ReLU) as its activation function, and He Normal as its kernel initializer. The second layer which is a hidden layer uses 2670 hidden units with Rectified Linear Unit (ReLU) Activation function. ReLU is used because of its faster and effective training in regression models. The third layer which is the output layer has 6 units, one each for predicting 6 cryptocurrencies. The output layer uses linear activation function.&lt;/p>
&lt;p>The overview of the final model can be seen in Figure 5.2. The predictions using the un-trained model can be seen in Figure 5.3, where we can observe the initialization of weights.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/model.png" alt="Model Overview">&lt;/p>
&lt;p>&lt;strong>Figure 5.2:&lt;/strong> Model Overview&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/intialize.png" alt="Visualization of Initial Weights">&lt;/p>
&lt;p>&lt;strong>Figure 5.3:&lt;/strong> Visualization of Initial Weights&lt;/p>
&lt;h3 id="53-training">5.3 Training&lt;/h3>
&lt;p>The neural network model is compiled before training. The model is compiled using Adam optimizer with a default learning rate of 0.001. The model uses Mean Squared Error as its loss function in order to reduce the error and give a close approximation of the cryptocurrency prices. Mean squared error is also used as a metric to visualize the performance of the model.&lt;/p>
&lt;p>The model is, then, trained by using X_train and y_train, as mentioned above, for 5000 epochs and by splitting the dataset for validation (20% for validation). The performance of the training of the final model for first 2500 epochs can be observed in Figure 5.4.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/dnn_with_normal_init.png" alt="Final Model Training">&lt;/p>
&lt;p>&lt;strong>Figure 5.4:&lt;/strong> Final Model Training&lt;/p>
&lt;p>This particular model was chosen because of its low validation mean squared error as compared to the performance of other models. Figure 5.5 represents the performance of a similar fully connected model with Random Normal as its initializer instead of He Normal. Figure 5.6 represents the performance of a Convolutional Neural Network. This model was trained with a much lower mean squared error but had a higher validation mean squared error and was therefore dropped.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/dnn_with_random_normal_init.png" alt="Performance of Fully Connected with Random Normal">&lt;/p>
&lt;p>&lt;strong>Figure 5.5:&lt;/strong> Performance of Fully Connected with Random Normal&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/cnn.png" alt="Performance of Convolutional Neural Network">&lt;/p>
&lt;p>&lt;strong>Figure 5.6:&lt;/strong> Performance of Convolutional Neural Network&lt;/p>
&lt;h3 id="54-prediction">5.4 Prediction&lt;/h3>
&lt;p>After training, the model is stored in a .h5 file, which can be used to make predictions. For making predictions, the project preprocesses the data provided which needs to be of the input dimension of the model i.e. of shape (1, 47). Both the scalers which were saved earlier in the preprocessing stage are loaded again using pickle-mixin. The feature scaler is used to transform the new data to normalized data. This normalized data of the given dimension is then used to predict the prices for six cryptocurrencies. Since regression models do not show accuracy directly, it can be measured manually by rounding off the predicted values and the corresponding true values to the decimal place of one or two and then getting the difference between the two and comparing it to a preset threshold. If the values are rounded off to one decimal place and the threshold is set to 0.05 on the normalized predictions, the accuracy of the prediction is approximately 88% and if the values are rounded off to two decimal places, the accuracy is approximately 62%. The predictions of the test data and the corresponding true values for Bitcoin can be observed in Figure 5.7, where similarities can be observed. Prediction for a new date for the prices of all six cryptocurrencies and its true values can be observed in Figure 5.8. Figure 5.9 also displays the actual result of this project as it can be observed that the predictions and the true values have similar trend with a low margin of error.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/prediction.png" alt="Prediction vs. True">&lt;/p>
&lt;p>&lt;strong>Figure 5.7:&lt;/strong> Prediction vs. True&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/pred.png" alt="Prediction vs. True for one day’s test data">&lt;/p>
&lt;p>&lt;strong>Figure 5.8:&lt;/strong> Prediction vs. True for one day’s test data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-332/raw/main/project/images/pred_vs_true.png" alt="Prediction vs. True for all cryptocurrencies">&lt;/p>
&lt;p>&lt;strong>Figure 5.9:&lt;/strong> Prediction vs. True for all cryptocurrencies&lt;/p>
&lt;h2 id="6-deployment">6. Deployment&lt;/h2>
&lt;h3 id="61-daily-update">6.1 Daily Update&lt;/h3>
&lt;p>The database is supposed to be updated daily using a web-app deployed on Heroku. Heroku is a cloud platform used for deploying web-apps of various languages and also uses a Git-server for repositories &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. This daily update web-app is triggered daily at 07.30 AM UTC i.e 2.00 AM EST. The web-app extracts the data for the previous day and updates all the collections. The new data is then preprocessed by using the saved feature normalizer. This normalized data is used to get predictions for the prices of cryptocurrencies for the day that just started. The web-app then gets the true values of the cryptocurrency prices for the previous day and updates the predictions collection using this data for future comparison. The web-app is currently deployed on Heroku and is triggered daily using Heroku Scheduler. The web-app is entirely coded in Python.&lt;/p>
&lt;h3 id="62-rest-service">6.2 REST Service&lt;/h3>
&lt;p>The data from the MongoDB databases can be accessed using a public RESTful API. The API is developed using Flask-Python. The API usage is given below.&lt;/p>
&lt;p>URL - &lt;code>https://crypto-project-api.herokuapp.com/&lt;/code>&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;strong>/get_data/single/market/index/date&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>Type - GET&lt;/em>&lt;/p>
&lt;p>Sample Request -&lt;/p>
&lt;p>&lt;code>https://crypto-project-api.herokuapp.com/get_data/single/crypto/bitcoin/2020-12-05&lt;/code>&lt;/p>
&lt;p>Sample Respose -&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;data&amp;quot;:
[
{
&amp;quot;close&amp;quot;:19154.23046875,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-05&amp;quot;,
&amp;quot;high&amp;quot;:19160.44921875,
&amp;quot;low&amp;quot;:18590.193359375,
&amp;quot;open&amp;quot;:18698.384765625
}
],
&amp;quot;status&amp;quot;:&amp;quot;Success&amp;quot;
}
&lt;/code>&lt;/pre>&lt;hr>
&lt;p>&lt;em>&lt;strong>/get_data/multiple/market/index/start_date/end_date&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>Type - GET&lt;/em>&lt;/p>
&lt;p>Sample Request -&lt;/p>
&lt;p>&lt;code>https://crypto-project-api.herokuapp.com/get_data/multiple/crypto/bitcoin/2020-12-02/2020-12-05&lt;/code>&lt;/p>
&lt;p>Sample Respose -&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;data&amp;quot;:
[
{
&amp;quot;close&amp;quot;:&amp;quot;19201.091796875&amp;quot;,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-02&amp;quot;,
&amp;quot;high&amp;quot;:&amp;quot;19308.330078125&amp;quot;,
&amp;quot;low&amp;quot;:&amp;quot;18347.71875&amp;quot;,
&amp;quot;open&amp;quot;:&amp;quot;18801.744140625&amp;quot;
},
{
&amp;quot;close&amp;quot;:&amp;quot;19371.041015625&amp;quot;,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-03&amp;quot;,
&amp;quot;high&amp;quot;:&amp;quot;19430.89453125&amp;quot;,
&amp;quot;low&amp;quot;:&amp;quot;18937.4296875&amp;quot;,
&amp;quot;open&amp;quot;:&amp;quot;18949.251953125&amp;quot;
},
{
&amp;quot;close&amp;quot;:19154.23046875,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-05&amp;quot;,
&amp;quot;high&amp;quot;:19160.44921875,
&amp;quot;low&amp;quot;:18590.193359375,
&amp;quot;open&amp;quot;:18698.384765625
}
],
&amp;quot;status&amp;quot;:&amp;quot;Success&amp;quot;
}
&lt;/code>&lt;/pre>&lt;hr>
&lt;p>&lt;em>&lt;strong>/get_predictions/date&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>Type - GET&lt;/em>&lt;/p>
&lt;p>Sample Request -&lt;/p>
&lt;p>&lt;code>https://crypto-project-api.herokuapp.com/get_predictions/2020-12-05&lt;/code>&lt;/p>
&lt;p>Sample Respose -&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;data&amp;quot;:
[
{
&amp;quot;bitcoin&amp;quot;:&amp;quot;16204.04&amp;quot;,
&amp;quot;dash&amp;quot;:&amp;quot;24.148237&amp;quot;,
&amp;quot;date&amp;quot;:&amp;quot;2020-12-05&amp;quot;,
&amp;quot;ethereum&amp;quot;:&amp;quot;503.43005&amp;quot;,
&amp;quot;litecoin&amp;quot;:&amp;quot;66.6938&amp;quot;,
&amp;quot;monero&amp;quot;:&amp;quot;120.718414&amp;quot;,
&amp;quot;ripple&amp;quot;:&amp;quot;0.55850273&amp;quot;
}
],
&amp;quot;status&amp;quot;:&amp;quot;Success&amp;quot;
}
&lt;/code>&lt;/pre>&lt;hr>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>After analyzing the historical data of Stock Market Indices, Foreign Exchange Rates and Cryptocurrency Prices, it can be concluded that there does exist a non-linear relationship between the three. It can also be concluded that cryptocurrency prices can be predicted and its trend can be estimated using Stock Indices and Forex Rates. There is still a large scope of improvement in reducing the mean squared error. The project can further improve the neural network model for better predictions. In the end, it is safe to conclude that the indicators of international politics like Stock Market Indices and Forex Exchange Rates are factors affecting the prices of cryptocurrency.&lt;/p>
&lt;h2 id="8-acknowledgement">8. Acknowledgement&lt;/h2>
&lt;p>Krish Hemant Mhatre would like to thank Indiana University and Luddy School of Informatics, Computing and Engineering for providing me with the opportunity to work on this project. He would also like to thank Dr. Geoffrey C. Fox, Dr. Gregor von Laszewski and the Assistant Instructors of ENGR-E-534 Big Data Analytics and Applications for their constant guidance and support.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Szmigiera, M. &amp;ldquo;Cryptocurrency Market Value 2013-2019.&amp;rdquo; Statista, 20 Jan. 2020, &lt;a href="https://www.statista.com/statistics/730876/cryptocurrency-maket-value">https://www.statista.com/statistics/730876/cryptocurrency-maket-value&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Lansky, Jan. &amp;ldquo;Possible State Approaches to Cryptocurrencies.&amp;rdquo; Journal of Systems Integration, University of Finance and Administration in Prague Czech Republic, &lt;a href="http://www.si-journal.org/index.php/JSI/article/view/335">http://www.si-journal.org/index.php/JSI/article/view/335&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Sovbetov, Yhlas. &amp;ldquo;Factors Influencing Cryptocurrency Prices: Evidence from Bitcoin, Ethereum, Dash, Litcoin, and Monero.&amp;rdquo; Journal of Economics and Financial Analysis, London School of Commerce, 26 Feb. 2018, &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3125347">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3125347&lt;/a>. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Sanders, Connor. &amp;ldquo;YahooFinancials.&amp;rdquo; PyPI, JECSand, 22 Oct. 2017, &lt;a href="https://pypi.org/project/yahoofinancials/">https://pypi.org/project/yahoofinancials/&lt;/a>. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Jaadi, Zakaria. &amp;ldquo;A Step-by-Step Explanation of Principal Component Analysis.&amp;rdquo; Built In, &lt;a href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis">https://builtin.com/data-science/step-step-explanation-principal-component-analysis&lt;/a>. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>&amp;ldquo;What Is Heroku.&amp;rdquo; Heroku, &lt;a href="https://www.heroku.com/what">https://www.heroku.com/what&lt;/a>. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data Application in E-commerce</title><link>/report/fa20-523-339/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-339/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Tao Liu, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/">fa20-523-339&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As a result of the last twenty year&amp;rsquo;s Internet development globally, the E-commerce industry is getting stronger and stronger. While customers enjoyed their convenient online purchase environment, E-commerce sees the potential for the data and information customers left during their online shopping process. One fundamental usage for this information is to perform a Recommendation Strategy to give customers potential products they would also like to purchase. This report will build a User-Based Collaborative Filtering strategy to provide customer recommendation products based on the database of previous customer purchase records. This report will start with an overview of the background and explain the dataset it chose &lt;em>Amazon Review Data&lt;/em>. After that, each step for the code and step made in a corresponding file &lt;em>Big_tata_Application_in_E_commense.ipynb&lt;/em> will be illustrated, and the User-Based Collaborative Filtering strategy will be presented step by step.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background">2. Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-data-preprocessing-and-cleaning">4. Data Preprocessing and cleaning&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-recommendation-rate-and-similarity-calculation">5. Recommendation Rate and Similarity Calculation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-accuracy">6. Accuracy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-benchmark">7. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-conclusion">8. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-acknowledgements">9. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#10-references">10. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> recommendation strategy,user-based, collaborative filtering, business, big data, E-commerce, customer behavior&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big data have many applications in scientific research and business, from those in the hardware perspective like Higgs Discovery to the software perspective like E-commence. However, with the passage of time, online shopping and E-commerce have become one of the most popular events for citizens' lives and society. Millions of goods are now sold online the customers all over the world. With the 5G technology&amp;rsquo;s implementation, this trend is now inevitable. These activities will create millions of data about customer&amp;rsquo;s behaviors like how they value products, how they purchase or sell the products, and how they review the goods purchased would have a tremendous contribution for corporations to analyze. These data can not only help convince the current strategies of E-commerce on the right track, but a potential way to see which step E-commerce can make up for attracting more customers to buy the goods. At the same time, these data can also be implemented as a way for recommendation strategies for E-commerce. It will help customers find the next products they like in a short period by implementing machine learning technology on Big Data. The corporations also can enjoy the increase of sales and attractions by recommendation strategies. A better recommendation strategy on E-commerce is now the new trend for massive data scientists and researchers’ target. Therefore, this field is now one of the most popular research areas in the data science fields.&lt;/p>
&lt;p>In this final project, An User-Based Collaborative Filtering Strategy will be implemented to get a taste of the recommendation strategy based on Customer&amp;rsquo;s Gift card purchase records and the item they also viewed and bought. The algorithm&amp;rsquo;s logic is the following: A used record indicates that customer who bought product A and also view/buy products B&amp;amp;C. When a new customer comes and shows his interest in B&amp;amp;C, product A would be recommended. This logic is addressed based on the daily-experience of customer behaviors on their E-commerce experience.&lt;/p>
&lt;h2 id="2-background">2. Background&lt;/h2>
&lt;p>Recommendation Strategy is a quite popular research area in recent years with a strong real-world influence. It is largely used in E-commerce platforms like Taobao, Amazon, etc. Therefore, It is obvious that there are plenty of recommendation strategies have been done. Though every E-commerce recommendation algorithm may be different from each other, the most popular technique for recommendation systems is called Collaborative Filtering. It is a technique that can filter out items that a user might like based on reactions by similar users. During this technique, the memory-based method is considered in this report since it uses a dataset to calculate the prediction using statistical techniques. This strategy will be able to fulfill in the local environment with a proper dataset. There are two kinds of memory-based methods available in the market: &lt;em>User-Based Collaborative Filtering&lt;/em>, &lt;em>Item-Based Collaborative Filtering&lt;/em> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This project will only focus on the User-Based Collaborative Filtering Strategy since Item-Based Collaborative Filtering requires a customer review rate for evaluation. The customer review rate for evaluation is not in the dataset available in the market. Therefore, Item-Based Collaborative Filtering unlikely to be implemented, and the User-Based Collaborative Filtering Strategy is considered.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>The dataset for this study is called &lt;em>Amazon Review Data&lt;/em> &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Particularly, since the dataset is now reached billions of amount, the subcategory gift card will be used as an example since the overall customer record is 1547 and the amount of data retrieved is currently in the right amount of training. This fact can help to perform User-Based Collaborative Filtering in a controlled timeline.&lt;/p>
&lt;h2 id="4-data-preprocessing-and-cleaning">4. Data Preprocessing and cleaning&lt;/h2>
&lt;p>The first step will be data collection and data cleaning. The raw data-set is imported directly from data-set contributors' online storage &lt;em>meta_Gift_Cards.json.gz&lt;/em> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> to Google Colab notebook. The raw database retrieved directly from the website will be shown in &lt;strong>Table 1&lt;/strong>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Attribute&lt;/th>
&lt;th style="text-align:center">Description&lt;/th>
&lt;th style="text-align:center">Example&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">category&lt;/td>
&lt;td style="text-align:center">The category of the record&lt;/td>
&lt;td style="text-align:center">[&amp;quot;Gift Cards&amp;quot;, &amp;ldquo;Gift Cards&amp;rdquo;]\&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">tech1&lt;/td>
&lt;td style="text-align:center">tech relate to it&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">description&lt;/td>
&lt;td style="text-align:center">The description of the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Gift card for the purchase of goods&amp;hellip;&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">fit&lt;/td>
&lt;td style="text-align:center">fit for its record&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">title&lt;/td>
&lt;td style="text-align:center">title for the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Serendipity 3 $100.00 Gift Card&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;strong>also_buy&lt;/strong>&lt;/td>
&lt;td style="text-align:center">the product also bought&lt;/td>
&lt;td style="text-align:center">[&amp;quot;B005ESMEBQ&amp;quot;]\&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">image&lt;/td>
&lt;td style="text-align:center">image of the gift card&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">tech2&lt;/td>
&lt;td style="text-align:center">tech relate to it&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">brand&lt;/td>
&lt;td style="text-align:center">brand of the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Amazon&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">feature&lt;/td>
&lt;td style="text-align:center">feature of the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Amazon.com Gift cards never expire&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">rank&lt;/td>
&lt;td style="text-align:center">rank of the product&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;strong>also_view&lt;/strong>&lt;/td>
&lt;td style="text-align:center">the product also view&lt;/td>
&lt;td style="text-align:center">[&amp;quot;BT00DC6QU4&amp;quot;]\&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">details&lt;/td>
&lt;td style="text-align:center">detail for the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;3.4 x 2.1 inches ; 1.44 ounces&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">main_cat&lt;/td>
&lt;td style="text-align:center">main category of the product&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;Grocery&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">similar_item&lt;/td>
&lt;td style="text-align:center">similar_item of the product&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">date&lt;/td>
&lt;td style="text-align:center">date of the product assigned&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">price&lt;/td>
&lt;td style="text-align:center">price of the product&lt;/td>
&lt;td style="text-align:center">&amp;quot;&amp;quot;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;strong>asin&lt;/strong>&lt;/td>
&lt;td style="text-align:center">product asin code&lt;/td>
&lt;td style="text-align:center">&amp;ldquo;B001BKEWF2&amp;rdquo;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 1:&lt;/strong> The description for the dataset&lt;/p>
&lt;p>Since the attributes &lt;em>category&lt;/em>, &lt;em>main_cat&lt;/em> are the same for the whole dataset, they will not be valid training labels. The attributes &lt;em>tech1&lt;/em>, &lt;em>fit&lt;/em>, &lt;em>tech2&lt;/em>, &lt;em>rank&lt;/em>, &lt;em>similar_item&lt;/em>, &lt;em>date&lt;/em>, &lt;em>price&lt;/em> have no/ extremely less filled in. That made them also invalid for being training labels. The attributes &lt;em>image&lt;/em>, &lt;em>description&lt;/em> and &lt;em>feature&lt;/em> is unique per item and hard to find the similarity in numeric purpose and then hard to be used as labels. Therefore, only attributes &lt;strong>also_buy&lt;/strong>, &lt;strong>also_view&lt;/strong>, &lt;strong>asin&lt;/strong> are trained as attributes and labels in this algorithm. &lt;strong>Figure 1&lt;/strong> is a shortcut for the raw database.&lt;/p>
&lt;pre>&lt;code>THE RAW DATABASE
The size of DATABASE : 1547
0
0 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
2 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
3 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
4 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
... ...
1542 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1543 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1544 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1545 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
1546 {&amp;quot;category&amp;quot;: [&amp;quot;Gift Cards&amp;quot;, &amp;quot;Gift Cards&amp;quot;], &amp;quot;te...
[1547 rows x 1 columns]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 1:&lt;/strong> The raw database&lt;/p>
&lt;p>For the training purpose, all asins that appeared in the dataset, either from &lt;em>also_buy &amp;amp; also_view&lt;/em> list or * asin*, have to be reformatted from alphabet character to numeric character. For example, the original label for a particular item may be called **B001BKEWF2**. It will now be reformatted to a numeric number as 0. In that case, it can be a better fit-in the next step training method and easy to track. This step will be essential since it will help the also_view and also_buy dataset to be reformatted and make sure they are reformed in the track without overlapping each other. Therefore, a reformat_asin function is called for reformatting all the asins in the dataset and is performed as a dictionary. A shortcut for the *Asin Dictionary* is shown in **Figure 2**.&lt;/p>
&lt;pre>&lt;code>The 4561 Lines of Reformatted ASIN reference dictionary as following.
{'B001BKEWF2': 0, 'B001GXRQW0': 1, 'B001H53QE4': 2, 'B001H53QEO': 3, 'B001KMWN2K': 4, 'B001M1UVQO': 5,
'B001M1UVZA': 6, 'B001M5GKHE': 7, 'B002BSHDJK': 8, 'B002DN7XS4': 9, 'B002H9RN0C': 10, 'B002MS7BPA': 11,
'B002NZXF9S': 12, 'B002O018DM': 13, 'B002O0536U': 14, 'B002OOBESC': 15, 'B002PY04EG': 16, 'B002QFXC7U': 17,
'B002QTM0Y2': 18, 'B002QTPZUI': 19, 'B002SC9DRO': 20, 'B002UKLD7M': 21, 'B002VFYGC0': 22, 'B002VG4AR0': 23,
'B002VG4BRO': 24, 'B002W8YL6W': 25, 'B002XNLC04': 26, 'B002XNOVDE': 27, 'B002YEWXZ0': 28, 'B002YEWXMI': 29,
'B003755QI6': 30, 'B003CMYYGY': 31, 'B003NALDC8': 32, 'B003XNIBTS': 33, 'B003ZYIKDM': 34, 'B00414Y7Y6': 35,
'B0046IIHMK': 36, 'B004BVCHDC': 37, 'B004CG61UQ': 38, 'B004CZRZKW': 39, 'B004D01QJ2': 40, 'B004KNWWPE': 41,
'B004KNWWP4': 42, 'B004KNWWR2': 43, 'B004KNWWRC': 44, 'B004KNWWT0': 45, 'B004KNWWRW': 46, 'B004KNWWQ8': 47,
'B004KNWWNG': 48, 'B004KNWWPO': 49, 'B004KNWWXQ': 50, 'B004KNWWUE': 51, 'B004KNWWYU': 52, 'B004KNWWWC': 53,
'B004KNWX3A': 54, 'B004KNWX1W': 55, 'B004KNWWZE': 56, 'B004KNWWSQ': 57, 'B004KNWX4Y': 58, 'B004KNWX12': 59,
'B004KNWX3U': 60, 'B004KNWX62': 61, 'B004KNWX2Q': 62, 'B004KNWX6C': 63...}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 2:&lt;/strong> The ASIN dictionary&lt;/p>
&lt;p>Then the data contained in the each record&amp;rsquo;s attributes: &lt;strong>also_view&lt;/strong> &amp;amp; &lt;strong>also_buy&lt;/strong> will be reformated as &lt;strong>Figure 3&lt;/strong> and &lt;strong>Figure 4&lt;/strong>. &lt;strong>Figure 3&lt;/strong> is about the also_view item in reformatted numeric numbers based on each item customer purchased. &lt;strong>Figure 4&lt;/strong> is about the also_buy item in reformatted numeric numbers based on each item customer purchased.&lt;/p>
&lt;pre>&lt;code>also_view List: The first 10 lines
Item 0 : []
Item 1 : [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,
2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]
Item 2 : [2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 922, 2036, 283,
2037, 2038, 2001, 2000, 2013, 2039, 2040, 2007, 2041, 2042, 2009, 1233, 2043,
2014, 234, 2044, 2012, 2005, 2045, 2046, 2002, 2047, 378, 2048, 1382, 2008,
2004, 2011, 2049, 2050, 2051, 2052, 2003, 2053, 2054, 2018, 2055, 2056]
Item 3 : []
Item 4 : []
Item 5 : []
Item 6 : []
Item 7 : []
Item 8 : []
Item 9 : []
Item 10 : [2057, 2058, 2059]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 3:&lt;/strong> The also_view list&lt;/p>
&lt;pre>&lt;code>also_buy List: The first 20 lines
Item 0 : []
Item 1 : []
Item 2 : [2026, 2028, 2027, 2049, 1382, 2037, 2012, 2023]
Item 3 : []
Item 4 : []
Item 5 : []
Item 6 : []
Item 7 : []
Item 8 : [4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233,
4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243,
4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252]
Item 9 : []
Item 10 : []
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 4:&lt;/strong> The also_buy list&lt;/p>
&lt;p>While the also_buy list and also_view list is addressed. It is also important to know how many times a particular item appeared in other items' also view list and also buy list. These dictionaries will help to calculate the recommendation rate later. &lt;strong>Figure 5&lt;/strong> and &lt;strong>Figure 6&lt;/strong> is an example for how many times item 2000 appeared in other item&amp;rsquo;s also_view and also_buy lists.&lt;/p>
&lt;pre>&lt;code>also_view dictionary: use Item 2000 as an example
Item 2000 : [1, 2, 11, 12, 51, 60, 63, 65, 66, 67, 85, 86, 90, 94, 99, 100, 101, 103, 107, 108, 113, 116, 123, 126, 127, 129, 130, 141, 142, 143, 145, 146, 147, 148, 194, 199, 200, 204, 217, 221, 225, 229, 230, 231, 232, 233, 234, 235, 251, 253, 254, 260, 264, 268, 269, 270, 271, 280, 284, 285, 286, 287, 288, 294, 295, 296, 298, 299, 305, 306, 307, 308, 309, 313, 319, 327, 328, 338, 339, 344, 346, 348, 355, 356, 360, 371, 372, 377, 380, 389, 394, 406, 407, 410, 415, 440, 456, 469, 480, 490, 494, 495, 496, 502, 505, 509, 511, 512, 514, 517, 519, 520, 527, 530, 548, 591, 595, 600, 608, 609, 621, 631, 633, 670, 671, 672, 673, 675, 681, 689, 691, 695, 697, 707, 708, 709, 719, 783, 792, 793, 796, 797, 801, 803, 804, 807, 810, 816, 817, 818, 819, 836, 840, 842, 856, 892, 902, 913, 914, 917, 921, 955, 968, 972, 974, 975, 979, 981, 990, 991, 997, 998, 999, 1000, 1001, 1003, 1005, 1006, 1007, 1010, 1011, 1014, 1015, 1017, 1018, 1023, 1024, 1026, 1027, 1028, 1031, 1032, 1035, 1037, 1038, 1039, 1040, 1042, 1043, 1050, 1069, 1070, 1084, 1114, 1115, 1116, 1117, 1119, 1143, 1153, 1171, 1175, 1192, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1207, 1208, 1213, 1217, 1218, 1220, 1222, 1233, 1236, 1238, 1242, 1244, 1245, 1246, 1249, 1251, 1258, 1268, 1270, 1280, 1285, 1289, 1290, 1292, 1295, 1315, 1318, 1319, 1324, 1328, 1330, 1333, 1336, 1341, 1345, 1346, 1347, 1348, 1352, 1359, 1361, 1365, 1366, 1373, 1378, 1384, 1389, 1394, 1395, 1396, 1403, 1405, 1406, 1407, 1414, 1415, 1417, 1418, 1419, 1420, 1423, 1424, 1426, 1427, 1430, 1431, 1432, 1433, 1434, 1437, 1443, 1453, 1454, 1455, 1457, 1458, 1462, 1463, 1464, 1467, 1468, 1469, 1470, 1472, 1474, 1475, 1477, 1478, 1480, 1481, 1482, 1486, 1488, 1492, 1496, 1497, 1498, 1499, 1500, 1501, 1504, 1505, 1506, 1508, 1509, 1512, 1513, 1514, 1515, 1523, 1530, 1533, 1537, 1539, 1546]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 5:&lt;/strong> The also_view dictionary&lt;/p>
&lt;pre>&lt;code>also_buy dictionary: use Item 2000 as an example
Item 2000 : [217, 231, 235, 236, 277, 284, 285, 286, 287, 306, 307, 308, 327,
359, 476, 482, 505, 583, 609, 719, 891, 922, 963, 1065, 1328, 1359,
1384, 1399, 1482, 1483, 1490, 1496, 1497, 1499, 1509, 1512, 1540]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 6:&lt;/strong> The also_buy dictionary&lt;/p>
&lt;h2 id="5-recommendation-rate-and-similarity-calculation">5. Recommendation Rate and Similarity Calculation&lt;/h2>
&lt;p>While all the dictionaries and attributes-label relationship are prepared in Part4, the recommendation rate calculation is addressed in this part. There are two types of similarity methods in this algorithm: &lt;strong>Cosine Similarity&lt;/strong> and &lt;strong>Euclidean Distance Similarity&lt;/strong> that perform the similarity calculation. Before calculating the similarity, the first step would be phrasing the recommendation rate for each item to another item. The &lt;strong>Figure 8&lt;/strong> is a shortcut for the recommendation rate matrix. It will use the logic in &lt;strong>Figure 7&lt;/strong>.&lt;/p>
&lt;pre>&lt;code>for item in the asin list:
for asin in the also_view dictionary:
if asin is founded in also_view dictionary[item] list:
score for this item increase 2
each item in the also_view_dict[asin]'s score will be also increase 2
for asin in the also_view dictionary:
if asin is founded in also_view dictionary[item] list:
score for this item increase 10
each item in the also_view_dict[asin]'s score will be also increase 10
for other scores which is currently 0, assigned the average value for it
return the overall matrix for the further step
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 7:&lt;/strong> The sudocode for giving the recommendation rate for the likelyhood of the next purchase item based the current purchase&lt;/p>
&lt;pre>&lt;code>Item 0 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 1 : [13.0, 52, 28, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 2, 2, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0 ...]
Item 2 : [29.5, 28, 182, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 4, 2, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5 ...]
Item 3 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 4 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 5 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 6 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 7 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 8 : [14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 290, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5 ...]
Item 9 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ...]
Item 10 : [1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 6, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5 ...]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 8:&lt;/strong> The shortcut for recommenation rate matrix&lt;/p>
&lt;p>The first similarity method implemented is &lt;em>Cosine Similarity&lt;/em> &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. It will use the cosine of the angle between vectors(see &lt;strong>Figure 9&lt;/strong>) to address the similarity between different items. By implementing this method with &lt;em>sklearn.metrics.pairwise&lt;/em> package, it will rephrase the whole recommendation similarity as &lt;strong>table 2&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/cosine-similarity.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> The cosine similarity&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">item&lt;/th>
&lt;th style="text-align:center">0&lt;/th>
&lt;th style="text-align:center">1.&lt;/th>
&lt;th style="text-align:center">2&lt;/th>
&lt;th style="text-align:center">3&lt;/th>
&lt;th style="text-align:center">4&lt;/th>
&lt;th style="text-align:center">5&lt;/th>
&lt;th style="text-align:center">6&lt;/th>
&lt;th style="text-align:center">7.&lt;/th>
&lt;th style="text-align:center">8.&lt;/th>
&lt;th style="text-align:center">&amp;hellip;1547&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">0&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">&amp;hellip;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">1&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.928569&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.873242&lt;/td>
&lt;td style="text-align:center">&amp;hellip;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">2&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.928569&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">0.&lt;/td>
&lt;td style="text-align:center">&amp;hellip;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>table 2:&lt;/strong> The shortcut for using consine similarity to address the recommendation result&lt;/p>
&lt;p>The second similarity method implemented is &lt;em>Euclidean Distance Similarity&lt;/em>&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. It will use Euclidean Distance to calculate the distance between each items as a way to calculate similarity (see &lt;strong>Figure 10&lt;/strong>). By implementing this method with &lt;em>scipy.spatial.distance_matrix&lt;/em> package, it will rephrase the whole recommendation similarity. &lt;strong>Figure 11&lt;/strong> is an example with the item 1.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/euclidean.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 10&lt;/strong> The Euclidean Distance calculation&lt;/p>
&lt;pre>&lt;code>Item 1 : [1005.70671669 0. 1370.09142031 1005.70671669 1005.70671669
1005.70671669 1005.70671669 1005.70671669 710.89169358 1005.70671669
905.23339532 862.88933242 971.0745337 1005.70671669 1005.70671669
1005.70671669 1005.70671669 1005.70671669 1005.70671669 1005.70671669...]
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Figure 11:&lt;/strong> The Euclidean Distance similarity example of item 1&lt;/p>
&lt;h2 id="6-accuracy">6. Accuracy&lt;/h2>
&lt;p>The accuracy for the consine_similarity and euclidean distance similarity with the number of items already purchased is shown as &lt;strong>Figure 12&lt;/strong>. Here the blue line represented the cosine similarity, and the red line represented the euclidean distance similarity. As presented, the more item joined as purchased, the less likely both similarity algorithms accurately locate the next item the customer may want to purchase next. However, overall the consine_similarity performed better accuracy compared to Euclidean Distance similarity. In &lt;strong>Figure 13&lt;/strong>, both accurate number and both wrong number is addressed. Both wrong numbers changed dramatically after more already-purchased items joined. This fact convinces the prior statement: this algorithm works better when the given object is &lt;em>1&lt;/em> but can&amp;rsquo;t handle many purchased item.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/CosVSEuc.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 12:&lt;/strong> The Cosine similarity and Euclidean Distance Accuracy Comparison&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/bothrightandwrong.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 13:&lt;/strong> The bothright and bothwrong accuracy comparison&lt;/p>
&lt;h2 id="7-benchmark">7. Benchmark&lt;/h2>
&lt;p>The Benchmark for each step for the project is stated in &lt;strong>Figure 14&lt;/strong> The overall Time spent is affordable. The accuracy calculation part(57s) and the Euclidean Distance algorithm implementation(74s) have taken the majority of time for the running. The Accuracy time consumed would be considered proper since it will randomly assign one to ten items and perform recommendation items based on it. The time spent is necessary and should be considered normal. The Euclidean Distance algorithm would be considered making sense since it is trying to perform the difference in two 1547X1547 matrixs.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Name&lt;/th>
&lt;th style="text-align:center">Status&lt;/th>
&lt;th style="text-align:center">Time&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">Data Online Downloading Process&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">1.028&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Raw Database&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.529&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Database Reformatting process&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.587&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Recommendation Rate Calculation&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">1.197&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Consine_Similarity&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.835&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Euclidean distance&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">73.895&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Recommendation strategy showcase-Cosine_Similarity&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.003&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Recommendation strategy showcase-Euclidean distance&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.004&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Accuracy&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">57.119&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Showcase-Cosine_Similarity&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.003&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Showcase-Euclidean distance&lt;/td>
&lt;td style="text-align:center">ok&lt;/td>
&lt;td style="text-align:center">0.003&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 14:&lt;/strong> Benchmark&lt;/p>
&lt;p>The time comparison for Cosine Similarity and Euclidean Distance Time Comparison is addressed in &lt;strong>Figure 15&lt;/strong> As stated, the euclidean distance algorithm has taken much more time than cosine similarity. Therefore, the cosine similarity should be considered as efficient in these two similarities.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/images/timecompare.png" alt="image info">&lt;/p>
&lt;p>&lt;strong>Figure 15:&lt;/strong> The Cosine Similarity and Euclidean Distance Time Comparison&lt;/p>
&lt;h2 id="8-conclusion">8. Conclusion&lt;/h2>
&lt;p>This project &lt;em>Big_tata_Application_in_E_commense.ipynb&lt;/em> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> is attempted to get a taste of the recommendation strategy based on &lt;em>User-Based Collaborative Filtering&lt;/em>. Based on this attemption, the two similarity methods: &lt;strong>Cosine Similarity&lt;/strong> and &lt;strong>Euclidean Distance&lt;/strong> are addressed. After analyzing accuracy and time consumption for each method, Cosine Similarity performed better in both the accuracy and implementation time. Therefore the cosine similarity method is recommended to use in the recommendation algorithm strategies.
This project should be aware of Limitations. Since the rating attribute is missing in the dataset, the recommendation rate was assigned by the author. Therefore, in real-world implementation, both methods' accuracy can be expected to be higher than in this project. Besides, the cross-section recommendation strategies are not implemented. This project is only focused on the gift card section recommendations. With the multiple aspects of goods customer purchases addressed, both methods' accuracy can also be expected to be higher.&lt;/p>
&lt;h2 id="9-acknowledgements">9. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="10-references">10. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Build a Recommendation Engine With Collaborative Filtering. Ajitsaria, A. 2020
&lt;a href="https://realpython.com/build-recommendation-engine-collaborative-filtering/">https://realpython.com/build-recommendation-engine-collaborative-filtering/&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Justifying recommendations using distantly-labeled reviews and fined-grained aspects. Jianmo Ni, Jiacheng Li, Julian McAuley. Empirical Methods in Natural Language Processing (EMNLP), 2019 &lt;a href="http://jmcauley.ucsd.edu/data/amazon/">http://jmcauley.ucsd.edu/data/amazon/&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>meta_Gift_Cards.json.gz &lt;a href="http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Gift_Cards.json.gz">http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Gift_Cards.json.gz&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Recommendation Systems : User-based Collaborative Filtering using N Nearest Neighbors. Ashay Pathak. 2019
&lt;a href="https://medium.com/sfu-cspmp/recommendation-systems-user-based-collaborative-filtering-using-n-nearest-neighbors-bf7361dc24e0">https://medium.com/sfu-cspmp/recommendation-systems-user-based-collaborative-filtering-using-n-nearest-neighbors-bf7361dc24e0&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Similarity and Distance Metrics for Data Science and Machine Learning. Gonzalo Ferreiro Volpi. 2019
&lt;a href="https://medium.com/dataseries/similarity-and-distance-metrics-for-data-science-and-machine-learning-e5121b3956f8">https://medium.com/dataseries/similarity-and-distance-metrics-for-data-science-and-machine-learning-e5121b3956f8&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Big_tata_Application_in_E_commense.ipynb &lt;a href="https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/Big_tata_Application_in_E_commense.ipynb">https://github.com/cybertraining-dsc/fa20-523-339/raw/main/project/Big_tata_Application_in_E_commense.ipynb&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Predicting Hotel Reservation Cancellation Rates</title><link>/report/fa20-523-323/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-323/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Anthony Tugman, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/">fa20-523-323&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>As a result of the Covid-19 pandemic all segments of the travel industry face financial struggle. The lodging segment, in particular, has had the financial records scrutinized revealing a glaring problem. Since the beginning of 2019, the lodging segment has seen reservation cancellation rates near 40%. At the directive of business and marketing experts, hotels have previously attempted to solve the problem through an increased focus on reservation retention, flexible booking policies, and targeted marketing. These attempts did not produce results, and continue to leave rooms un-rented which is detrimental to the bottom line. This document will explain the creation and testing of a novel process to combat the rising cancellation rate. By analyzing reservation data from a nationwide hotel chain, it is hoped that an algorithm may be developed capable of predicting the likeliness that a traveler is to cancel a reservation. The resulting algorithm will be evaluated for accuracy. If the resulting algorithm has a satisfactory accuracy, it would make clear to the hotel industry that the use of big data is key to solving this problem.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. DataSets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-data-preprocessing">4. Data Preprocessing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-model-creation">5. Model Creation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> travel, finance, hospitality, tourism, data analysis, environment, big data&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big Data is a term that describes the large volume of data that is collected and stored by a business on a day-to-day basis. While the scale of this data is impressive, what is more interesting is what can be done by analyzing the data &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Becoming more commonplace, companies are beginning to use Big Data to gain advantage in competing, innovating, and capturing customers. It is necessary for businesses to collaborate with Data Scientists to expose patterns and other insights that can be gained from inspection of the data. This collaboration is amongst software engineers, hardware engineers, and Data Scientists who develop powerful machine learning algorithms to efficiently analyze the data. Businesses have multiple benefits to gain from effectively utilizing Big Data including cost savings, time reductions, market condition insights, advertising insights, as well as driving innovation and product development &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The lodging industry takes a hit to the bottom line each year as the rate of room reservation cancellations continues to rise. In the instance that a guest cancels a room without adequate notice there are additional expenses the hotel faces to re-market the room as available. If the room is unable to be rented, the hotel loses the revenue &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. At the directive of business and marketing experts, hotels have attempted to solve this problem through an increased focus on reservation retention, flexible booking policies, and targeted marketing campaigns. However, even with these efforts, the reservation cancellation rate continues to rise unchecked reaching upwards of 40% in 2019 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. By analyzing the Big Data the lodging industry collects on its customers, it would be possible to form a model capable of predicting whether a customer is likely to cancel a reservation. To develop this model, a large data set was sourced that tracked anonymized reservation information for a chain of hotels over a three-year period. Machine learning techniques will be applied to the data set to form a model capable of producing the aforementioned predictions. If the model proves to be statistically significant, recommendations will be made to the lodging industry as to how the results should be interpreted.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>Room reservation completion rates are an important consideration for revenue management in the hotel industry. Unsurprisingly, there have been previous attempts at creating an algorithm capable of predicting room cancellation rates. However, these attempts seem to have taken a more general approach such as predicting cancellations by particular ranges of dates &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Other attempts make broad claims about the accuracy of the resulting algorithm without reliably proving so through statistical analysis &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Most importantly for the scope of this course, these previous attempts use subsets of the full data set available. This has the potential to lead to unpredictability in the performance of the algorithm.&lt;/p>
&lt;p>To differentiate from and improve on the attempts previously mentioned, the algorithm produced as a result of the current effort will form a model capable of predicting if a reservation with cancel based on the reservation as a whole, rather than a subset of dates. Not predict the cancellation rate over general stipulations, but rather for a specific customer. Additionally, a special focus will be on proving that the created algorithm is statistically significant and can accurately be extrapolated to larger subsets of the data. Finally, for initial training and testing, larger subsets of the data sets will be utilized due to the increased processing power available from Google Colab. It is important to note that the first-mentioned previous work &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> appears to use a proprietary data set while the second-mentioned work &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> appears to utilize the same data set that this study will as well.&lt;/p>
&lt;h2 id="3-datasets">3. DataSets&lt;/h2>
&lt;p>Locating a data set appropriate for this task proved to be challenging. Privacy concerns and the desire to keep internal corporate information confidential adds difficulty in locating the necessary data. Ultimately, the following data repository was selected to train and test the reservation cancellation, prediction model:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://www.kaggle.com/jessemostipak/hotel-booking-demand">Hotel Booking Demand&lt;/a> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>The Hotel Booking Demand data set contains approximately 120,000 unique entries with 32 describing attributes. This data comes from a popular data science website, and previous analysis has occurred. This data set was featured as part of a small contest on the hosting website where the goal was to predict the likelihood of a guest making a reservation based on certain attributes while this study will instead attempt to predict the likelihood that a reservation is canceled. The 32 individual attributes of the data will be evaluated for their weight on the outcome of cancellation. Unlike previously mentioned studies, the attribute describing how the booking was made (in person, online) will be utilized. Researchers believe that the increase in reservations booked through online third-party platforms is contributing to the increase in cancellation &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Considering this attribute may have a significant impact on the overall accuracy of the developed predictive algorithm. Finally, the data set provides information on reservations over the span of a three-year period. During the training and testing phase, an appropriate amount of data will be used from each year to account for trends in cancellations that may have occurred over time.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Attribute&lt;/th>
&lt;th style="text-align:center">Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">hotel&lt;/td>
&lt;td style="text-align:center">hotel type (resort or city)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">is_canceled&lt;/td>
&lt;td style="text-align:center">reservation canceled (true/false)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">lead_time&lt;/td>
&lt;td style="text-align:center">number of days between booking and arrival&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">arrival_date_year&lt;/td>
&lt;td style="text-align:center">year of arrival date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">arrival_date_month&lt;/td>
&lt;td style="text-align:center">month of arrival date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">arrival_date_week_number&lt;/td>
&lt;td style="text-align:center">week number of year for arrival date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">arrival_date_day_of_month&lt;/td>
&lt;td style="text-align:center">day of arrival date&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">stays_in_weekend_nights&lt;/td>
&lt;td style="text-align:center">number of weekend nights (Sat. and Sun.) booked&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">stays_in_week_nights&lt;/td>
&lt;td style="text-align:center">number of week nights (Mon. to Fri.) booked&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">adults&lt;/td>
&lt;td style="text-align:center">number of adults&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">children&lt;/td>
&lt;td style="text-align:center">number of children&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">babies&lt;/td>
&lt;td style="text-align:center">number of babies&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">meal&lt;/td>
&lt;td style="text-align:center">meal booked (multiple categories)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">country&lt;/td>
&lt;td style="text-align:center">country of origin&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">market_segment&lt;/td>
&lt;td style="text-align:center">market segment (multiple categories)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">distribution_channel&lt;/td>
&lt;td style="text-align:center">booking distribution channel (multiple categories)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">is_repeated_guest&lt;/td>
&lt;td style="text-align:center">is repeat guest (true/false)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">previous_cancellations&lt;/td>
&lt;td style="text-align:center">number of times customer has canceled previously&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">previous_bookings_not_canceled&lt;/td>
&lt;td style="text-align:center">number of times customer has completed a reservation&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">reserved_room_type&lt;/td>
&lt;td style="text-align:center">reserved room type&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">assigned_room_type&lt;/td>
&lt;td style="text-align:center">assigned room type&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">booking_changes&lt;/td>
&lt;td style="text-align:center">number of changes made to reservation from booking to arrival&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">deposit_type&lt;/td>
&lt;td style="text-align:center">deposit made (true/false)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">agent&lt;/td>
&lt;td style="text-align:center">ID of the travel agent&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">company&lt;/td>
&lt;td style="text-align:center">ID of the company&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">days_in_waiting_list&lt;/td>
&lt;td style="text-align:center">how many days customer took to confirm reservation&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">customer_type&lt;/td>
&lt;td style="text-align:center">type of customer (multiple categories)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">adr&lt;/td>
&lt;td style="text-align:center">average daily rate&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">required_car_parking_space&lt;/td>
&lt;td style="text-align:center">number of parking spaces required for reservation&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">total_of_special_requests&lt;/td>
&lt;td style="text-align:center">number of special requests made&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">reservation_status&lt;/td>
&lt;td style="text-align:center">status of reservation&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">reservation_status_date&lt;/td>
&lt;td style="text-align:center">date status was last updated&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="4-data-preprocessing">4. Data Preprocessing&lt;/h2>
&lt;p>The raw data set &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> is imported directly from Kaggle into a Google Colab notebook &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Data manipulation is handled using Pandas. Pandas was specifically written for data manipulation and analysis, and will make for a simple process preprocessing the data. The raw data set must be prepared before a model can be developed from it. Before preprocessing the data:&lt;/p>
&lt;ul>
&lt;li>shape: (119390, 32)&lt;/li>
&lt;li>duplicate entries: 31,994&lt;/li>
&lt;/ul>
&lt;p>The features of the data set, the categories other than what is being predicted, have varying levels of importance to the predictive model. By inspection, the following features are removed:&lt;/p>
&lt;ul>
&lt;li>country: in a format unsuitable for predictive model, unable to convert&lt;/li>
&lt;li>agent: the ID number of the booking agent will not affect reservation outcome&lt;/li>
&lt;li>babies: no reservation had babies&lt;/li>
&lt;li>children: no reservation had children&lt;/li>
&lt;li>company: the ID number of the booking company will not affect reservation outcome&lt;/li>
&lt;li>reservation_status_date: intermediate status of reservation is irrelevant&lt;/li>
&lt;/ul>
&lt;p>In addition, duplicates are removed. In the case of the features &amp;lsquo;reserved_room_type&amp;rsquo; and &amp;lsquo;assigned_room_type&amp;rsquo; what is of interest is if the guest was given the requested room type. As the room code system has been anonymized and is proprietary to the brand, it is impossible to make inferences other than this. To simplify the number of features, a Boolean comparison is performed on &amp;lsquo;reserved_room_type&amp;rsquo; and &amp;lsquo;assigned_room_type&amp;rsquo;. &amp;lsquo;reserved_room_type&amp;rsquo; and &amp;lsquo;assigned_room_type&amp;rsquo; are deleted while the Boolean results are converted to integer values then placed in a new feature category, &amp;lsquo;room_correct&amp;rsquo;. As it stands, multiple data entries across various features are strings. In order to build a predictive model, the data entries are converted to integers.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#8f5902;font-style:italic">#Convert to numerical values&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;City Hotel&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;HB&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Online TA&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;TA/TO&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;No Deposit&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#39;Transient&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Check-Out&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;0&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;Resort Hotel&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;January&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;BB&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Ofline TA/TO&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;GDS&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#39;Non Refund&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Transient-Party&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">Canceled&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;], &amp;#39;&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;February&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;SC&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Groups&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Refundable&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Group&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#4e9a06">&amp;#39;No-Show&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;2&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;March&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;FB&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Direct&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Contract&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;3&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;April&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Undefined&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Corporate&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;4&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;May&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Complementary&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;5&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;June&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;Aviation&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;6&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;July&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;7&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;August&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;8&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;September&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;9&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;October&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;10&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;November&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;11&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000">df&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">df&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">replace&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;December&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;12&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>After preprocessing the data:&lt;/p>
&lt;ul>
&lt;li>shape: (84938, 25)&lt;/li>
&lt;li>duplicate entries: 0&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure1.png" alt="Data Snapshot">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Snapshot of Data Set after Preprocessing&lt;/p>
&lt;h2 id="5-model-creation">5. Model Creation&lt;/h2>
&lt;p>To form the predictive model a Random Forest Classifier will be used. With the use of the sklearn package, the Random Forest Classifier is simple to implement. The Random Forest Classifier is based on the concept of a decision tree. A decision tree is a series of yes/no question asked about the data which eventually leads to a predicted class or value &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. To start the creation of the model, the data must first be split into a training and testing set. Typically, this is a ratio that must be adjusted to determine which will result in the higher accuracy. Here are the accuracy outcomes for various ratios:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Train/Test Ratio&lt;/th>
&lt;th style="text-align:center">Accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">80/20&lt;/td>
&lt;td style="text-align:center">77.64%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">70/30&lt;/td>
&lt;td style="text-align:center">77.66%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">60/40&lt;/td>
&lt;td style="text-align:center">79.56%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">50/50&lt;/td>
&lt;td style="text-align:center">77.92%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">40/60&lt;/td>
&lt;td style="text-align:center">75.73%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">30/70&lt;/td>
&lt;td style="text-align:center">74.71%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">20/80&lt;/td>
&lt;td style="text-align:center">73.13%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The train/test ratio of 60/40 had the best initial accuracy of 79.56% so this ratio will be used in the creation of the final model. For the initial test, all remaining features will be used to train the reservation cancellation outcome. To determine the accuracy of the resulting model, the number of predicted cancellations is compared to the number of actual cancellations. As the model stands, the accuracy is at 79.56%.
This model relies on 23 features for the prediction. With this many features it is possible that some features have no effect on the cancellation outcome. It is also possible that some features are so closely related that calculating each individually hinders performance while having little effect on outcome. To evaluate the importance of features, Pearson&amp;rsquo;s correlation coefficent can be used. Correlation coefficients are used in statistics to measure how strong the relationship is between two variables. The calculation formula returns a value between -1 and 1 where a value of 1 indicates a strong positive relationship, -1 indicates a strong negative relationship, and 0 indicates that there is no relationship at all &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Figure 2 shows the correlation between the remaining features.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure2.png" alt="Initial Model Correlation">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Pearson&amp;rsquo;s Correlation Graph of Remaining Features&lt;/p>
&lt;p>In Figure 2 it is straightforward to identify the correlation between the target variable &amp;lsquo;is_canceled&amp;rsquo; and the remaining features. It does not appear than any variable has a strong positive or negative correlation, returning a value close to positive or negative 1. There does however appear to be a dominant correlation between &amp;lsquo;is_canceled&amp;rsquo; and three features: &amp;lsquo;lead_time&amp;rsquo;, &amp;lsquo;adr&amp;rsquo;, and &amp;lsquo;room_correct&amp;rsquo;. The train/test ratio is again 60/40 and the baseline accuracy of the model is 79.56%. The remaining features &amp;lsquo;lead_time&amp;rsquo;, &amp;lsquo;adr&amp;rsquo;, and &amp;lsquo;room_correct&amp;rsquo; are used to develop the new model. Accuracy is again determined by comparing the number of predicted cancellations to the number of actual cancellations. The updated model has an accuracy of 85.18%. Figure 3 shows the correlation between the remaining features. It is important to note that the relationship between the remaining features and target value does not appear to be strong, however there is a correlation nonetheless.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure3.png" alt="Updated Model Correlation">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Pearson&amp;rsquo;s Correlation Graph of Updated Remaining Features&lt;/p>
&lt;p>As a final visualization, Figure 4 shows a comparison between the predicted and actual cancellation instances. The graph reveals an interesting pattern, the model is over predicting early in the data set and under predicting as it proceeds through the data set. Further inspection and manipulation of the Random Forest parameters were unable to eliminate this pattern.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure4.png" alt="Results Predicted vs. Actual">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Model Results Predicted vs. Actual&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>To measure program performance in the Google Colab notebook, Cloudmesh Common &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> was used to create a benchmark. In this instance, performance was measured for overall code execution, data loading, preparation of the data, the creation of model one, and the creation of model two. The most important increase in performance was between the creation of models one and two. With 23 features, model one took 8.161 seconds to train while model 2, with 3 features, took 7.01 seconds to train. By reducing the number of features between the two models there is a 5.62% increase in accuracy and a 14.10% decrease in processing time. Figure 5 provides more insight into the parameters the benchmark tracked and returned. Additionally, the table provides an analysis of computation time:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Train/Test Ratio&lt;/th>
&lt;th style="text-align:center">Accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">80/20&lt;/td>
&lt;td style="text-align:center">77.64%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">70/30&lt;/td>
&lt;td style="text-align:center">77.66%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">60/40&lt;/td>
&lt;td style="text-align:center">79.56%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">50/50&lt;/td>
&lt;td style="text-align:center">77.92%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">40/60&lt;/td>
&lt;td style="text-align:center">75.73%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">30/70&lt;/td>
&lt;td style="text-align:center">74.71%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">20/80&lt;/td>
&lt;td style="text-align:center">73.13%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-323/raw/main/project/images/figure5.png" alt="Benchmark Results">&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Cloudmesh Benchmark Results&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>From the results of the updated predictive model, it is apparent that the lodging industry should invest focus into the Big Data they keep on their customers. As each hotel chain is unique, it would be necessary for each to develop their own predictive model however it has been demonstrated that such a model would be effective in reducing the number of rooms going unoccupied from reservation cancellation. As the model is predicting at 85% accuracy, this is a 35% increase in the amount of reservation cancellations that can be accounted for over the current predictive techniques. To prevent further damage from reservation cancellations the hotel would have theoretically been able to overbook room reservations by 35% or less as they anticipated cancellations.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&amp;ldquo;Big Data - Definition, Importance, Examples &amp;amp; Tools&amp;rdquo;, RDA, 2020. [Online]. Available: &lt;a href="https://www.rd-alliance.org/group/big-data-ig-data-development-ig/wiki/big-data-definition-importance-examples-tools#:~:text=Big%20data%20is%20a%20term,day%2Dto%2Dday%20basis.&amp;amp;text=It's%20what%20organizations%20do%20with,decisions%20and%20strategic%20business%20moves">https://www.rd-alliance.org/group/big-data-ig-data-development-ig/wiki/big-data-definition-importance-examples-tools#:~:text=Big%20data%20is%20a%20term,day%2Dto%2Dday%20basis.&amp;amp;text=It's%20what%20organizations%20do%20with,decisions%20and%20strategic%20business%20moves&lt;/a>. [Accessed: 12- Nov- 2020]. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>&amp;ldquo;Predicting Hotel Booking Cancellations Using Machine Learning - Step by Step Guide with Real Data and Python&amp;rdquo;, Linkedin.com, 2020. [Online]. Available: &lt;a href="https://www.linkedin.com/pulse/u-hotel-booking-cancellations-using-machine-learning-manuel-banza/">https://www.linkedin.com/pulse/u-hotel-booking-cancellations-using-machine-learning-manuel-banza/&lt;/a>. [Accessed: 08- Nov- 2020]. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>&amp;ldquo;(PDF) Predicting Hotel Booking Cancellation to Decrease Uncertainty and Increase Revenue&amp;rdquo;, ResearchGate, 2020. [Online]. Available: &lt;a href="https://www.researchgate.net/publication/310504011_Predicting_Hotel_Booking_Cancellation_to_Decrease_Uncertainty_and_Increase_Revenue">https://www.researchgate.net/publication/310504011_Predicting_Hotel_Booking_Cancellation_to_Decrease_Uncertainty_and_Increase_Revenue&lt;/a>. [Accessed: 08- Nov- 2020. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>&amp;ldquo;Predicting Hotel Cancellations with Machine Learning&amp;rdquo;, Medium, 2020. [Online]. Available: &lt;a href="https://towardsdatascience.com/predicting-hotel-cancellations-with-machine-learning-fa669f93e794">https://towardsdatascience.com/predicting-hotel-cancellations-with-machine-learning-fa669f93e794&lt;/a>. [Accessed: 08- Nov- 2020]. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>&amp;ldquo;Hotel booking demand&amp;rdquo;, Kaggle.com, 2020. [Online]. Available: &lt;a href="https://www.kaggle.com/jessemostipak/hotel-booking-demand">https://www.kaggle.com/jessemostipak/hotel-booking-demand&lt;/a>. [Accessed: 08- Nov- 2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>&amp;ldquo;Global Cancellation Rate of Hotel Reservations Reaches 40% on Average&amp;rdquo;, Hospitality Technology, 2020. [Online]. Available: &lt;a href="https://hospitalitytech.com/global-cancellation-rate-hotel-reservations-reaches-40-average">https://hospitalitytech.com/global-cancellation-rate-hotel-reservations-reaches-40-average&lt;/a>. [Accessed: 08- Nov- 2020]. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-323/blob/main/project/colabnotebook/DataAnalysis.ipynb">https://github.com/cybertraining-dsc/fa20-523-323/blob/main/project/colabnotebook/DataAnalysis.ipynb&lt;/a>. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>&amp;ldquo;An Implementation and Explanation of the Random Forest in Python&amp;rdquo;, Medium, 2020. [Online]. Available: &lt;a href="https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76">https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76&lt;/a>. [Accessed: 12- Nov- 2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>&amp;ldquo;Correlation Coefficient: Simple Definition, Formula, Easy Calculation Steps&amp;rdquo;, Statistics How To, 2020. [Online]. Available: &lt;a href="https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/">https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/&lt;/a>. [Accessed: 12- Nov- 2020]. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a> &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analysis of Future of Buffalo Breeds and Milk Production Growth in India</title><link>/report/fa20-523-326/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-326/project/project/</guid><description>
&lt;h1 id="heading">&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-326/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-326/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Gangaprasad Shahapurkar, fa20-523-326, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-326/blob/main/project/project.md">Edit&lt;/a> &lt;a href="https://github.com/cybertraining-dsc/fa20-523-326/blob/main/project/code/project.ipynb">Python Notebook&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Water buffalo (&lt;strong>Bubalus bubalis&lt;/strong>) is also called &lt;em>Domestic Water Buffalo&lt;/em> or &lt;em>Asian Water Buffalo&lt;/em>. It is large bovid originating in Indian subcontinent, Southeast Asia, and China and today found in other regions of world - Europe, Australia, North America, South America and some African countries. There are two extant types recognized based on morphological and behavioral criteria:&lt;/p>
&lt;ol>
&lt;li>River Buffalo - Mostly found in Indian subcontinent and further west to the Balkans, Egypt, and Italy&lt;/li>
&lt;li>Swamp Buffalo - Found from west of Assam through Southeast Asia to the Yangtze valley of China in the east&lt;/li>
&lt;/ol>
&lt;p>India is the largest milk producer and consumer compared to other countries in the world and stands unique in terms of the largest share of milk being produced coming from buffaloes. The aim of this academic project is to study the livestock census data of buffalo breeds in India and their milk production using Empirical Benchmarking analysis method at state level. Looking at the small sample of data, our analysis indicates that we have been seeing increasing trends in past few years in livestock and milk production but there are considerable opportunities to increase production using combined interventions.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-datasets">3. Choice of Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-software-components">4.1 Software Components&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-data-processing">4.2 Data Processing&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#421-eda">4.2.1 EDA&lt;/a>&lt;/li>
&lt;li>&lt;a href="#422-feature-engineering">4.2.2 Feature Engineering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#43-modelling">4.3 Modelling&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#431-data-preperation">4.3.1 Data Preperation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#432-empirical-benchmarking-model">4.3.2 Empirical Benchmarking Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#433-linear-regression">4.3.3 Linear Regression&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-results">5. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> hid 326, i532, buffalo, milk production, livestock, benchmarking, in-milk yield, agriculture, india, analysis&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Indian agriculture sector has been playing a vital role in overall contribution to Indian economy. Most of the rural community in the nation still make their livelihood on dairy farming or agriculture farming. Dairy farming itself has been on its progressive stage from past few years and it is contributing to almost more than 25% of agriculture Gross Domestic Product (GDP) &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Livestock rearing has been integral part of the nation&amp;rsquo;s rural community and this sector is leveraging the economy in a big way considering the growth seen. It not only provides food, income, employment but also plays major role in life of farmers. It also does other contributions to the overall rural development of the nation. The output of livestock rearing such as milk, egg, meat, and wool provides everyday income to the farmers on daily basis, it provides nutrition to consumers and indirectly it helps in contributing to the overall economy and socio-economic development of the country.&lt;/p>
&lt;p>The world buffalo population is estimated at 185.29 million, spread in some 42 countries, of which 179.75 million (97%) are in Asia (Source: Fao.org/stat 2008). Hence major share of buffalo milk production in world comes from Asia (see Figure 1). India has 105.1 million and they comprise approximately 56.7 percent of the total world buffalo population. During the last 10 years, the world buffalo population increased by approximately 1.49% annually, by 1.53% in India, 1.45% in Asia and 2.67% in the rest of the world. Figure 1 shows worldwide share of milk production from buffalo breed. Figure 2 highlights percentage contribution of Asia including other top 2 contributors to world milk production.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/milk_production_world.png" alt="Milk Production World">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Production of Milk, whole fresh buffalo in World + (Total), Average 2013 - 2018 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/milk_production_by_region.png" alt="Milk Production Share">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Production share of Milk, whole fresh buffalo by region, Average 2013 - 2018 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>Production of milk and meat from buffaloes in Asian countries over the last decades has shown a varying pattern: in countries such as India, Sri Lanka, Pakistan and China. Buffaloes are known to be better at converting poor-quality roughage into milk and meat. They are reported to have 5% higher digestibility of crude fibre than high-yielding cows; and a 4% to 5% higher efficiency of utilization of metabolic energy for milk production &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>, &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>After studying literatures and researches it was noticed that there has been some research around to quantify livestock yield gaps. There is no standard methodology, but multiple methods were combined for research. Researchers were able to calculate relative yield gaps for the dairy production in India and Ethiopia &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. There was analysis based on attainable yields using Empirical Benchmarking, and Stochastic Frontier Analysis to evaluate possible interventions for increasing production (household modelling). It was noticed that large yield gaps exist for dairy production in both countries, and packages of interventions are required to bridge these gaps rather than single interventions. Part of the research was borrowed to analyze the limited dataset chosen as part of this project.&lt;/p>
&lt;h2 id="3-choice-of-datasets">3. Choice of Datasets&lt;/h2>
&lt;p>Number of online literatures and datasets were checked to find out suitable dataset required for this project analysis. Below dataset were found promising:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://dahd.nic.in/about-us/divisions/statistics">DAHD Data&lt;/a> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>, &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>, &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> (&lt;strong>20th India Livestock Census Data&lt;/strong>)&lt;/li>
&lt;/ol>
&lt;p>The Animal Husbandry Statistics Division of the Department of Animal Husbandry &amp;amp; Dairying Division (DAHD) is responsible for the generation of animal husbandry statistics through the schemes of livestock census and integrated sample surveys &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Survey is defined by Indian Agriculture Statistics Research Institute (IASRI) &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. This is the only scheme through which considerable data, particularly on the production estimate of major livestock products, is being generated for policy formulation in the livestock sector. It is mandate for this division to&lt;/p>
&lt;ul>
&lt;li>Conduct quinquennial livestock census&lt;/li>
&lt;li>Conduct annual sample survey through integrated sample survey&lt;/li>
&lt;li>Publish annual production estimates of milk, eggs, meat, wool and other related animal husbandry statistics based on survey&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>&lt;a href="http://www.fao.org/faostat/en/#data">FAO Data&lt;/a> &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>, &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>Food and Agriculture Organization (FAO) of United Nation publishes worldwide data on the aspects of dairy farming which can also be visualized online with the options provided. Some of the data from this source was used to extract useful summary needed in analysis.&lt;/p>
&lt;ol start="3">
&lt;li>[UIDAI Data] (&lt;a href="https://uidai.gov.in">https://uidai.gov.in&lt;/a>) &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>&lt;/li>
&lt;/ol>
&lt;p>Unique Identification Authority of India (UIDAI) was created with the objective to issue Unique Identification numbers (UID), named as &lt;strong>Aadhaar&lt;/strong>, to all residents of India. Projected population data of 2020 was extracted from this source.&lt;/p>
&lt;p>In addition to above, other demographics information such as area of each state, district count was extracted from OpenStreetMap &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. Agricultural zone information was extracted from report of Food and Nutrition Security Analysis, India, 2019 &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;h3 id="41-software-components">4.1 Software Components&lt;/h3>
&lt;p>This project has been implemented in Python 3.7 version. Jupyter Notebook application was used to develop the code and produce a notebook document. Jupyter notebook is a Client-Server architecture-based application which allows modification and execution of code through web browser. Jupyter notebook can be installed locally and accessed through localhost browser or it can be installed on a remote machine and accessed via internet &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>, &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Following python libraries were used in overall code development. Before running the code, one must make sure that these libraries are installed.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Pandas&lt;/strong> This is a high performance and easy to use library. It was used for data cleaning, data analysis, &amp;amp; data preparation.&lt;/li>
&lt;li>&lt;strong>NumPy&lt;/strong> NumPy is python core library used for scientific computing. Some of the basic functions were used in this project.&lt;/li>
&lt;li>&lt;strong>Matplotlib&lt;/strong> This is a comprehensive library used for static, animated and interactive visualization.&lt;/li>
&lt;li>&lt;strong>OS&lt;/strong> This is another standard library of Python which provides miscellaneous operating system interface functions.&lt;/li>
&lt;li>&lt;strong>Scikit-learn (Sklearn)&lt;/strong> Robust library that provides efficient tools for machine learning and statistical modelling.&lt;/li>
&lt;li>&lt;strong>Seaborn&lt;/strong> Python data visualization library based on matplotlib.&lt;/li>
&lt;/ul>
&lt;h3 id="42-data-processing">4.2 Data Processing&lt;/h3>
&lt;p>The raw data retrieved from various sources was in excel or report format. The data was pre-processed and stored back in csv format for the purpose of this project and to easily process it. This dataset was further processed through various stages via EDA, feature engineering and modelling.&lt;/p>
&lt;h4 id="421-eda">4.2.1 EDA&lt;/h4>
&lt;p>Preprocessed dataset selected for this analysis contained information at the state level. There were two seperate pre-processed dataset used. Below was the nature of the attributes in the main dataset which had buffalo information:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>State Name:&lt;/strong> Name of each state in India. One record for each state.&lt;/li>
&lt;li>&lt;strong>Buffalo count:&lt;/strong> Total number of male and female buffaloes. Data recorded for 14 types of buffalo breeds. One attribute for each type of female and male breeds.&lt;/li>
&lt;li>&lt;strong>In Milk animals:&lt;/strong> Number of In-Milk animals per state (figures in 000 no&amp;rsquo;s) recorded each year from 2013 to 2019. One attribute per year&lt;/li>
&lt;li>&lt;strong>Yield per In-Milk animal:&lt;/strong> Yield per In-Milk animals per state (figures in kg/day) recorded each year from 2013 to 2019. One attribute per year.&lt;/li>
&lt;li>&lt;strong>Milk production:&lt;/strong> - Milk production per state (figures in 000 tones) recorded each year from 2013 to 2019. One attribute per year.&lt;/li>
&lt;/ul>
&lt;p>Below was the nature of the attributes in the secondary dataset which had demographic information:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Geographic information:&lt;/strong> features captured at state level where each feature represented - projected population of 2020, total districts in each state, total villages in each state, official area of each state in square kilometer.&lt;/li>
&lt;li>&lt;strong>Climatic information:&lt;/strong> One of attribute for each zone highlighted in Table 1&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Table 1:&lt;/strong> Agro climatic regions in India&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Zones&lt;/th>
&lt;th>Agro-Climatic Regions&lt;/th>
&lt;th>States&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Zone 1&lt;/td>
&lt;td>Western Himalayan Region&lt;/td>
&lt;td>Jammu and Kashmir, Himachal Pradesh, Uttarakhand&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 2&lt;/td>
&lt;td>Eastern Himalayan Region&lt;/td>
&lt;td>Assam, Sikkim, West Bengal, Manipur, Mizoram, Andhra Pradesh, Meghalaya, Tripura&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 3&lt;/td>
&lt;td>Lower Gangetic Plains Region&lt;/td>
&lt;td>West Bengal&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 4&lt;/td>
&lt;td>Middle Gangetic Plains Region&lt;/td>
&lt;td>Uttar Pradesh, Bihar, Jharkhand&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 5&lt;/td>
&lt;td>Upper Gangetic Plains Region&lt;/td>
&lt;td>Uttar Pradesh&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 6&lt;/td>
&lt;td>Trans-Gangetic Plains Region&lt;/td>
&lt;td>Punjab, Haryana, Delhi and Rajasthan&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 7&lt;/td>
&lt;td>Eastern Plateau and Hills Region&lt;/td>
&lt;td>Maharashtra, Chhattisgarh, Jharkhand, Orissa and West Bengal&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 8&lt;/td>
&lt;td>Central Plateau and Hills Region&lt;/td>
&lt;td>Madhya Pradesh, Rajasthan, Uttar Pradesh, Chhattisgarh&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 9&lt;/td>
&lt;td>Western Plateau and Hills Region&lt;/td>
&lt;td>Maharashtra, Madhya Pradesh, Chhattisgarh and Rajasthan&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 10&lt;/td>
&lt;td>Southern Plateau and Hills Region&lt;/td>
&lt;td>Andhra Pradesh, Karnataka, Tamil Nadu, Telangana, Chhattisgarh&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 11&lt;/td>
&lt;td>East Coast Plains and Hills Region&lt;/td>
&lt;td>Orissa, Andhra Pradesh, Tamil Nadu and Pondicherry&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 12&lt;/td>
&lt;td>West Coast Plains and Ghat Region&lt;/td>
&lt;td>Tamil Nadu, Kerala, Goa, Karnataka, Maharashtra, Gujarat&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 13&lt;/td>
&lt;td>Gujarat Plains and Hills Region&lt;/td>
&lt;td>Gujarat, Madhya Pradesh, Rajasthan, Maharashtra&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 14&lt;/td>
&lt;td>Western Dry Region&lt;/td>
&lt;td>Rajasthan&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zone 15&lt;/td>
&lt;td>The Islands Region&lt;/td>
&lt;td>Andaman and Nicobar, Lakshadweep&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Figure 3 shows top 10 states from livestock census having total number of buffalo counts. Uttar Pradesh was the state which reported a greater number of buffaloes compared to any other states in the country.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/top10states.png" alt="Milk Production Share">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Top 10 state by buffalo counts&lt;/p>
&lt;p>There were greater number of female buffaloes reported in country (80%) compared to male buffalo breeds.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/buffaloshare.png" alt="Milk Production Share">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Buffalo breeds ratio by male and female&lt;/p>
&lt;h4 id="422-feature-engineering">4.2.2 Feature Engineering&lt;/h4>
&lt;p>Murrah buffalo shown in Figure 5 is the most productive and globally famous breed &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>, &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>. This breed is resistant to diseases and can adjust to various Indian climate conditions.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/murrah_buffalo.jpeg" alt="Murrah buffalo">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Murrah buffalo (Bubalus bubalis), globally famous local breed of Haryana, were exported to many nations &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>, &lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup>&lt;/p>
&lt;p>In feature engineering multiple attributes were derived needed for modelling or during analysis. Table 2 shows percentage share of Murrah buffalo breed in top 10 states having highest number of total buffaloes. Though Uttar Pradesh was top state in India in terms of total number of buffaloes but percentage share of Murrah buffalo was more in state of Punjab.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong> Murrah buffalo percent share in top 10 state with buffalo count&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>State Name&lt;/th>
&lt;th>Murrah Buffalo Count&lt;/th>
&lt;th>Total Buffalo Count&lt;/th>
&lt;th>% Murrah Breed&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>UTTAR PRADESH&lt;/td>
&lt;td>20110852&lt;/td>
&lt;td>30625334&lt;/td>
&lt;td>65.67&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RAJASTHAN&lt;/td>
&lt;td>6448563&lt;/td>
&lt;td>12976095&lt;/td>
&lt;td>49.70&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ANDHRA PRADESH&lt;/td>
&lt;td>5227270&lt;/td>
&lt;td>10622790&lt;/td>
&lt;td>49.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HARYANA&lt;/td>
&lt;td>5011145&lt;/td>
&lt;td>6085312&lt;/td>
&lt;td>82.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PUNJAB&lt;/td>
&lt;td>4116508&lt;/td>
&lt;td>5159734&lt;/td>
&lt;td>79.78&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BIHAR&lt;/td>
&lt;td>2419952&lt;/td>
&lt;td>7567233&lt;/td>
&lt;td>31.98&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MADHYA PRADESH&lt;/td>
&lt;td>1446078&lt;/td>
&lt;td>8187989&lt;/td>
&lt;td>17.66&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MAHARASHTRA&lt;/td>
&lt;td>986981&lt;/td>
&lt;td>5594392&lt;/td>
&lt;td>17.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TAMIL NADU&lt;/td>
&lt;td>435634&lt;/td>
&lt;td>780431&lt;/td>
&lt;td>55.82&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UTTARAKHAND&lt;/td>
&lt;td>378917&lt;/td>
&lt;td>987775&lt;/td>
&lt;td>38.36&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Survey dataset had three primary attributes reported at the state level. Data reported from 2013 to 2019 for in-milk animals, yield per in-milk animals and milk production per state were averaged for analysis purpose. Total number of buffaloes per breed type were calculated from the data provided in the dataset. Following list of breeds were identified from dataset &lt;sup id="fnref:21">&lt;a href="#fn:21" class="footnote-ref" role="doc-noteref">21&lt;/a>&lt;/sup>.&lt;/p>
&lt;ul>
&lt;li>Banni&lt;/li>
&lt;li>Bhadawari&lt;/li>
&lt;li>Chilika&lt;/li>
&lt;li>Jaffarabadi&lt;/li>
&lt;li>Kalahandi&lt;/li>
&lt;li>Marathwadi&lt;/li>
&lt;li>Mehsana&lt;/li>
&lt;li>Murrah&lt;/li>
&lt;li>Nagpuri&lt;/li>
&lt;li>Nili Ravi&lt;/li>
&lt;li>Non-Descript&lt;/li>
&lt;li>Pandharpuri&lt;/li>
&lt;li>Surti&lt;/li>
&lt;li>Toda&lt;/li>
&lt;/ul>
&lt;p>Data showed that Uttar Pradesh had highest average milk production with in the top 10 states whereas Punjab state had highest average yield per in-milk animals. Figure 6 shows the share of top 3 breeds in both the state. Common attribute seen between two states was highest number of Murrah breed buffaloes compared to other breeds.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/punjab_up_state.png" alt="TOP TWO States">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Top 3 types of buffalo breeds of Uttar Pradesh and Punjab&lt;/p>
&lt;h3 id="43-modelling">4.3 Modelling&lt;/h3>
&lt;h4 id="431-data-preperation">4.3.1 Data Preperation&lt;/h4>
&lt;p>Data from main dataset and supplementary dataset which was demographics data was merged into one dataset. This dataset was not labelled and it was small dataset. We considered average milk production as our target label for analysis. The rest of the features were divided based on categorical and numerical nature. Our dataset did not had any categorical features except state name which was used as index column so no futher processing was considered for this attribute. All the features were of numerical nature and all the data points were not on same scale. Hence datapoints were normalized for further processing.&lt;/p>
&lt;h4 id="432-empirical-benchmarking-model">4.3.2 Empirical Benchmarking Model&lt;/h4>
&lt;p>There are two dominant approach of economic modelling to estimate the production behavior - Empirical Benchmarking and Stochastic Frontier Analysis &lt;sup id="fnref:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup>, &lt;sup id="fnref:23">&lt;a href="#fn:23" class="footnote-ref" role="doc-noteref">23&lt;/a>&lt;/sup>. Empirical Benchmarking is simple modelling method, and it is one of the two dominant approach. This method was used to analyze past 6 years of data points available in the livestock dataset. In this approach milk production data of past 6 years was averaged. Top 10 states with most milk production reported were compared with average of the whole sample. The problem analyzed as part of this project was relatively small. The comparison did not consider all possible characteristics for modelling.&lt;/p>
&lt;p>Correlation of target variable with various demographics features available in the dataset was calculated. Table 3 and Table 4 shows the positive and negative coorelation with target variable average milk production respectively. We noticed from Table 3 that average milk production contribution was getting affected by number of in-milk animals reported in the particular year census data and their was also factor of climatic conditions affecting the milk production. India is divided into 15 Agro climate zones. Agro zone 6 is Trans-Gangetic Plains region. Indian states Chandigarh, Delhi, Haryana, Punjab, Rajasthan (some parts) falls under this zone. Agricultural development has shown phenomenal growth in overall productivity and in providing better environment for dairy farming in this zone &lt;sup id="fnref:24">&lt;a href="#fn:24" class="footnote-ref" role="doc-noteref">24&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;strong>Table 3:&lt;/strong> Positive coorelation of target with demographics features&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>%&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>avg_milk_production&lt;/td>
&lt;td>1.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>avg_in_milk&lt;/td>
&lt;td>0.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>total_female&lt;/td>
&lt;td>0.90&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>total_male&lt;/td>
&lt;td>0.71&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>agro_climatic_zone6&lt;/td>
&lt;td>0.50&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 4:&lt;/strong> Negative coorelation of target with demographics features&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>%&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>official_area_sqkm&lt;/td>
&lt;td>-0.17&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>agro_climatic_zone2&lt;/td>
&lt;td>-0.19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>avg_yield_in_milk&lt;/td>
&lt;td>-0.23&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>district_count&lt;/td>
&lt;td>-0.34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>proj_population_2020&lt;/td>
&lt;td>-0.94&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="433-linear-regression">4.3.3 Linear Regression&lt;/h4>
&lt;p>Our target variable considered was a continous variable. In an attempt to perform dimension reduction, Principal Component Analysis (PCA) was applied to available limited dataset. In the example presented, an pipeline was constructed that had dimension reduction followed by Linear regression classifier. Grid search cross validation was applied (GridSearchCV) to find the best parameters and score. Linear regression was applied with default parameter settings whereas parameter range was passed for PCA. Below is snapshot of pipeline implemented.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#8f5902;font-style:italic"># Define a pipeline to search for the best combination of PCA truncation&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># and classifier regularization.&lt;/span>
&lt;span style="color:#000">pca&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">PCA&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># Linear Regression without parameters&lt;/span>
&lt;span style="color:#000">linear&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">LinearRegression&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;span style="color:#000">full_pipeline_with_predictor&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">Pipeline&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>
&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;preparation&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">num_pipeline&lt;/span>&lt;span style="color:#000;font-weight:bold">),&lt;/span>
&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;pca&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>&lt;span style="color:#000">pca&lt;/span>&lt;span style="color:#000;font-weight:bold">),&lt;/span>
&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;linear&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">linear&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;span style="color:#000;font-weight:bold">])&lt;/span>
&lt;span style="color:#8f5902;font-style:italic"># Parameters of pipelines:&lt;/span>
&lt;span style="color:#000">param_grid&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;span style="color:#4e9a06">&amp;#39;pca__n_components&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">5&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">15&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">30&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">45&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">64&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="5-results">5. Results&lt;/h2>
&lt;p>Based on simple Empirical Benchmarking Analysis and trends noticed in data it appears that it is possible to increase production past currently attainable yields (see Figure 7). The current scale of the yield does indicate that, leading states have best breeds of buffaloes. Different methods of analyzing yield gaps can be combined to give estimates of attainable yields. It will also help to evaluate possible interventions to increase production and profits.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/avgmilkproduction.png" alt="TOP 10 States">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Average milk production in top 10 state with benchmark&lt;/p>
&lt;p>The best parameter came out of the pipeline implemented are highlighted here. The results were not promising due the limited dataset so further experimentation was not attempted, but one thing was noticed that cross validated score came out to 0.28 and dimension reduction to 5 components.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#000">Best&lt;/span> &lt;span style="color:#000">parameter&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">CV&lt;/span> &lt;span style="color:#000">score&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.282&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;span style="color:#000;font-weight:bold">{&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;pca__n_components&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">5&lt;/span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;span style="color:#000">PCA&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">copy&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#3465a4">True&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">iterated_power&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;auto&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">n_components&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#3465a4">None&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">random_state&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#3465a4">None&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;span style="color:#000">svd_solver&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;auto&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">tol&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.0&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">whiten&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#3465a4">False&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-326/raw/main/project/images/covariance.png" alt="COV Analysis">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Covariance Heat Map&lt;/p>
&lt;p>We were able to calculate correlation of census data with other socioeconomic factors like population information, climate information (see Figure 8). The biggest probable limitation here was availability of good quality data. It would have been possible to conduct the analysis at finer level if more granular level data would have been available. Our analysis had to be done state level rather than at district level or specific area.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>The analysis done above with the limited dataset showed that there are considerable gaps in the average yield per in-milk buffalo of state Punjab and Uttar Pradesh, compared to other states in top 10 list. These states have larger share of Murrah breed buffaloes. Based on the data trends it appears that it is possible to increase the production past current attenable numbers. However, this would need to combine different methods and multiple strategies.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>PIB Delhi. (2019). Department of Animal Husbandry &amp;amp; Dairying releases 20th Livestock Census, 16 (Oct 2019).
&lt;a href="https://pib.gov.in/PressReleasePage.aspx?PRID=1588304">https://pib.gov.in/PressReleasePage.aspx?PRID=1588304&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>FAO - Food and Agriculture Organization of United Nation, Accessed: Nov. 2020, &lt;a href="http://www.fao.org/faostat/en/#data">http://www.fao.org/faostat/en/#data&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Mudgal, V.D. (1988). &amp;ldquo;Proc. of the Second World Buffalo Congress, New Delhi, India&amp;rdquo;, 12 to 17 Dec.:454. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Alessandro, Nardone. (2010). &amp;ldquo;Buffalo Production and Research&amp;rdquo;. Italian Journal of Animal Science. 5. 10.4081/ijas.2006.203. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Mayberry, Dianne (07/2017). Yield gap analyses to estimate attainable bovine milk yields and evaluate options to increase production in Ethiopia and India. Agricultural systems (0308-521X), 155 , p. 43. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Department of Animal Husbandry and Dairying. &lt;a href="http://dahd.nic.in/about-us/divisions/statistics">http://dahd.nic.in/about-us/divisions/statistics&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Department of Animal Husbandry and Dairying, Accessed: Oct. 2020, &lt;a href="http://dadf.gov.in/sites/default/filess/20th%20Livestock%20census-2019%20All%20India%20Report.pdf">http://dadf.gov.in/sites/default/filess/20th%20Livestock%20census-2019%20All%20India%20Report.pdf&lt;/a> &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Department of Animal Husbandry and Dairying, Accessed: Oct. 2020, &lt;a href="http://dadf.gov.in/sites/default/filess/Village%20and%20Ward%20Level%20Data%20%5BMale%20%26%20Female%5D.xlsx">http://dadf.gov.in/sites/default/filess/Village%20and%20Ward%20Level%20Data%20%5BMale%20%26%20Female%5D.xlsx&lt;/a> &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Department of Animal Husbandry and Dairying, Accessed: Oct. 2020, &lt;a href="http://dadf.gov.in/sites/default/filess/District-wise%20buffalo%20population%202019_0.pdf">http://dadf.gov.in/sites/default/filess/District-wise%20buffalo%20population%202019_0.pdf&lt;/a> &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>IASRI - Indian Agriculture Statistics Research Institute. &lt;a href="https://iasri.icar.gov.in/">https://iasri.icar.gov.in/&lt;/a> &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>F.A.O. (2008). Food and Agriculture Organization. Rome Italy. STAT &lt;a href="http://database.www.fao.org">http://database.www.fao.org&lt;/a> &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>Unique Identification Authority of India, Accessed: Nov. 2020, &lt;a href="https://uidai.gov.in/images/state-wise-aadhaar-saturation.pdf">https://uidai.gov.in/images/state-wise-aadhaar-saturation.pdf&lt;/a> &lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>OpenStreetMap, Accessed: Nov. 2020, &lt;a href="https://wiki.openstreetmap.org/wiki/Main_Page">https://wiki.openstreetmap.org/wiki/Main_Page&lt;/a> &lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>Food and Nutrition Security Analysis, India, 2019, Accessed: Nov. 2020, &lt;a href="http://mospi.nic.in/sites/default/files/publication_reports/document%281%29.pdf">http://mospi.nic.in/sites/default/files/publication_reports/document%281%29.pdf&lt;/a> &lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>Corey Schafer. Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough. (Sep. 22, 2016). Accessed: Nov. 07, 2020. [Online Video]. Available: &lt;a href="https://www.youtube.com/watch?v=HW29067qVWk">https://www.youtube.com/watch?v=HW29067qVWk&lt;/a> &lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16" role="doc-endnote">
&lt;p>&lt;em>The Jupyter Notebook&lt;/em>. Jupyter Team. Accessed: Nov. 07, 2020. [Online]. Available: &lt;a href="https://jupyter-notebook.readthedocs.io/en/stable/notebook.html">https://jupyter-notebook.readthedocs.io/en/stable/notebook.html&lt;/a> &lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17" role="doc-endnote">
&lt;p>Bharathi Dairy Farm. &lt;a href="http://www.bharathidairyfarm.com/about-murrah.php">http://www.bharathidairyfarm.com/about-murrah.php&lt;/a> &lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18" role="doc-endnote">
&lt;p>Water Buffalo. Accessed: Oct 26, 2020. [Online]. Available: &lt;a href="https://en.wikipedia.org/wiki/Water_buffalo">https://en.wikipedia.org/wiki/Water_buffalo&lt;/a> &lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19" role="doc-endnote">
&lt;p>Kleomarlo. Own work, CC BY-SA 3.0. [Online]. Available: &lt;a href="https://commons.wikimedia.org/w/index.php?curid=4349862">https://commons.wikimedia.org/w/index.php?curid=4349862&lt;/a> &lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20" role="doc-endnote">
&lt;p>ICAR - Central Institute for Research on Buffaloes. &lt;a href="https://cirb.res.in/">https://cirb.res.in/&lt;/a> &lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:21" role="doc-endnote">
&lt;p>List of Water Buffalo Breeds. Accessed: Oct 26, 2020. [Online]. Available: &lt;a href="https://en.wikipedia.org/wiki/List_of_water_buffalo_breeds">https://en.wikipedia.org/wiki/List_of_water_buffalo_breeds&lt;/a> &lt;a href="#fnref:21" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:22" role="doc-endnote">
&lt;p>Bogetoft P., Otto L. (2011) Stochastic Frontier Analysis SFA. In: Benchmarking with DEA, SFA, and R. International Series in Operations Research &amp;amp; Management Science, vol 157. Springer, New York, NY. &lt;a href="https://doi.org/10.1007/978-1-4419-7961-2_7">https://doi.org/10.1007/978-1-4419-7961-2_7&lt;/a> &lt;a href="#fnref:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:23" role="doc-endnote">
&lt;p>Aigner, Dennis (07/1977). &amp;ldquo;Formulation and estimation of stochastic frontier production function models&amp;rdquo;. Journal of econometrics (0304-4076), 6 (1), p. 21. &lt;a href="https://doi.org/10.1016/0304-4076(77)90052-5">https://doi.org/10.1016/0304-4076(77)90052-5&lt;/a> &lt;a href="#fnref:23" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:24" role="doc-endnote">
&lt;p>Farm Mechanization-Department of Agriculture and Cooperation, India, Accessed: Nov. 2020, &lt;a href="http://farmech.gov.in/06035-04-ACZ6-15052006.pdf">http://farmech.gov.in/06035-04-ACZ6-15052006.pdf&lt;/a> &lt;a href="#fnref:24" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Online Store Customer Revenue Prediction</title><link>/report/fa20-523-337/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-337/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-337/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-337/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Balaji Dhamodharan, &lt;a href="mailto:bdhamodh@iu.edu">bdhamodh@iu.edu&lt;/a>, fa20-523-337
Anantha Janakiraman, &lt;a href="mailto:ajanakir@iu.edu">ajanakir@iu.edu&lt;/a>, fa20-523-351&lt;/p>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-337/edit/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>&lt;strong>Situation&lt;/strong> The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies. The objective of this project is to explore different machine learning techniques and identify an optimized model that can help the marketing team understand customer behavior and make informed decisions.&lt;/p>
&lt;p>&lt;strong>Task&lt;/strong> The challenge is to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully this exploration will lead to actionable insights and help allocating marketing budgets for those companies who choose to use data analysis on top of GA data [^1].&lt;/p>
&lt;p>&lt;strong>Action&lt;/strong> This exploration is based on a Kaggle competition and there are two datasets available in Kaggle. One is the test dataset (test.csv) and the other one is the training dataset (train.csv) and together the datasets contain customer transaction information ranging from August 2016 to April 2018. The action plan for this project is to first conduct data exploration that includes but not limited to investigating the statistics of data, examining the target variable distribution and other data distributions visually, determining imputation strategy based on the nature of missing data, exploring different techniques to scale the data, identifying features that may not be needed - for example, columns with constant variance, exploring different encoding techniques to convert categorical data to numerical data and identifying features with high collinearity. The preprocessed data will then be trained using a linear regression model with basic parameter setting and K-Fold cross validation. Based on the outcome of this initial model further experimentation will be conducted to tune the hyper parameters including regularization and also add new derived features to improve the accuracy of the model. Apart from linear regression other machine learning techniques like ensemble methods will be explored and compared.&lt;/p>
&lt;p>&lt;strong>Result&lt;/strong> The best performing model determined based on the RMSE value will be used in the inference process to predict the revenue per customer. The Kaggle competition requires to predict the natural log of sum of all transactions per customer&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. Datasets&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-metrics">2.1 Metrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-source-code">2.2 Source Code&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-methodology">3. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-load-data">3.1 Load Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-data-exploration">3.2 Data Exploration&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#321-exploratory-data-analysis">3.2.1 Exploratory Data Analysis&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#33-data-pre-processing">3.3 Data Pre-Processing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#34-feature-engineering">3.4 Feature Engineering&lt;/a>&lt;/li>
&lt;li>&lt;a href="#34-feature-selection">3.4 Feature Selection&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#341-data-preparation">3.4.1 Data Preparation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#35-model-algorithms-and-optimization-methods">3.5 Model Algorithms and Optimization Methods&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#351-linear-regression-model">3.5.1 Linear Regression Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#352-xgboost-regressor">3.5.2 XGBoost Regressor&lt;/a>&lt;/li>
&lt;li>&lt;a href="#353-lightgbm-regressor">3.5.3 LightGBM Regressor&lt;/a>&lt;/li>
&lt;li>&lt;a href="#354-lasso-regression">3.5.4 Lasso Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#355-ridge-regressor">3.5.5 Ridge Regressor&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-benchmark-results">4. Benchmark Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-software-technologies">5. Software Technologies&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-model-pipeline">6.1 Model Pipeline&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-feature-exploration-and-pre-processing">6.2 Feature Exploration and Pre-Processing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#63-outcome-of-experiments">6.3 Outcome of Experiments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#64-limitations">6.4 Limitations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-previous-explorations">7. Previous Explorations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowlegements">8. Acknowlegements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ecommerce, regression analysis, big data&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The objective of this exploration is to predict the natural log of total revenue per customer which is a real valued continuous output and an algorithm like linear regression will be ideal to predict the response variable that is continuous using a set of predictor variables given the basic assumption that there is a linear relationship between the predictor and response variables.&lt;/p>
&lt;h2 id="2-datasets">2. Datasets&lt;/h2>
&lt;p>As mentioned in the earlier sections, the dataset used in this model exploration was downloaded from Kaggle &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> and available in CSV file format. The training contains more than 872K observations and based on the size of the dataset it would be ideal to use mini-batch or gradient descent optimization techniques to identify the coefficients that best describe the model. The target variable as observed in the dataset is a continuous variable which implies that the use case is a regression problem. As mentioned earlier, there are several machine learning techniques that can be explored for this type of problem including regression and ensemble methods with different parameter settings. The sparsity of potential features in the datasets indicates that multiple experimentations will be required to determine the best performing model. Also based on initial review of the datasets, it also observed that some of the categorical features exhibit low to medium cardinality and if these features are going to be retained in the final dataset used for training then it is important to choose the right encoding technique.&lt;/p>
&lt;ul>
&lt;li>Train.csv User transactions from August 1st, 2016 to August 1st, 2017 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/li>
&lt;li>Test.csv User transactions from August 2nd, 2017 to April 30th, 2018 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/li>
&lt;/ul>
&lt;h3 id="21-metrics">2.1 Metrics&lt;/h3>
&lt;p>The metrics used for evaluation in this analysis is the root mean squared error (RMSE). The root mean squared error function forms the objective/cost function which will be minimized to estimate optimal parameters for the linear function using Gradient Descent. The plan is to conduct multiple experiments with different iterations to obtain convergence and try different hyper-parameters (e.g. learning rate).&lt;/p>
&lt;p>RMSE is defined as:&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/loss.png" alt="Figure 1.1">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> RMSE&lt;/p>
&lt;p>where y-hat is the natural log of the predicted revenue for a customer and y is the natural log of the actual summed revenue value plus one as seen in Figure-1.&lt;/p>
&lt;h3 id="22-source-code">2.2 Source Code&lt;/h3>
&lt;p>Follow this &lt;a href="https://github.com/cybertraining-dsc/fa20-523-337/blob/main/project/code/project.ipynb">link&lt;/a> to the source code for subsequent sections in this report.&lt;/p>
&lt;h2 id="3-methodology">3. Methodology&lt;/h2>
&lt;p>The CRISP-DM process methodology was followed in this project. The high-level implementation steps are shown in Figure-2.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/methodology.png" alt="Methodology">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Project Methodology&lt;/p>
&lt;h3 id="31-load-data">3.1 Load Data&lt;/h3>
&lt;p>The data that was obtained from Kaggle was over 2.6 GB (for Train and Test). As the size of the dataset was significantly large, it was hosted onto a storage bucket in Google Cloud Platform and ingested into the modeling process through standard application API libraries. Also in this project from the available datasets, the Train dataset was used to build the models and test the results because the real end goal is to test the effectiveness of algorithm. Since the test set doesn&amp;rsquo;t contain the Target Variable (rightly so!), it will not be consumed during the testing and evaluation phase in this exploration.&lt;/p>
&lt;h3 id="32-data-exploration">3.2 Data Exploration&lt;/h3>
&lt;p>The dataset obtained for this project is large and it contains over 872k records. The dataset also contains 12 predictor variables and 1 target variable. The target Variable is totals.transactionRevenue and the objective of this exploration is to predict the total transaction revenue of an online store customer as accurately as possible.&lt;/p>
&lt;p>&lt;strong>Target Variable:&lt;/strong> The Target Variable is totals.transactionRevenue has the transaction value of each visit. But, this column contains 98.72% of missing values for revenue (no purchase). The Target variable had a skewed distribution originally and after performing a lognormal distribution on the target variable it has a normal distribution.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/y_after_transformation.png" alt="Target Variable">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Target Variable&lt;/p>
&lt;h4 id="321-exploratory-data-analysis">3.2.1 Exploratory Data Analysis&lt;/h4>
&lt;p>&lt;strong>Browser:&lt;/strong> The most popular browser is Google Chrome. Also, it was observed during the analysis that second and third best users were using safari and firefox respectively.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/device_browser.png" alt="Browser Variable">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Browser Variable&lt;/p>
&lt;p>&lt;strong>Device Category:&lt;/strong> Almost 70% of users were accessing online store via desktop&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/device_category.png" alt="Device Category Variable">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Device Category Variable&lt;/p>
&lt;p>&lt;strong>OperatingSystem:&lt;/strong> Windows is the popular operating system among the desktop users. However, among the mobile users, what&amp;rsquo;s interesting is, almost equal number of ios users (slightly lower) as android users accessed google play store. The reason why this is interesting is because, google play store is primarily used by android users and ios users almost always use Apple Store for downloading apps to their mobile devices. So it is interesting to know, almost equal number of ios users visit google store as well.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/device_operating_system.png" alt="OperatingSystem">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Operating System&lt;/p>
&lt;p>&lt;strong>GeoNetwork-City:&lt;/strong> Mountain View, California tops the cities list for the users who accessed online store. However in the top 10 cities, 4 cities are from California.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/geo_network_city.png" alt="GeoNetwork-City">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> GeoNetwork City&lt;/p>
&lt;p>&lt;strong>GeoNetwork-Country:&lt;/strong> Customers from US are way ahead of other customers from different countries. May be this could be due to the fact that online store data that was provided was pulled from US Google Play Store (Possible BIAS!).&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/geo_network_country.png" alt="GeoNetwork-Country">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> GeoNetwork Country&lt;/p>
&lt;p>&lt;strong>GeoNetwork-Region:&lt;/strong> It is already known that majority of the customers are from US, so America region tops the list.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/geo_network_continent.png" alt="GeoNetwork-Region">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> GeoNetwork Region&lt;/p>
&lt;p>&lt;strong>GeoNetwork-Metro:&lt;/strong> SFO tops the list for all metro cities, followed by New York and then London.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/geo_network_metro.png" alt="GeoNetwork-Metro">&lt;/p>
&lt;p>&lt;strong>Figure 10:&lt;/strong> GeoNetwork Region&lt;/p>
&lt;p>&lt;strong>Ad Sources:&lt;/strong> Google Merchandise and Google Online Store are the top sources where the traffic is coming from to the Online Store.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/traffic_source_adcontent.png" alt="GeoNetwork-Metro">&lt;/p>
&lt;p>&lt;strong>Figure 11:&lt;/strong> Ad Sources&lt;/p>
&lt;h3 id="33-data-pre-processing">3.3 Data Pre-Processing&lt;/h3>
&lt;p>Data Pre-Processing is an important step to build a Machine Learning Model. The data pre-processing step typically consists of data cleaning, transformation, standardization and feature selection, so only the most cleaner and accurate data is fed to a model. The dataset that was downloaded for this project contains several issues with formatting, lot of missing values, less to no variance (zero variance) in some of features and it was also observed the target variable does not have random distribution. The variables such as totals_newVisits, totals_bounces, trafficSource_adwordsClickInfo_page, trafficSource_isTrueDirect, totals_bounces, totals_newVisits had missing values. The missing values were imputed with zeroes, so the machine learning algorithm is able to execute without errors and there are no issues during categorical to numerical encoding. This is a very important step in building the Machine Learning Pipeline.&lt;/p>
&lt;h3 id="34-feature-engineering">3.4 Feature Engineering&lt;/h3>
&lt;p>Feature Engineering is the process of extracting the hidden signals/features, so the model can use these features to increase the predictive power. This step is the fundamental difference between a good model and a bad model. Also, there is no one-size-fits all approach for Feature Engineering. It is extremely time consuming and requires a lot of domain knowledge as well.
For this project, a set of sophisticated functions to extract date related values such as Month, Year, Data, Weekday, WeekofYear have been created. It was observed that browser and operating systems are redundant features and instead of removing them, they were merged to create a combined feature which will potentially increase the predictive power. Also as part of feature engineering several features were derived like mean, sum and count of pageviews and hits that should help increase the feature space and ultimately reduce the total error increasing the overall accuracy of the model.&lt;/p>
&lt;h3 id="34-feature-selection">3.4 Feature Selection&lt;/h3>
&lt;p>Feature Selection refers to selection of features in your data that would improve your machine learning model. There is subtle variation between Feature Selection and Feature Engineering. The Feature Engineering technique is designed to extract more feature from the dataset and the feature selection technique allows only relevant features into the dataset. Also, how does anyone know what are the relevant features? There are several methodologies and techniques that are developed over the years but there is no one-size-fits-all methodology.&lt;/p>
&lt;p>Feature Selection like Feature Engineering is more of an art than science. There are several iterative procedure that uses Information Gain, Entropy and Correlation scores to decide which feature gets into the model. There are also advanced Deep learning models that can be built or tree based models that can help observe variables of high importance after the model is built. Similar to Feature Engineering, Feature Selection should also require domain specific knowledge to develop festure selection strategies.&lt;/p>
&lt;p>In this project, the features that had constant variance in the data were dropped and also the features that had mostly null values with only one Non-null value were dropped too. These features do not possess any statistical significance and add very less value to the modeling process. Also, depending on the final result, different techniques and strategies can be explored to optimize and improve the performance of the model.&lt;/p>
&lt;h4 id="341-data-preparation">3.4.1 Data Preparation&lt;/h4>
&lt;p>Scikit learn has inbuilt libraries to handle Train/Test Split as part the model_selection package. The dataset was split randomly with 80% Training and 20% Testing datasets.&lt;/p>
&lt;h3 id="35-model-algorithms-and-optimization-methods">3.5 Model Algorithms and Optimization Methods&lt;/h3>
&lt;p>Different Machine Learning algorithms and techniques were explored in this project and the outcome of the exploration along with different parameter settings have been discussed in the following sections.&lt;/p>
&lt;h4 id="351-linear-regression-model">3.5.1 Linear Regression Model&lt;/h4>
&lt;p>Linear regression is a supervised learning model that follows a linear approach in that it assumes a linear relationship between one ore more predictor variables (x) and a single target or response variable (y). The target variable can be calculated as a linear combination of the predictors or in other words the target is the calculated by weighted sum of the inputs and bias where the weights are estimated through different optimization techniques. Linear regression is referred to as simple linear regression when there is only one predictor variable involved and referred to as multiple linear regression when there are more than one predictor variables involved. The error between the predicted output and the ground truth is generally calculated using RMSE (root mean squared error). This is one of the classic modeling techniques that was explored in this project because the target variable (revenue per customer) is a real valued continuous output and exhibits a significant linear relationship with the independent variables or the input variables &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In the exploration, SKLearn Linear Regression performed well overall. A 5 fold cross validation was performed and the best RMSE Score for this model observed was: 1.89. As shown in the Figure-12, the training and test RMSE error values are very close indicating that there is no overfitting the data.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/linear_regression3.png" alt="Linear Regression">&lt;/p>
&lt;p>&lt;strong>Figure 12:&lt;/strong> Linear Regression Model&lt;/p>
&lt;h4 id="352-xgboost-regressor">3.5.2 XGBoost Regressor&lt;/h4>
&lt;p>XGBoost regression is a gradient boosting regression technique and one of the popular gradient boosting frameworks that exists today. It follows the ensemble principle where a collection of weak learners improve the prediction accuracy. The prediction in the current step S is weighed based on the outcomes from the previous step S-1. Weak learning is slightly better than random learning and that is one of the key strengths of gradient boosting technique. The XGBoost algorithm was explored for this project for several reasons including it offers built-in regularization that helps avoid overfitting, it can handle missing values effectively and it also does cross validation automatically. The feature space for the dataset being used is sparse and believe the potential to overfit the data is high which is one of the primary reasons for exploring XGBoost &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>XGBoost Regressor performed very well. It was the best performing model with the lowest RMSE score of 1.619. Also the training and test scores are reasonably close and it doesn&amp;rsquo;t look like there was the problem of over fitting the training data. Multiple training iterations of this model were explored with different parameters and most of the iterations resulted in significant error reduction compared to the other models making it the best performing model overall.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/xgboost3new.png" alt="XGBoost">&lt;/p>
&lt;p>&lt;strong>Figure 13:&lt;/strong> XGBoost Model&lt;/p>
&lt;h4 id="353-lightgbm-regressor">3.5.3 LightGBM Regressor&lt;/h4>
&lt;p>LightGBM is a popular gradient boosting framework similar to XGBoost and is gaining popularity in the recent days. The important difference between lightGBM and other gradient boosting frameworks is that LightGBM grows the tree vertically or in other words it grows the tree leaf-wise compared to other frameworks where the trees grow horizontally. In this project the lightGBM framework was experimented primarily because this framework works well on large dataset with more than 10K observations. The algorithm also has a high throughput while using reasonably less memory but there is one problem with overfitting the data which was controlled to a large extent in this exploration using appropriate hyper parameter setting and achieved optimal performance &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>LightGBM Regression was the second best performing model in terms of RMSE scores. Also the training and test scores observed were slightly different indicating a potential problem of overfitting as discussed earlier. As in other experiments, multiple training iterations of this model were explored with different parameter settings and although it achieved reasonable error reduction compared to most of the other models that were explored, it still did not outperform the XGBoost regressor making it the second best performing model.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/lighgbm3new.png" alt="lightgbm">&lt;/p>
&lt;p>&lt;strong>Figure 14:&lt;/strong> LightGBM Model&lt;/p>
&lt;h4 id="354-lasso-regression">3.5.4 Lasso Regression&lt;/h4>
&lt;p>Lasso is a regression technique that uses L1 regularization. In statistics, lasso regression is a method to do automatic variable selection and regularization to improve prediction accuracy and performance of the statistical model. Lasso regression by nature makes the coefficient for some of the variables zero meaning these variables are automatically eliminated from the modeling process. The L1 regularization parameter helps control overfitting and will need to be explored for a range of values for a specific problem. When the regularization penalty tends to be zero there is no regularization, and the loss function is mostly influenced by the squared loss and in contrary if the regularization penalty tends to be closer to infinity then the objective function is mostly influenced by the regularization part. It is always ideal to explore a range of values for the regularization penalty to improve the accuracy and avoid overfitting &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In this project, Lasso is one of the important techniques that was explored primarily because the problem being solved is a regression problem and there is possibility to overfit the data due to the number of observations and feature space. During the model training phase different ranges for regularization penalty were explored and the appropriate value that helped achieve maximum reduction in the total RMSE score was identified.&lt;/p>
&lt;p>Lasso performed slightly better than baseline model. However, it did not outperform lightGBM or XGBoost or tree based models in general.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/lasso3new.png" alt="lasso">&lt;/p>
&lt;p>&lt;strong>Figure 15:&lt;/strong> Lasso Model&lt;/p>
&lt;h4 id="355-ridge-regressor">3.5.5 Ridge Regressor&lt;/h4>
&lt;p>Ridge is a regression technique that uses L2 regularization. Ridge regression does not offer automatic variable selection in the sense that it is not make the weights zero on any of the variable used in the model and the regularization term in a ridge regression is slightly different than the lasso regression. The regularization term is the sum of the square of coefficients multiplied by the penalty whereas in lasso it is the sum of the absolute value of the coefficients. The regularization term is a gentle trade-off between fitting the model and overfitting the model and like in lasso it helps improve prediction accuracy as well as performance of the statistical model. The L2 regularization parameter helps control overfitting and will need to be explored for a range of values for a specific problem similar to Lasso. The regularization parameter also helps reduce multicollinearity in the model. Similar to Lasso, when the regularization penalty tends to be zero there is no regularization, and the loss function is mostly influenced by the squared loss and in contrary if the regularization penalty tends to be closer to infinity then the objective function is mostly influenced by the regularization part. As in the case of Lasso, it is always ideal to explore a range of values for the regularization penalty to improve the accuracy and avoid overfitting &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In this project, Ridge regression is one of the important techniques that was explored again primarily because the problem being solved is a regression problem and there is possibility to overfit the data due to the number of observations and feature space. During the model training phase different ranges for regularization penalty were explored and the appropriate value that helped achieve maximum reduction in the total RMSE score was identified. Ridge performed slightly better than baseline model. However like Lasso, it did not outperform lightGBM or XGBoost or tree based models in general.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/ridge3new.png" alt="Ridge">&lt;/p>
&lt;p>&lt;strong>Figure 16:&lt;/strong> Ridge Model&lt;/p>
&lt;h2 id="4-benchmark-results">4. Benchmark Results&lt;/h2>
&lt;p>There are some interesting observations from the benchmark results seen in Figure-17. As expected, the data exploration and pre-processing performed very well and the data load and flattening of JSON took only a few hundred milliseconds on a platform like Google Colab compared to more than a minute running the same code locally on a desktop. The grid search for linear regression as expected took more time than the grid search for regularization techniques which is an interesting finding. The model training phase using ridge regression took only half the time approximately 250 seconds compared to 811 seconds for lasso regression. The highest training time of approximately 1800 seconds was recorded with XGBoost regressor and although the original assumption was that this modeling technique would consume time and significant system resources to complete the training process, the total time of 1800 seconds was certainly in the higher end. But, considering the fact that random forest regressor took more than 90 minutes to complete the training process during the experimentation phase, XGBoost performed much better than random forest. The other interesting observation was between LightGBM and XGBoost where LightGBM took significantly less time than XGBoost regressor and if performance and high availability are key considerations during operationalization with slight compromise on model performance then LightGBM would be an ideal candidate for real time operationalization.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/benchmark1.png" alt="Benchmark">&lt;/p>
&lt;p>&lt;strong>Figure 17:&lt;/strong> Benchmark Results&lt;/p>
&lt;h2 id="5-software-technologies">5. Software Technologies&lt;/h2>
&lt;p>In this project tools like Python and Google Colab Jupyter Notebook were used. Also several Python packages were employed in this exploration such as Pandas, Numpy, Matplotlib, sklearn&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>As a project team the intention was to create a template that can be utilized for any ML project. The dataset that was used for this project was challenging in a way that it required a lot of data cleaning, flattening and transformation to get the data into the required format.&lt;/p>
&lt;h3 id="61-model-pipeline">6.1 Model Pipeline&lt;/h3>
&lt;p>In this project, multiple regression and tree based models from scikit learn library were explored with various hyper parameter setting and other methods like the lightGBM. The goal of the model pipeline was to explore and examine data, identify data pre-processing methods, imputation strategies, derive features and try different feature extraction methods was to perform different experiments with different parameter setting and identify the optimized with low RMSE that can be operationalized in Production. The parameters were explored within the boundary of this problem setting using different techniques.&lt;/p>
&lt;h3 id="62-feature-exploration-and-pre-processing">6.2 Feature Exploration and Pre-Processing&lt;/h3>
&lt;p>As part of this project a few features were engineered and included in the training dataset. The feature importance visualizations that were generated after the model training process indicate that these engineered features were part of the top 30% of high impact features and they contributed reasonably to improving the overall accuracy of the model. During additional experimentation phase, the possibility of including few other potential features that could be derived from the dataset was explored and those additional features were included in the final dataset that was used during model training. Although these features did not contribute largely to reducing the error it gave an opportunity to share ideas and methods to develop these new features. Also during feature exploration phase other imputation strategies were evaluated, attempted to identify more outliers and tried different encoding techniques for categorical variables and ultimately determined that label encoder or ordinal encoder is the best way forward. Also some of the low importance features were excluded and the model was retrained to validate if the same or better RMSE value could be achieved.&lt;/p>
&lt;h3 id="63-outcome-of-experiments">6.3 Outcome of Experiments&lt;/h3>
&lt;p>Multiple modeling techniques were explored as part of this project like Linear regression, gradient boosting algorithms and linear regression regularization techniques. The techniques were explored with basic parameter setting and based on the outcome of those experiments, the hyper parameters were tuned using grid search to obtain the best estimator evaluated on RMSE. Also, during grid search K-Fold cross validation of training data was used and the cross validated results were examined through a results table. The fit_intercept flag played a significant role resulting in an optimal error. As part of the different experimentations that were performed, random forest algorithm was also explored but it suffered performance issues and it seemed like it would require more iterations to converge which is why it was dropped from our results and further exploration. Although random forest was not explored, gradient boosting techniques were part of the experimentations and the best RMSE from XGBoost. The LightGBM regressor was also explored with different parameter settings but it did not produce better RMSE score than XGBoost.&lt;/p>
&lt;p>In the case of XGBoost, there was improvement to the RMSE score as different tree depths, feature fraction, learning rate, number of children, bagging fraction, sub-sample were explored. There was significant improvement to the error metric when these parameters were adjusted in an intuitive way. Also, linear regression with regularization techniques were explored and although there was some improvement to the error metric compared to the basic linear regression model they did not perform better than the gradient boosting method that was explored. So, based on different explorations and experimentations a reasonably conclusion can be made that gradient boosting technique performed better for the given problem setting and generated the best RMSE score. Based on the evaluation results of XGBoost on the dataset used, the recommendation would be to test the XGBoost model with real time data and the performance of the model can be evaluated in real-time scenario too and additionally, if needed, hyper parameter tuning can be performed on the XGBoost model specifically for the real-time scenario &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. The feature engineering process on the dataset helped derive features with additional predictive value and a pipeline was built to reuse the same process in different modeling techniques. Five different models were tested including Linear Regression, XGBoost, Light GBM, Lasso and Ridge. The summary of all the models can be seen in Figure-17.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-337/raw/main/project/images/model_results_new2new.png" alt="Model_Results">&lt;/p>
&lt;p>&lt;strong>Figure 18:&lt;/strong> Model Results Summary&lt;/p>
&lt;h3 id="64-limitations">6.4 Limitations&lt;/h3>
&lt;p>Due to the limited capacity of our Colab Notebook setup, there was difficulty in performing cross Validation for XGBoost and LightGBM. The KFold cross validation with different parameter settings would have helped identify the best estimator for these models, helped achieve even better rmse scores and potentially avoid overfitting, if any. The tree based models performed well in this dataset and it would be beneficial to explore other tree based models like Random Forest in the future and evaluate/compare the performance.&lt;/p>
&lt;h2 id="7-previous-explorations">7. Previous Explorations&lt;/h2>
&lt;p>The Online GStore customer revenue prediction problem is a Kaggle competition with more than 4100 entries. It is one of the popular challenges in Kaggle with a prize money of $45,000. Although the goal was not to make it to the top in the leader board, the challenge gave a huge opportunity to explore different methods, techniques, tools and resources. The one important difference between many of the previous of explorations versus what has been achieved in this exploration is the number of different machine learning algorithms that was explored and the performance for each of those different techniques were examined. Based on review of several submissions in Kaggle there were only a very few kernel entries that explored different parameter settings and making intuitive adjustments to them to make the model perform at an optimum level like what has been accomplished in this project. The other uniqueness that was brought to this submission was identifying techniques that offered good performance and consumed less system resources in terms of operationalization. There is lot of scope to continue exploration and attempt other techniques to identify the best performing model.&lt;/p>
&lt;h2 id="8-acknowlegements">8. Acknowlegements&lt;/h2>
&lt;p>The team would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the other instructors in the Big Data Applications course for their guidance and support through the course of this project and advise on documenting the results of various explorations.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Kaggle Competition,2019,Predict the Online Store Revenue,[online] Available at: &lt;a href="https://www.kaggle.com/c/ga-customer-revenue-prediction/rules">https://www.kaggle.com/c/ga-customer-revenue-prediction/rules&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Kaggle Competition,2019,Predict the Online Store Revenue, Data, [online] Available at: &lt;a href="https://www.kaggle.com/c/ga-customer-revenue-prediction/data">https://www.kaggle.com/c/ga-customer-revenue-prediction/data&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Machine Learning Mastery,2016,Brownlee, Linear Regression Model, [online] Available at: &lt;a href="https://machinelearningmastery.com/linear-regression-for-machine-learning">https://machinelearningmastery.com/linear-regression-for-machine-learning&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>XGBoost 2020,xgboost developers, XGBoost, [online] Available at: &lt;a href="https://xgboost.readthedocs.io">https://xgboost.readthedocs.io&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Datacamp,2019,Pathak, Using XGBoost in Python, [online] Available at: &lt;a href="https://www.datacamp.com/community/tutorials/xgboost-in-python">https://www.datacamp.com/community/tutorials/xgboost-in-python&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Towards Datascience,2017,Lutins, Ensemble Methods in Machine Learning, [online] Available at: &lt;a href="https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f">https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>Medium 2017,Mandot, What is LightGBM,[online] Available at: &lt;a href="https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc">https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc&lt;/a> &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>Kaggle 2018,Daniel, Google Analytics Customer Revenue Prediction, [online] Available at: &lt;a href="https://www.kaggle.com/fabiendaniel/lgbm-starter">https://www.kaggle.com/fabiendaniel/lgbm-starter&lt;/a> &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>Towards Datascience,2018,Bhattacharya, Ridge and Lasso regression, [online] Available at: &lt;a href="https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b">https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b&lt;/a> &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>Datacamp,2019,Oleszak, Regularization: Ridge, Lasso and Elastic Net, [online] Available at: &lt;a href="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net">https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net&lt;/a> &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data in Sports Game Predictions and How It is Used in Sports Gambling</title><link>/report/fa20-523-331/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-331/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-331/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-331/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-331/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-331/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Mansukh Kandhari, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-331/">fa20-523-331&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-331/blob/master/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Big data in sports is being used more and more as technology advances and this has a very big impact, especially when it comes to sports gambling. Sports gambling has been around for a while and it is gaining popularity with it being legalized in more places across the world. It is a very lucrative industry and the bookmakers use everything they can to make sure the overall odds are in their favor so they can reduce the risk of paying out to the betters and ensure a steady return. Sports statistics and data is more important than ever for bookmakers to come up with the odds they put out to the public. Odds are no longer just determined by expert analyzers for a specific sport. The compilation of odds uses a lot of historical data about team and player performance and looks at the most intricate details in order to ensure accuracy. Bookmakers spend a lot of money to employ the best statisticians and the best algorithms. There are also many companies that solely focus on sports data analysis, who often work with bookmakers around the world. On the other hand, big data for sports game analysis is also used by gamblers to gain a competitive edge. Many different algorithms have been created by researchers and gamblers to try to beat the bookmakers, some more successful than others. Oftentimes these not only involve examining sports data, but also analysing data from different bookmakers odds in order to determine the best bets to place. Overall, big data is very important in this field and this research paper aims to show the various techniques that are used by different stakeholders.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background">2. Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-how-bookmakes-determine-odds">3. How bookmakes Determine odds&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-poisson-model">5. Poisson Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-algorithms-and-prediction-models">6. Algorithms and prediction models&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> sports, sportsbook, betting, gambling, data analysis, machine learning, punter(British word for gambler)&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big Data in sports has been used for years by various stakeholders in this industry to do everything from predicting game outcomes to injury prevention. It is also becoming very prevalent in the area of sports gambling. Ever since the Supreme court decision in Murphy v. National Collegiate Athletic Association that overturned a ban on sports betting, the majority of states in the US have passed legislation to allow sports gambling &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. In 2019, the global sports betting market was valued at 85.047 US Dollars so this is an already very big industry that is expanding &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. There are various platforms that allow betting in this industry including tangible sports books, casinos, racetracks, and many online and mobile gambling apps. The interesting thing about big data in sports betting is that it is being used on both sides in this market. It is used by bookmakers to create game models and come up with different spreads and odds, but big data analysis is also being used by gamblers to gain a competetive advantage and place more accurate bets. Various prediction models using machine learning and big data analytics have been created and they can sometimes be very accurate. For example, Google correctly predicted 14 out of the 16 matches in the 2014 world cup and Microsoft did even better by correctly predicting 15 out of the 16 matches during that year &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Many big companies have spent a lot of time gathering lots of data and creating prediction algorithms, inlcuding ESPN&amp;rsquo;s Football Power index that gives the probility of one team beating another, Analytics Powerhouse 538 that determines scores of games using their ELO method, and Accuscore which runs Mone Carlo Simulations on worldwide sporting events &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Bookmakers use all their possible tools and algorithms to put out the best odds that will give them a return. They often employ teams of statisticians that use the most advanced prediction models and information from data analysis companies to come up with their odds. If sports data analysis is vastly being used by people other than bookies and prediction models can often be very accurate, one might wonder how people haven&amp;rsquo;t made millions off sports betting and how bookmakers are still in business? This report analyzes how big data analytics are used by bookmakers to come up with the odds they put out for games while also examining how it is used on the gamblers side. It aims to analysize various prediction models created by sports betters, researchers, and AI companies, and see how they compare to the way big data is used by bookmakers. Besides giving an analysis of how big data is used in this field, it will show if betting guided by prediction models can give a consistent return.&lt;/p>
&lt;h2 id="2-background">2. Background&lt;/h2>
&lt;p>Many people with an interest in sports betting prediction have created models, some that involve machine learning and AI. A few of these projects have had some very interesting results using different types of data and analysis techniques. Jordan Bailey created an NBA prediction model based on the over under bets &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. For context, bookmakers will set a point total for a game and bettors can bet on whether the actual score will be over or under the point total set by the book. This type of betting is offered for many types of sports. Using NBA box scores for 5 previous seasons and data on historical betting lines created by various bookmakers, Bailey created a logistical regression model that would return a prediction on if the score was over or under a point total set by a bookmaker. Two models were created, one that predicted if a game would be over a set line and one that predicted if it would be under a set line. The datasets for these models were structured in a way where each specific game was represented as the box scores for the 3 previous games for each team, so 6 previous games &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. When creating the model, the first four seasons were used as the training set and the fifth season was used as the testing data to make predictions on &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. In order to determine the significance of results, Bailey set up a &amp;ldquo;confidence threshold&amp;rdquo; of 62 percent for the probability his model returned on a game being over or under. The prediction was &amp;ldquo;confident&amp;rdquo; if the probability of the prediction was above the set threshold. When testing the models, the over model predicted 88 games confidently and 52 games correctly, with an accuracy of 59.09 percent. The under model predicted 96 games confidently and correctly predicted 52 games with an accuracy of 54.16 percent &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. To simulate how the model would perform on betting with 10,000 dollars, a bet was made every time the model predicted a confident bet for the 2018 NBA season. The accuracy on the bets were 52.52 percent and the total after the simulation was 11,880 dollars.&lt;/p>
&lt;h2 id="3-how-bookmakes-determine-odds">3. How bookmakes Determine odds&lt;/h2>
&lt;p>It is no secret that bookmakers use a lot of data and apply various statistical techniques to come up with betting odds. The statistical techniques used and the data that bookmakers look at vary from sport to sport, for example, a popular method for modeling soccer uses the Poisson distribution since it can be very accurate but also because it makes it easy to add time decay to the inputs &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Big data and data accuracy plays such a big part for bookmakers that many companies in the sports betting market have multi million dollar contracts with big leagues like the NFL &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. The NBA also recently extended their contracts with Sportradar and Genius Sports group that will have the rights to distribute official NBA data to licenced sports betting operators in the United States &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Companies like Sportradar collect and analyze official data and provide services to various bookmakers. The accuracy of data can be very important, a difference of something as little as one yard can make such a big difference; therefore, the industry values the accuracy of data that the leagues itself can provide. Bookmakers employ various mathematicians to analyze historical sports data to come up with odds; however, sports data isn’t the only thing that bookmakers look at when determining how they will make odds for a game. At the end of the day, the gambling industry is a numbers game that thrives on ensuring the probability is in the houses favor. Bookmakers use various techniques involving big data and factors such as public opinion to do so.&lt;/p>
&lt;p>Big data is being used more and more in various industries, and in the gambling industry, it isn&amp;rsquo;t just used to come up with odds. One major way it is used is by gathering data about user demographics &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. When using an online sportsbook, the casino can gather data about a users age, location, gender, excetera which can provide valuable insights that can be used for product development and marketing purposes. By using user demographic data to provide targeted advertising, casinos and bookmakers can attract more betters which will increase revenue. As said by former odds compiler Matthew Trenhaile, &amp;ldquo;Their [bookmakers] product is entertainment and not the selling of an intellectual contest between punter and bookmaker. It is foolish to think this has ever been the product that bookmakers have sold.They sell an adrenaline rush and anyone who thinks great characters pitting themselves against the punters and taking anyone on in horse racing betting rings is what betting used to be about is kidding himself or herself.&amp;rdquo; Due to the probability of making money in sports betting, and really every type of gambling, being in the houses favor, online casinos and sportsbooks use big data to increase the number of bets placed by customers; nevertheless, using models to come up with odds is the heart of this industry which makes it the most important way big data analytics is used by bookmakers.&lt;/p>
&lt;p>When odds are being made for a sports book, a lot of things are taken into consideration in the process. Bookmakers have teams of statisticians that analyze historical data of the teams in order to come up with game prediction models, often using machine learning based algorithms&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. When bookmakers are actually making the odds, these statistical models created from large amounts of data aren&amp;rsquo;t the only thing they use. When bookmakers are creating odds, their goal isn&amp;rsquo;t to come up with an accurate game prediction, it is to have the lowest probability of paying out, so they will add a margin in order to statistically ensure a profit regardless of the outcome. Some times public opinion is used by bookmakers to sway their odds. For example, if a team has been on an unexpected winning streak, the bookmakers will often overestimate their odds, even against a team that will statistically do better than them, since people will be more inclined to take that bet resulting in the bookmaker reducing their probability of paying out &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Furthermore, bookmakers will often &amp;ldquo;hedge&amp;rdquo; bets to cover potential losses if an unexpected outcome occurs. For example, if a large amount of people are betting on a team regardless of the odds, the bookmakers will have a large payout if that outcome occurs, so they will start offering more favorable odds on the opposite outcome so they can bring in bets that would cover their losses &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. At the end of the day, when bookmakers set out their odds they will always make sure they are statistically in their favor. Even though bookmakers heavily analyze sports data in order to come up with prediction models, the odds put out don&amp;rsquo;t exactly reflect the true probability of a game outcome. Game prediction models is used to come up with the most probable event occurring but bookmakers add a margin that is skews the actual probability in order to statistically ensure a profit &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. An example of a coin toss can show how these margins work &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. If one were to bet on a coin toss, there is a 50 percent change of heads winning and a 50 percent change of tails winning so that means neither side is favored and the market of this bet is 100 percent. As a bookmaker is trying to ensure a profit, they will add a margin to the actual game winning probability in order to mitigate risk and ensure that the odds are in their favor. The margins that the bookmakers put on the actual probability is determined by many factors such as public opinion and perception of a team &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. Gamblers are actually able to calculate the margins that the bookmakers put on a bet using a relatively simple formula. This formula varies for the type of sports the gambler is trying to calculate the odds for. In a two way market such as tennis or basketball, a person can figure out the bookmakers margin using the decimal odds places for both sides &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. This formula is 100(1/decimal odds) + 1000(1/other decimal odds). The amount the market percentage is over 100 is the margin the bookmaker has on the better; therefore, the margin percentage the bookmaker has over the gambler can be determined by subtracting 100 from that formula.&lt;/p>
&lt;h2 id="5-poisson-model">5. Poisson Model&lt;/h2>
&lt;p>One of the most popular models for soccer game predictions is the Poisson distribution model. According to former odds compiler Matthew Trenhaile, the Poisson distribution model for soccer prediction can be very accurate and is very useful since it is easy to add time decay to the inputs &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Refinements can easily be made as a game progresses and goal input changes to easily re calculate the odds, which is useful for bookmakers. The Poisson distribution for soccer game predictions is not only used on a large scale by bookmakers to calculate odds, but it is also often used by even small time bettors to determine how they will bet. A Poisson model for soccer games can even be created on Excel for betters who want to place their bets more accurately. This process works by using historical data of how many goals a team scored and how many goals they let in and comparing it to a leagues average in order to determine the number of goals each team is likely to score in a game. It starts with calculating the average number of goals scored for home games and away games for the whole league and determining a team&amp;rsquo;s &amp;ldquo;attack strength&amp;rdquo; and &amp;ldquo;defense strength&amp;rdquo; &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>. The attack strength is a team&amp;rsquo;s average number of goals per game divided by the league average of goals per game. Similarly, a teams defense strength is determined by dividing a teams average number of goals conceded by the leagues average number of goals conceded. The goal expectancy for the home team is calculated by multiplying the team&amp;rsquo;s attack strength with the away team&amp;rsquo;s defense strength and the league&amp;rsquo;s average number of home goals. The goal expectancy for the away team is calculated by multiplying the away teams attack strength with the home teams defense strength and multiplying it by the leagues average number of away goals &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>. With this information, one can determine the probability for the range of goal outcomes on both sides using a formula created by the French mathematician Simeon Denis Poisson. The Poisson distribution indicates the probability of a given number of events occurring over a fixed interval, so it can be used to determine the probability of the number of goals scored in a soccer game. The formula for this for soccer prediction is P(x events in interval) = (e-μ) (μx) / x! . This formula determines the probability of the number of goals being scored (x) using Euler’s number (e) and the goal expectancy (μ). With this formula, we can see the probability each team has for scoring a number of goals in a game. Usually the distribution is done for 0-5 goals to see the percentages of each team scoring on the goal interval. This can be used by bookmakers to determine odds and by gamblers to make well educated bets.&lt;/p>
&lt;h2 id="6-algorithms-and-prediction-models">6. Algorithms and prediction models&lt;/h2>
&lt;p>Gamblers often use various models, driven by big data, in order to help them place more accurate bets. Many different models have been created in the sports field that use factors such as historical sports data in order to come up with game prediction models. A lot of people who have come up with good prediction models will not share how they work and sometimes offer a subscription service where eager gamblers can pay to receive game picks. When it comes to making the best sports betting algorithms, it isn&amp;rsquo;t just about the amount of data a person can acquire. Creating algorithms that predict well takes understanding the sport and the meaning behind each type of data. In regards to creating sports prediction algorithms and the data that goes behind it, Micheal Beuoy, an actuary and popular sports data analyst said, &amp;ldquo;I think it takes discipline combined with a solid understanding of the sport you’re trying to analyze. If you don’t understand the context behind your numbers, no amount of advanced analysis and complicated algorithms is going to help you make sense of them. You need discipline because it is very easy to lock in on a particular theory and only pay attention to the data points that confirm that theory. A good practice is to always set aside a portion of your data before you start analysing. Once you’ve built what you think is your &amp;ldquo;best&amp;rdquo; model or theory, you can then test it against this alternative dataset.&amp;rdquo; Creating sports prediction algorithms requires a lot of different types of analysis and the methods that yield the best results are always changing.&lt;/p>
&lt;p>Creating good models requires understanding the sport well and using specific types of data in the algorithms. A creator of an NBA game prediction algorithm who runs a website called Fast Break Bets, which sells a game prediction service, primarily uses NBA game statistics known as efficiency metrics to come up with his model &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>. As the creator of the algorithm is profiting off eager gamblers, the exacts of how it works have not been released but the creator explains the type of data he uses to make his algorithm work. The NBA is a game of efficiency since there is a shot clock and possessions are changed very quickly, so the score total of games can greatly vary by how fast or slow paced a team is. The creator of this algorithm uses an offensive and defensive rating that measures how many points a team scores and allows per 100 possessions, since 100 possessions is close to the NBA average of possessions per game &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>. The algorithm also uses effective field goal percentage, turnover rate, and rebounding rate with offensive and defensive rating to optimize the efficiency metrics. Another major factor that this creator uses in the algorithm is the NBA season schedule and how often a team plays games in a time span &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>. This is due to the fact that players get fatigued playing games close to each other and coaches will therefore limit the amount of time some of the players will be on the court in order to give them a rest. This is important since player statistics can greatly vary from person to person on a team. Using this information of efficiency metrics, player performance, and the frequency of games played, the creator was able to create a prediction model that works well enough for people to pay for his game picks.&lt;/p>
&lt;p>In research done by Manuel Silvero, he studied 5 famous algorithms that used Neural Network and Machine learning and concluded that their accuracy varies from 50-70 percent, depending on the sport &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>. Purucker in 1996 was one of the first computational sports prediction model and used an Artificial Neural Network with backward propagation &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>. It was 61 percent accurate. In 2003, Khan expanded Puruckers work and was more acurate. Data on 208 matches was collected and the elements used were total yardage differential, rushing yardage differential, turnover differential, away team indicator and home team indicator &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>. The first 192 matches of the season were used as the training data set for the model. When tested on the remaining games of the season, the models predicted at a 75 percent accuracy. This was compared to predictions created by 8 ESPN sportscasters for the same games and they only predicted 63 percent of those matches correctly.&lt;/p>
&lt;p>One of the most accurate models created, in terms of receiving a good return on a bet, was created by Lisandro Kaunitz of the University of Tokyo, and relied on data from odds that bookmakers put out rather than historical game data &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. When it comes to statistical models for sports, big data from historical sports games are often analyzed in order to come up with game predictions and to gain insight on things like team, player, and position performance. In the market of sports betting, these models are used to come up with odds and also by bettors to place bets. Gamblers have came up with different game prediction models in order to beat the books, mainly comprising of historical sports data while sometimes also using historical betting data &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Kaunitz created a model that mainly focused on analysing data of the odds created by bookmakers, rather than sports team data, to predict good bets to place. The basis of his model relied on a technique bookmakers use to reduce their payout risk, known as hedging. This concept and the way bookmakers use it to reduce their risk of payout is covered in section 3. Kaunitz betting model worked by gathering the odds for a game created by various bookmakers and determining the average odds available. Using statistical analysis of odds offered, Kaunitz was able to determine any outliers from the average odds for a game &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Using these outliers, Kaunitz could determine if a bet would favor them or not. After various simulations and models, Kaunitz&amp;rsquo;s and his team took their strategy into the real world, and their bets payed out 47.2 percent of the time. They received an 8.5 percent return and profited $957.50 in 265 bets &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Due to their impressive returns, bookmakers caught on and started to limit the amount that they could bet&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>Overall, big data plays a very important role in the sports betting industry and it is used by various stakeholders. Bookmakers use it to come up with odds and gamblers use it for a competitive advantage. Although data analysis is very important on both ends, this research shows that it is very hard to receive a consistent return as a gambler. From 1989 to 2000 for NFl betting, the bookmakers favorite won 66.7 percent of the time and from 2001 and 2012, the bookmakers favorite won 66.9 percent of the time &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>. Even though technology has advanced and people use the most sophisticated algorithms to come up with prediction models, the bookmaker seems to have the advantage. This is due to the fact that bookmakers spend tons of money gathering the most accurate data and employ some of the best statisticians and sports analysing firms, but also due to the way they hedge bets and use public opinion to modify odds in order prevent potential losses. Bookmakers adjust for a margin when they are compiling their odds, because just like everything in the gambling industry, the probability is set up so that the house will always win in the long run. Gamblers have created various algorithms in order to make the most educated sports bet. These use historical team data but sometimes also use data from betting odds. Some of the best betting algorithms work by analyzing bookmakers' odds and determining where the odds are significantly different from the expected outcome of the game. As seen with Lisandro Kaunitz from the University of Tokyo, when bookmakers see that gamblers are beating the system they can start to limit a person&amp;rsquo;s bets. Overall, big data plays a huge role in the sports gambling industry. Even though what happens on the field or the court is often based on chance, there are significant trends that can be seen when statistically analyzing sports data. At the end of the day, big data plays a big role in this industry for bookmakers and gamblers alike.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>INSIGHT: Sports Betting in States Races on a Year After SCOTUS Overturns Ban,&amp;quot; Bloomberg Law, 04-Jun-2019. [Online]. Available: &lt;a href="https://news.bloomberglaw.com/us-law-week/insight-sports-betting-in-states-races-on-a-year-after-scotus-overturns-ban">https://news.bloomberglaw.com/us-law-week/insight-sports-betting-in-states-races-on-a-year-after-scotus-overturns-ban&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Research and Markets, &amp;ldquo;Global Sports Betting Market Worth $85 Billion in 2019 - Industry Assessment and Forecasts Throughout 2020-2025,&amp;rdquo; GlobeNewswire News Room, 31-Aug-2020. [Online]. Available: &lt;a href="https://www.globenewswire.com/news-release/2020/08/31/2086041/0/en/Global-Sports-Betting-Market-Worth-85-Billion-in-2019-Industry-Assessment-and-Forecasts-Throughout-2020-2025.html">https://www.globenewswire.com/news-release/2020/08/31/2086041/0/en/Global-Sports-Betting-Market-Worth-85-Billion-in-2019-Industry-Assessment-and-Forecasts-Throughout-2020-2025.html&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>R. Delgado, &amp;ldquo;How Big Data is Changing the Gambling World: Articles: Chief Data Officer,&amp;rdquo; Articles | Chief Data Officer | Innovation Enterprise, 01-Sep-2016. [Online]. Available: &lt;a href="https://channels.theinnovationenterprise.com/articles/how-big-data-is-changing-the-gambling-world">https://channels.theinnovationenterprise.com/articles/how-big-data-is-changing-the-gambling-world&lt;/a>. [Accessed: 29-Nov-2020]. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>J. Zalcman, &amp;ldquo;HOW TO CREATE A SPORTS BETTING ALGORITHM,&amp;rdquo; Oddsfactory, 16-Nov-2020. [Online]. Available: &lt;a href="https://theoddsfactory.com/how-to-create-a-sports-betting-algorithm/">https://theoddsfactory.com/how-to-create-a-sports-betting-algorithm/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>J. Bailey, &amp;ldquo;Applying Data Science to Sports Betting,&amp;rdquo; Medium, 18-Sep-2018. [Online]. Available: &lt;a href="https://medium.com/@jxbailey23/applying-data-science-to-sports-betting-1856ac0b2cab">https://medium.com/@jxbailey23/applying-data-science-to-sports-betting-1856ac0b2cab&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>J. Bailey, &amp;ldquo;Jordan-Bailey/DSI_Capstone_Project,&amp;rdquo; DSI_Capstone_Project, 14-Sep-2018. [Online]. Available: &lt;a href="https://github.com/Jordan-Bailey/DSI_Capstone_Project/blob/master/Technical_Report.md">https://github.com/Jordan-Bailey/DSI_Capstone_Project/blob/master/Technical_Report.md&lt;/a>. [Accessed: 01-Dec-2020]. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>M. Trenhaile, &amp;ldquo;How Bookmakers Create their Odds, from a Former Odds Compiler,&amp;rdquo; Medium, 29-Jun-2017. [Online]. Available: &lt;a href="https://medium.com/@TrademateSports/how-bookmakers-create-their-odds-from-a-former-odds-compiler-5b36b4937439">https://medium.com/@TrademateSports/how-bookmakers-create-their-odds-from-a-former-odds-compiler-5b36b4937439&lt;/a>. [Accessed: Nov-2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>K. J. Brooks, &amp;ldquo;The new game in town for pro sports leagues: Selling stats,&amp;rdquo; CBS News, 09-Jan-2020. [Online]. Available: &lt;a href="https://www.cbsnews.com/news/nba-nfl-sports-nascar-leagues-selling-stats-to-gambling-companies/">https://www.cbsnews.com/news/nba-nfl-sports-nascar-leagues-selling-stats-to-gambling-companies/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>C. Murphy, &amp;ldquo;NBA extends data partnerships with Sportradar and Genius Sports Group,&amp;rdquo; SBC Americas, 29-Oct-2020. [Online]. Available: &lt;a href="https://sbcamericas.com/2020/10/29/nba-extends-data-partnerships-with-sportradar-and-genius-sports-group/">https://sbcamericas.com/2020/10/29/nba-extends-data-partnerships-with-sportradar-and-genius-sports-group/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>&amp;ldquo;How Big Data Analytics Are Transforming the Global Gambling Industry,&amp;rdquo; Analytics Insight, 17-Jan-2020. [Online]. Available: &lt;a href="https://www.analyticsinsight.net/how-big-data-analytics-are-transforming-the-global-gambling-industry/">https://www.analyticsinsight.net/how-big-data-analytics-are-transforming-the-global-gambling-industry/&lt;/a>. [Accessed: Oct-2020]. &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>arXiv, &amp;ldquo;The Secret Betting Strategy That Beats Online Bookmakers,&amp;rdquo; MIT Technology Review, 19-Oct-2017. [Online]. Available: &lt;a href="https://www.technologyreview.com/2017/10/19/67760/the-secret-betting-strategy-that-beats-online-bookmakers/">https://www.technologyreview.com/2017/10/19/67760/the-secret-betting-strategy-that-beats-online-bookmakers/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>A. Dörr, &amp;ldquo;How to apply predictive analytics to Premiership football to beat the bookies,&amp;rdquo; Dataconomy, 19-Mar-2019. [Online]. Available: &lt;a href="https://dataconomy.com/2019/03/how-to-apply-predictive-analytics-to-premiership-football-to-beat-the-bookies%EF%BB%BF/">https://dataconomy.com/2019/03/how-to-apply-predictive-analytics-to-premiership-football-to-beat-the-bookies%EF%BB%BF/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>S. Hubbard, &amp;ldquo;Betting Margins Explained: How to Calculate Sports Margins,&amp;rdquo; BettingLounge, 24-Sep-2020. [Online]. Available: &lt;a href="https://bettinglounge.co.uk/guides/sports-betting-explained/betting-margins/">https://bettinglounge.co.uk/guides/sports-betting-explained/betting-margins/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>&amp;ldquo;Sportsbook Profit Margins,&amp;rdquo; Sports Insights, 18-Sep-2015. [Online]. Available: &lt;a href="https://www.sportsinsights.com/betting-tools/sportsbook-profit-margins/">https://www.sportsinsights.com/betting-tools/sportsbook-profit-margins/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>B. Cronin, &amp;ldquo;Poisson Distribution: Predict the score in soccer betting,&amp;rdquo; Pinnacle, 27-Apr-2017. [Online]. Available: &lt;a href="https://www.pinnacle.com/en/betting-articles/Soccer/how-to-calculate-poisson-distribution/MD62MLXUMKMXZ6A8">https://www.pinnacle.com/en/betting-articles/Soccer/how-to-calculate-poisson-distribution/MD62MLXUMKMXZ6A8&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16" role="doc-endnote">
&lt;p>Stephen, &amp;ldquo;NBA Betting Model Explained: Sports Betting Picks, Tips, and Blog,&amp;rdquo; FAST BREAK BETS, 11-Nov-2017. [Online]. Available: &lt;a href="https://www.fastbreakbets.com/nba-picks/nba-betting-model-explained/">https://www.fastbreakbets.com/nba-picks/nba-betting-model-explained/&lt;/a>. [Accessed: 05-Dec-2020]. &lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17" role="doc-endnote">
&lt;p>M. Silverio, &amp;ldquo;My findings on using machine learning for sports betting: Do bookmakers always win?,&amp;rdquo; Medium, 26-Aug-2020. [Online]. Available: &lt;a href="https://towardsdatascience.com/my-findings-on-using-machine-learning-for-sports-betting-do-bookmakers-always-win-6bc8684baa8c">https://towardsdatascience.com/my-findings-on-using-machine-learning-for-sports-betting-do-bookmakers-always-win-6bc8684baa8c&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18" role="doc-endnote">
&lt;p>R. P. Bunker and F. Thabtah, &amp;ldquo;A machine learning framework for sport result prediction,&amp;rdquo; Applied Computing and Informatics, 19-Sep-2017. [Online]. Available: &lt;a href="https://www.sciencedirect.com/science/article/pii/S2210832717301485">https://www.sciencedirect.com/science/article/pii/S2210832717301485&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19" role="doc-endnote">
&lt;p>M. Beouy, &amp;ldquo;BGO - The Casino of the Future,&amp;rdquo; bgo Online Casino. [Online]. Available: &amp;lt;https://www.bgo.com/casino-of-the-future/the-future-of-sports-betting/. [Accessed: 05-Dec-2020]&amp;gt;. &lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analyzing LSTM Performance on Predicting the Stock Market for Multiple Time Steps</title><link>/report/fa20-523-313/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-313/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final Type: Project&lt;/p>
&lt;p>Fauzan Isnaini, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/">fa20-523-313&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Predicting the stock market has been an attractive field of research for a long time because it promises big wealth for anyone who can find the secret. For a long time, traders around the world have been relying on fundamental analysis and technical analysis to predict the market. Now with the advancement of big data, some financial institutions are beginning to predict the market by creating a model of the market using machine learning. While some researches produce promising results, most of them are directed at predicting the next day&amp;rsquo;s market behavior. In this study, we created an LSTM model to predict the market for multiple time frames. We then analyzed the performance of the model for some different time periods. From our observations, LSTM is good at predicting 30 time steps ahead, but the RMSE became larger as the time frame gets longer.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-macd-in-technical-analysis">2.1 MACD in Technical Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-time-series-forecasting">2.2 Time Series Forecasting&lt;/a>&lt;/li>
&lt;li>&lt;a href="#23-using-lstm-in-stock-prediction-and-quantitative-trading">2.3 Using LSTM in Stock Prediction and Quantitative Trading&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-technology">4.1 Technology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#42-data-preprocessing">4.2 Data Preprocessing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#43-the-lstm-model">4.3 The LSTM Model&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-results">5. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion-and-future-works">6. Conclusion and Future Works&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> stock, market, predictive analytics, LSTM, random forest, regression, technical analysis&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Stock market prediction is a fascinating field of study for many analysts and researchers becauseof the significant amount of money circulating in the market. While there are numerous studies conducted in this field, predicting the stock market remains a challenging task, because of its noisy and non-stationary nature &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The stock market is &amp;ldquo;noisy&amp;rdquo; because it is sensitive to mass psychology. The trends and patterns in the stock market can also change abruptly because of bad news, natural disasters, and some unforeseen circumstances, thus it is considered non-stationary.&lt;/p>
&lt;p>The efficient market hypothesis even suggests that predicting or forecasting the financial market is unrealistic because price changes in the real world are unpredictable. All the changes in prices of the financial market are based on immediate economic events or news. Investors are profit-oriented, their buying or selling decisions are made according to the most recent events regardless of past analysis or plans. The argument about this Efficient Market Hypothesis has never been ended. So far, there is no strong proof that can verify if the efficient market hypothesis is proper or not &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>However, as Mostafa &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> claims, financial markets are predictable to a certain extent. The past experience of many price changes over a certain period of time in the financial market and the undiscounted serial correlations among vital economic events affecting the future financial market are two main pieces of evidence opposing the Efficient Market Hypothesis.&lt;/p>
&lt;p>The most popular methods in predicting the stock markets are technical and fundamental analysis. Fundamental analysis is mainly based on three essential aspects &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>: (i) macroeconomic analysis such as Gross Domestic Products and Consumer Price Index (CPI) which analyses the effect of the macroeconomic environment on the future profit of a company, (ii) industry analysis which estimates the value of the company based on industry status and prospect, and (iii) company analysis which analyses the current operation and financial status of a company to evaluate its internal value.&lt;/p>
&lt;p>On the other hand, technical analysis is grouped into eight domains &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>: sentiment, flow-of-funds, raw data, trend, momentum, volume, cycle, and volatility. Sentiment represents the behaviors of various market participants. Flow-of-funds is a type of indicator used to investigate the financial status of various investors to pre-evaluate their strength in terms of buying and selling stocks, then, corresponding strategies, such as short squeeze can be adopted. Raw data include stock price series and price patterns such as K-line diagrams and bar charts. Trend and momentum are examples of price-based indicators, trend is used for tracing the stock price trends while momentum is used to evaluate the velocity of the price change and judge whether a trend reversal in stock price is about to occur. Volume is an indicator that reflects the enthusiasm of both buyers and sellers for investing, it is also a basis for predicting stock price movements. The cycle is based on the theory that stock prices vary periodically in the form of a long cycle of more than 10 years containing short cycles of a few days or weeks. Finally, volatility is often used to investigate the fluctuation range of stock prices and to evaluate risk and identify the level of support and resistance.&lt;/p>
&lt;p>While those two are still the most popular approaches, the age of big data has brought a new method to predict the stock market: quantitative analysis. In this new method, the stock market is captured into a mathematical model, and machine learning is used to predict its behavior. Research by Alzazah and Cheng &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> analyzed more than 50 articles to compare various machine learning (ML) and deep learning (DL) methods used to find which method could be more effective in prediction and for which types and amount of data. This research has proven that quantitative analysis with LSTM gives a promising result as the predictor of a stock market.&lt;/p>
&lt;p>In this study, we analyzed the performance of LSTM in predicting the stock market for multiple time frames. LSTM model is used because it is designed to forecast, predict, and classify time series data &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Despite the promising result in LSTM, most of the previous studies are conducted in building a model to predict the next day&amp;rsquo;s price. Thus, we wanted to know how accurate the LSTM model in predicting the stock market for a longer time frame (i.e. from daily to monthly time frame). We also chose to incorporate technical analysis rather than fundamental analysis in our model, because while fundamental analysis tends to be accurate in the yearly period, it could not predict the fluctuation in the given time frame.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;h3 id="21-macd-in-technical-analysis">2.1 MACD in Technical Analysis&lt;/h3>
&lt;p>MACD &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> is an acronym for moving average convergence/divergence. It is a widely used technical indicator to confirm either the bullish or bearish phase of the market. In essence, the MACD indicator shows the perceived strength of a downward or upward movement in price. Technically, it’s an oscillator, which is a term used for indicators that fluctuate between two extreme values, for example, from 0 to 100.
MACD evolved from the exponential moving average (EMA), which was proposed by Gerald Appel in the 1970s. The standard MACD is the 12-day EMA subtracted by the 26-day EMA, which is also called the DIF. The MACD histogram, which was developed by T. Aspray in 1986, measures the signed distance between the MACD and its signal line calculated using the 9-day EMA of the MACD, which is called the DEA. Similar to the MACD, the MACD histogram is an oscillator that fluctuates above and below the zero line. The construction formula of MACD is given in figure 1.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/MACDFormula.png" alt="MACD Formula">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> MACD formula &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The number of the MACD histogram is usually called the MACD bar or OSC. The analysis process of the cross and deviation strategy of DIF and DEA includes the following three steps: (i) Calculate the values of DIF and DEA, (ii)When DIF and DEA are positive, the MACD line cuts the signal line in the uptrend, and the divergence is positive, there is a buy signal confirmation, and (iii)When DIF and DEA are negative, the signal line cuts the MACD line in the downtrend, and the divergence is negative, there is a sell signal confirmation.&lt;/p>
&lt;h3 id="22-time-series-forecasting">2.2 Time Series Forecasting&lt;/h3>
&lt;p>Time series analysis and dynamic modeling &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> is an interesting research area with a great number of applications in business, economics, ﬁnance, and computer science. The aim of time series analysis is to study the path observations of time series and build a model to describe the structure of data and then predict the future values of time series. Due to the importance of time series forecasting in many branches of applied sciences, it is essential to build an effective model with the aim of improving the forecasting accuracy. A variety of time series forecasting models have been evolved in the literature.&lt;/p>
&lt;p>Time series forecasting is traditionally performed in econometric using ARIMA models, which is generalized by Box and Jenkins. ARIMA has been a standard method for time series forecasting for a long time. Even though ARIMA models are very prevalent in modeling economical and ﬁnancial time series, they have some major limitations. For instance, in a simple ARIMA model, it is hard to model the non-linear relationships between variables. Furthermore, it is assumed that there is a constant standard deviation in errors in ARIMA model, which is in practice may not be satisﬁed. When an ARIMA model is integrated with a Generalized Auto-regressive Conditional Heteroskedasticity(GARCH) model, this assumption can be relaxed. On the other hand, the optimization of a GARCH model and its parameters might be challenging and problematic. There are several other applications of ARIMA for modeling short and long-run effects of economics parameters.&lt;/p>
&lt;p>Recently, new techniques in deep learning have been developed to address the challenges related to the forecasting models. LSTM (Long Short-Term Memory) is a special case of the Recurrent Neural Network (RNN) method that was initially introduced by Hochreiter and Schmidhuber. Even though it is a relatively new approach to address prediction problems. Deep learning-based approaches have gained popularity among researchers.&lt;/p>
&lt;p>LSTM is designed to forecast, predict, and classify time series data even long time lags between vital events that happened before. LSTMs have been applied to solve several problems; among those, handwriting Recognition and speech recognition made LSTM famous. LSTM has copious advantages compared with traditional back-propagation neural networks and normal recurrent neural networks. The constant error backpropagation inside memory blocks enables LSTM ability to overcome long time lags in case of problems similar to those discussed above; LSTM can handle noise, distributed representations, and continuous values; LSTM requires no need for parameter fine-tuning, it works well over a broad range of parameters such as learning rate, input gate bias, and output gate bias &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="23-using-lstm-in-stock-prediction-and-quantitative-trading">2.3 Using LSTM in Stock Prediction and Quantitative Trading&lt;/h3>
&lt;p>During the pre-deep learning era, Financial Time Series modeling has mainly concentrated in the field of ARIMA and any modifications on this, and the result has proved that the traditional time series model does provide decent predictive power to a limit. More recently, deep learning methods have demonstrated better performances thanks to improved computational power and the ability to learn non-linear relationships enclosed in various financial features.&lt;/p>
&lt;p>The direction of the financial market is always stochastic and volatile and the return of the security return is deemed to be unpredictable. Analysts now are trying to apply the modeling techniques from Natural Language Processing into the field of Finance as the similarity of having the sequential property in the data. Zhou &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup> has constructed and applied the Long Short Term Memory Model (LSTM) and the traditional ARIMA model, into the prediction of stock prices on the next day. It was proven that the LSTM model performed better than the ARIMA model.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>This project used the historical data of the Jakarta Composite Index (JKSE) from Yahoo Finance &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. The JKSE is a national stock index of Indonesia, which consists of 700 companies. We choose to incorporate the composite index because it has a beta value of 1, which means it has neutral volatility compared to an individual stock to be incorporated into a model. The dataset contains the Open, High, Low, Close, and Volume data for daily time period on the stock index. The daily data is taken from January 1st, 2013 until November 17th, 2020. We choose the daily data over the monthly data because it offers a more complete pattern. Figure 2 and 3 provides a snapshot of the first few rows of the daily and monthly data respectively.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newdailyhead.png" alt="Head of Daily Data">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Snapshot of the first rows of the daily data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newMonthlyHead.png" alt="Head of Monthly Data">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Snapshot of the first rows of the monthly data&lt;/p>
&lt;p>We also used the MACD technical indicator as an input to our model. The MACD parameters are generated using the ta-lib library &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> based on the Yahoo Finance data. Figure 4 and 5 provides a snapshot of the first few rows of the daily and monthly data respectively after incorporating the MACD technical indicator.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/MACDonDaily.png" alt="Daily MACD">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> MACD on the daily data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newMACDonMonthly.png" alt="Monthly MACD">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> MACD on the monthly data&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;h3 id="41-technology">4.1 Technology&lt;/h3>
&lt;p>Python &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup> was the language of choice for this project. This was an easy decision for these reasons &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>:&lt;/p>
&lt;ol>
&lt;li>Python as a language has an enormous community behind it. Any problems that might be encountered can be easily solved with a trip to Stack Overflow. Python is among the most popular languages on the site which makes it very likely there will be a direct answer to any query.&lt;/li>
&lt;li>Python has an abundance of powerful tools ready for scientific computing. Packages such as Numpy, Pandas, and SciPy are freely available and well documented. Packages such as these can dramatically reduce, and simplify the code needed to write a given program. This makes iteration quick.&lt;/li>
&lt;li>Python as a language is forgiving and allows for programs that look like pseudo code. This is useful when pseudocode given in academic papers needs to be implemented and tested. Using Python, this step is usually reasonably trivial.&lt;/li>
&lt;/ol>
&lt;p>In building the LSTM model, Keras &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup> library is used. It contains numerous implementations of commonly used neural network building blocks such as layers, objectives, activation functions, optimizers, and a host of tools to make working with image and text data easier. The code is hosted on GitHub, and community support forums include the GitHub issues page, a Gitter channel, and a Slack channel.&lt;/p>
&lt;h3 id="42-data-preprocessing">4.2 Data Preprocessing&lt;/h3>
&lt;p>After downloading the historical datasets from Yahoo Finance, the MACD technical indicator is generated using the ta-lib library. Because MACD needs to capture data from the previous time period, the MACD values on the first rows of the data are missing. These rows are then removed before being split into 8:2 proportions for training and testing purposes in the LSTM model.&lt;/p>
&lt;h3 id="43-the-lstm-model">4.3 The LSTM Model&lt;/h3>
&lt;p>A multivariate LSTM model with two hidden layers is used, with a dropout parameter of 0.2. Adam is used as the optimization algorithm. The model uses 90 days time steps, which means it uses the past 90 days of data to predict the output. It has 8 features, which are the Close, Low, High, Open, Volume, MACD, MACD Signal, and MACD Histogram. It then gives one output, which is the open price for the given time frame. We then analyze the performance of our model for each of the time frames.&lt;/p>
&lt;h2 id="5-results">5. Results&lt;/h2>
&lt;p>We used callback function to find the best number of epochs in the model. Figure 6 gives the mean squared error (MSE) curve of the prediction in the training dataset for each given epoch. It shows that the MSE converges after 20 epochs, with a value of 0.0243.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newlossepochs.png" alt="Epoch Loss">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> MSE on the training data for each given epoch&lt;/p>
&lt;p>Figure 7 shows the root mean squared error (RMSE) on the testing dataset for each time frame. It clearly shows that the RMSE becomes bigger on a longer time frame. When predicting the next day period, the RMSE is 323.41, while when predicting 30 days ahead, the RMSE increase to 481.32. But overall, these values are still acceptable because they are smaller than the standard deviation of the actual dataset of 667.31.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/RMSEonTimeFrame.png" alt="RMSE">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> RMSE on the training data for each time frame&lt;/p>
&lt;p>Figure 8 and Figure 9 compare the predicted values on the training data for 1 day and 30 days time frames respectively, while Figure 10 and 11 give the comparison on the test data. It can be seen that the model cannot predict steep ramps in the price change, thus it is lagged from the actual price. The predicted price becomes further lagged when predicting for a longer time frame, thus resulting in a bigger RMSE.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newOneDayPredict.png" alt="Next Day Prediction">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Comparison between the next day prediction and its actual values based on the training data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newThirtyDaysPredict.png" alt="30 Days Prediction">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Comparison between the 30 days time frame prediction and its actual values based on the training data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/newonedaytest.png" alt="Next Day Test">&lt;/p>
&lt;p>&lt;strong>Figure 10:&lt;/strong> Comparison between the next day prediction and its actual values based on the testing data&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/thirtydaystest.png" alt="30 Days Test">&lt;/p>
&lt;p>&lt;strong>Figure 11:&lt;/strong> Comparison between the 30 days time frame prediction and its actual values based on the testing data&lt;/p>
&lt;p>We also found that a longer training dataset does not always give a better prediction because the model might overfit with the training data. In fact, when we used historical market data from January 2000, the RMSE became close to 3,000. This might be due to overall the stock market tends to always get higher every year. When the data is too old, the model needs to compensate for the time when the price is still very low.&lt;/p>
&lt;p>We also capture the time needed to run each critical process using cloudmesh-common benchmark and stopwatch framework &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The stopwatch recordings are shown in Table 1. The table shows that training the model took the longest time. It also highlights the system&amp;rsquo;s specification used in running the program.&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong> Benchmark results&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/benchmark1.png" alt="Table 1">&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-313/raw/main/project/images/stopwatch.png" alt="Stopwatch">&lt;/p>
&lt;h2 id="6-conclusion-and-future-works">6. Conclusion and Future Works&lt;/h2>
&lt;p>We have analyzed the performance of LSTM in predicting the stock price for different time frames. While it gives a promising result in predicting the next day&amp;rsquo;s price, the prediction becomes less accurate for a longer time frame. This might be due to the non-stationarity nature of the stock market. The stock market trends can change abruptly because of a sudden change in the political and economic conditions. Using the daily market data, our model gives promising results within 30 days time frame.
This project has analyzed the performance of LSTM using RMSE, but further research may measure the performance based on the potential financial gain. After all, the stock market is a place to make money, thus financial gain is a better metric of performance.
Further improvement may also be done on our model. We only used price data and MACD technical indicator for the prediction. Further research may utilize other technical indicators, such as RSI and Stochastics to get a better prediction.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Geoffrey Fox, Dr. Gregor von Laszewski, and the associate instructors in the FA20-BL-ENGR-E534-11530: Big Data Applications course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions concerning exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>D. Shah, H. Isah, and F. Zulkernine, &amp;ldquo;Stock Market Analysis: A Review and Taxonomy of Prediction Techniques,&amp;rdquo; International Journal of Financial Studies, vol. 7, no. 2, p. 26, 2019. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>F. S. Alzazah and X. Cheng, &amp;ldquo;Recent Advances in Stock Market Prediction Using Text Mining: A Survey,&amp;rdquo; E-Business [Working Title], 2020. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>A. Mostafa and Y. S., &amp;ldquo;Introduction to financial forecasting. Applied Intelligence,&amp;rdquo; Applied Intelligence, 1996. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>S. Siami-Namini, N. Tavakoli, and A. S. Namin, &amp;ldquo;A Comparison of ARIMA and LSTM in Forecasting Time Series,&amp;rdquo; 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), 2018. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>G. von Laszewski, &amp;ldquo;cloudmesh/cloudmesh-common,&amp;rdquo; GitHub, 2020. [Online]. Available: &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>J. Wang and J. Kim, &amp;ldquo;Predicting Stock Price Trend Using MACD Optimized by Historical Volatility.&amp;rdquo; &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>V. Bielinskas, &amp;ldquo;Multivariate Time Series Prediction with LSTM and Multiple features (Predict Google Stock Price),&amp;rdquo; Youtube, 2020. [Online]. Available: &lt;a href="https://www.youtube.com/watch?v=gSYiKKoREFI">https://www.youtube.com/watch?v=gSYiKKoREFI&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>&amp;ldquo;Composite Index (JKSE) Charts, Data &amp;amp; News,&amp;rdquo; Yahoo! Finance, 08-Dec-2020. [Online]. Available: &lt;a href="https://finance.yahoo.com/quote/%5EJKSE/">https://finance.yahoo.com/quote/^JKSE/&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>J. Bosco and F. Khan, Stock Market Prediction and Efficiency Analysis using Recurrent Neural Network. Berlin, Germany: 2018, 2018. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>F. Isnaini, &amp;ldquo;cybertraining-dsc/fa20-523-313,&amp;rdquo; GitHub, 08-Dec-2020. [Online]. Available: &lt;a href="https://github.com/cybertraining-dsc/fa20-523-313/blob/main/project/code/multivariate.ipynb">https://github.com/cybertraining-dsc/fa20-523-313/blob/main/project/code/multivariate.ipynb&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>TA-Lib. [Online]. Available: &lt;a href="https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html">https://mrjbq7.github.io/ta-lib/func_groups/momentum_indicators.html&lt;/a>. [Accessed: 08-Dec-2020]. &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Stock Price Reactions to Earnings Announcements</title><link>/report/fa20-523-336/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-336/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-336/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-336/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-336/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-336/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Matthew Frechette, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-336/">fa20-523-336&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-336/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>On average the US stock market sees a total of $192 billion a day trading hands. Massive companies, hedge funds, and other high level institutions use the markets to capitalize on companies potential and growth over time. The authors used Financial Modeling Prep to gather over 20 years of historical stock data and earnings calls to understand better what happens during company&amp;rsquo;s earnings annoucements. The results showed that over a large sample size of companies, identifying a strong coorelation was rather difficult, yet companies with strong price trend tendencies were more predictable to beat earnings expectations.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-and-previous-work">2. Background And Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-the-data">3. The Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-the-idea">4. The Idea&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-the-process">5. The Process&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-data-collection">5.1. Data Collection&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-finding-earnings-data">5.2. Finding Earnings Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-calculations-and-results">5.3. Calculations And Results&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-the-results">6. The Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-test-results---calculation-1">6.1 Test Results - Calculation 1&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-test-results---calculation-2">6.2 Test Results - Calculation 2&lt;/a>&lt;/li>
&lt;li>&lt;a href="#63-full-results---calculation-3">6.3 Full Results - Calculation 3&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgements">8. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> stocks, stock, finance, earnings, company, market, stock market, revenue, eps&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>For the final project a topic was picked that is wildly popular in today’s world. The stock markets. Mathematicians and data scientists, for decades upon decades, have dedicated billions of dollars to study market patterns, movements, and company predictions. With the stock market providing so much potential for riches, it is no doubt that it has gained the attention and spending dollars of some of the most influential and richest companies in the world. Although this project comes nowhere near to what some hedge funds and data scientists are currently doing, my idea for the project was thought of in hopes to get a slightly better understanding on the ways the prices move after a company releases their earnings reports. Earnings reports are issued by companies after each fiscal quarter (4 months) and provide some interesting insight into how the company is doing, if they have improved, and whether or not they have reached their goals. Earnings also provide great opportunities for investors as they can find companies with good or bad earnings to profit off of (either short or long).
For this final project, a report along with a software component was picked to better understand the topic as a whole. Sitting down to write software for a project would allow for a better grasp of the topic and to understand the data at hand better.&lt;/p>
&lt;h2 id="2-background-and-previous-work">2. Background And Previous Work&lt;/h2>
&lt;p>After reviewing other data studies and public ideas on the topic, it was concluded that there weren&amp;rsquo;t any major studies done specifically on the price reaction during earnings. The studies that are currently out there didn&amp;rsquo;t cover quiet the amount of data that was wanted to be seen, nor did they covered all of the calculation points that should have been in a full study. Additionally, most of the studies that were done focused more on the technical side of price movements and price patterns, but not a lot on the fundamental earnings of a company. Back testing, is the act of testing a strategy in the past to see how it would perform. (Finding the success rate, return on investment, investment hold time, etc.) Since this study was not particularly looking to find the best investment strategy, but simply to find how price and earnings announcements interact with one another, it was chosen not to incorporate a back testing strategy into the software, but use percentage test analysis and evaluate multiple areas of price changes during the earnings day. These tests were dependent on the trend of the stock price and the earnings results from the announcement. In addition to determining the price reaction from these announcements, it would also provide and insight into how buyer sentiment changes during these times as a price drop usually indicates negative buyer/investor interest in the security.&lt;/p>
&lt;h2 id="3-the-data">3. The Data&lt;/h2>
&lt;p>For this project data from Financial Modeling Prep is used, or FMP as it refer to in the code and report. Financial Modeling Prep is a three-year-old internet company headquartered in France that provides financial data to other organizations around the world. Their data is reliable and has a fast pull speed which allowed the software to capture company data quickly and efficiently. FMP supported all of the data requirements that were needed for this project: Company Historical Prices, Earnings Dates, Earnings per share, and company expected earnings.&lt;/p>
&lt;h2 id="4-the-idea">4. The Idea&lt;/h2>
&lt;p>Going into this project, it was wanted to find a way to best measure and capture buyer / investor sentiment about a company but wasn’t sure exactly where to start. There were a wide range of ideas that included evaluating price changes due to executive staff changes, price changes during days of the week/month/year, and even price changes based on the weather that day. Although some ideas may have been more intricate and harder to calculate than others, it was thought that the best idea to capture the sentiment of investors was to further investigate price changes in stocks around their earning dates. Many investors are hesitant to invest in companies in the short term during these periods as it is sometimes cause for uncertainty and harsh volatility in the stock’s price.
For this project, the studies planned to look at markets as a whole and had the ability to access over 7000 US based stock’s data to get more broad and hopefully accurate results. However, to only get better and more know/verifiable stock data, only stocks from the S&amp;amp;P500 list were picked. The idea for the software included finding the historical stock price on the days that earnings were being released and capturing how the actual earnings per share (EPS) compared to the predicted earnings per share. If the actual EPS is equal to or greater than the predicted EPS that advisors publish, which is all public data, the stock price is thought to increase during the trading day. The program developed tested this theory. Additionally, it was wanted to see how these stock prices were affected by stocks in certain trends. A stock is typically said to be in an uptrend if it is making higher highs, and high lows, and also above the 20 period and 50 period moving averages. A stock making lower lows and lower highs it said to be in a downtrend. The software was able to capture this by comparing the stock’s historical price data and moving averages at the time of the earnings release. My original prediction was that stocks in an uptrend that underperformed on earnings would recover faster / have a lower loss% than that of stocks in a downtrend that underperformed on earnings. All of the resulting data is shown in the results section of this report.&lt;/p>
&lt;h2 id="5-the-process">5. The Process&lt;/h2>
&lt;h3 id="51-data-collection">5.1. Data Collection&lt;/h3>
&lt;p>The first and simplest part of the project was to gather the data from FMP. As this is a generally routine task and would need to be completed for every stock that was needed to be accessed, a function was created to more effectively gather this data. FMPgetStockHistoricalData(ticker, apiKey). The function took in a stock ticker and API key. The stock ticker must be reflective of a currently listed company on one of the US stock-exchanges. For example, entering a company that has been delisted (STGC: Startech Global) would result in an invalid result. The function also requires an FMP API key which can be purchased with unlimited pull requests for less than $20/month. This function returns a list of OHLC objects which store the stock’s open, high, low and close prices for the day, in addition to the month day and year of the price data. This is incredibly valuable data as it allows the software quick access to specific days in the company’s history.&lt;/p>
&lt;h3 id="52-finding-earnings-data">5.2. Finding Earnings Data&lt;/h3>
&lt;p>Secondly, the earnings data from the company must be gathered, FMP has the ability to pull earnings results going back roughly 20 years depending on the company, this is more than adequate for this software as the calculations will not solely be using the earnings reports from one company or industry. After the earnings dates, eps, and expected eps is pulled from the API call, it is stored in an earningsData object which possessed the date, eps, expected eps, revenue, and expected revenue for that specific earning call. A function in the software called FMPfindStockEarningData() returns a list of all earningsData object for further analysis.&lt;/p>
&lt;h3 id="53-calculations-and-results">5.3. Calculations And Results&lt;/h3>
&lt;p>The final and most complex area of the software’s processes include the calculations and results formulations functions. This is where all of the company’s stock data is computed to better understand the price action after the company exhibits an earnings call. This function formulates 10 main calculations listed in Table 1.&lt;/p>
&lt;p>The first set of calculation would be used to identify stocks that are projected to beat earnings based off the historical trend. Using a strategy like this is not recommended and most likely will not be very accurate as a trend does not always correlate to the company actually being profitable.&lt;/p>
&lt;p>&lt;strong>Table 1&lt;/strong>: Table shows the calculations that are to be performed on the SP500 dataset.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Calculation&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>A.&lt;/td>
&lt;td>% of (+) beat earnings when the stock is in an uptrend&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>B.&lt;/td>
&lt;td>% of (-) missed earnings when the stock is in a downtrend&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>These calculations find the likelihood of a stock’s price movement based on the company&amp;rsquo;s earnings results.
Stocks that perform well (or beat earnings) are typically looked at by investors as buying opportunities, and thus the security’s price increases. This is not always the case however, and the results of this will be shown later in the report. Sometimes, investors project the stock to beat earnings by more than others and in turn find even some positive earnings results bearish. This can cause major stockholder to sell their shares and bring the price down.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Calculation&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>C.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from open&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>D.&lt;/td>
&lt;td>% of (+) beat earnings, where price increase from the previous day’s close&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>E.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from open&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>F.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from the previous day’s close&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The final set of calculations the software is performing, looks at all parts of the stock and its trend. It identifies the likelihood of a stock price increasing due to (+) beat or (-) missed earnings, while it is in a specific type of trend reflective of the earnings direction. This can be used by investors to find stocks in an uptrend or downtrend, who also want to play the earnings direction and try to profit from it.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Calculation&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>G.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from open and is in a current uptrend&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>H.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from the previous day’s close and is in an uptrend&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>I.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from open and is in a current downtrend&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>J.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from the previous day’s close and is in a downtrend&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="6-the-results">6. The Results&lt;/h2>
&lt;p>The results here are caclulated based on the data gathered and calculated during SP500 companyies' earnings.&lt;/p>
&lt;h3 id="61-test-results---calculation-1">6.1 Test Results - Calculation 1&lt;/h3>
&lt;p>&lt;strong>Table 2&lt;/strong>: Table shows the results of the calculation on AAPL stock price going back 20 years.&lt;/p>
&lt;p>Stock Scanned: AAPL&lt;/p>
&lt;p>Caclulated using the code on: &lt;a href="https://github.com/cybertraining-dsc/fa20-523-336/blob/main/project/project.py">https://github.com/cybertraining-dsc/fa20-523-336/blob/main/project/project.py&lt;/a> API key required.&lt;/p>
&lt;ul>
&lt;li>Total Data Points Evaluated: 10,000&lt;/li>
&lt;li>Total Beat Earnings: 64 (84.2%)&lt;/li>
&lt;li>Total Missed Earnings: 12 (15.8%)&lt;/li>
&lt;li>Calculation Total in Seconds: 1&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Calculation&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Result&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>A.&lt;/td>
&lt;td>% of (+) beat earnings when the stock is in an uptrend&lt;/td>
&lt;td>84%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>B.&lt;/td>
&lt;td>% of (-) missed earnings when the stock is in a downtrend&lt;/td>
&lt;td>0%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from open&lt;/td>
&lt;td>40.63%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>D.&lt;/td>
&lt;td>% of (+) beat earnings, where price increase from the previous day’s close&lt;/td>
&lt;td>67.19%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>E.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from open&lt;/td>
&lt;td>50%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>F.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from the previous day’s close&lt;/td>
&lt;td>75%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>G.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from open and is in a current uptrend&lt;/td>
&lt;td>39.68%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>H.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from the previous day’s close and is in an uptrend&lt;/td>
&lt;td>66.67%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>I.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from open and is in a current downtrend&lt;/td>
&lt;td>0%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>J.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from the previous day’s close and is in a downtrend&lt;/td>
&lt;td>0%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="62-test-results---calculation-2">6.2 Test Results - Calculation 2&lt;/h3>
&lt;p>&lt;strong>Table 3&lt;/strong>: Table shows the results of the calculation on AAPL, MSFT, TWLO, GE, and NVDA&amp;rsquo;s stock price going back to their IPO. This caclulation gathers more data.&lt;/p>
&lt;p>Stocks Scanned: AAPL, MSFT, TWLO, GE, NVDA&lt;/p>
&lt;p>For the second results scan, 5 popular companies within the S&amp;amp;P500 were picked, in the technology and energy sector. This first test was meant to get a baseline of calculations of strong stocks in this index and get an estimated calculation time for the operation. After running the first calculation, the results were as followed. The total calculation time clocked in at just over 8 seconds for the 5 stock calculations (1.6 seconds/stock). At this rate, calculating all 500 stocks from the S&amp;amp;P500 should take around 13:20 minutes.&lt;/p>
&lt;p>From the 5 stocks scanned, there were over 36,170 data points evaluated, 222 earnings beats, and 98 earnings misses. When stocks are in a current uptrend, the company is expected to beat earnings expectations 78.6% of the time, and when the stock is in a current downtrend the company is expected to miss earnings 59.7% of the time. This means that if the stock is in an uptrend, investors predicting a company will beat earnings would be correct more than 3/4 of the time. Of the stocks evaluated, if the company beat earnings, the price would increase from the open 42.3% of the time, and increase from the past close 61.3% of the time. Additionally, if a company missed earnings expectations, the stock price closed below the open 59% of the time, and below the previous close 60% of the time. This leads to the prediction that investors are more concerned about the company missing earnings, rather than the company beating earnings. The company missing earning expectations is more detrimental to the stock price than the company beating earnings predictions. Lastly, of these 5 stocks, the price increases from open, when earnings have been beat and the stock is in an uptrend, 43.5% of the time. Notice since we added the uptrend filter on this scan, it results a higher calculation than calculation C but only slightly. Stocks that are in an uptrend and beat earnings, increase from the previous close 63.9% of the time. Again, these results are only slightly higher than calculation D. If the stock is in a downtrend and the company misses earnings, the stock price will decrease from the open 60.1% of the time and will decrease from the previous close 54.3% of the time. This means that stocks that are in a downtrend and miss earnings, tend to actually have a spike up in premarket hours (before open 9:30amET) and then crash further throughout the day, since the decrease from open % is greater than the decrease from past close %.&lt;/p>
&lt;p>Caclulated using the code on: &lt;a href="https://github.com/cybertraining-dsc/fa20-523-336/blob/main/project/project.py">https://github.com/cybertraining-dsc/fa20-523-336/blob/main/project/project.py&lt;/a> API key required.&lt;/p>
&lt;ul>
&lt;li>Total Data Points Evaluated: 36,170&lt;/li>
&lt;li>Total Beat Earnings: 222 (69.38%)&lt;/li>
&lt;li>Total Missed Earnings: 98 (30.62%)&lt;/li>
&lt;li>Calculation Total in Seconds: 8&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Calculation&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Result&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>A.&lt;/td>
&lt;td>% of (+) beat earnings when the stock is in an uptrend&lt;/td>
&lt;td>78.60%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>B.&lt;/td>
&lt;td>% of (-) missed earnings when the stock is in a downtrend&lt;/td>
&lt;td>59.74%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from open&lt;/td>
&lt;td>42.34%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>D.&lt;/td>
&lt;td>% of (+) beat earnings, where price increase from the previous day’s close&lt;/td>
&lt;td>61.26%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>E.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from open&lt;/td>
&lt;td>59.18%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>F.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from the previous day’s close&lt;/td>
&lt;td>60.2%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>G.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from open and is in a current uptrend&lt;/td>
&lt;td>43.46%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>H.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from the previous day’s close and is in an uptrend&lt;/td>
&lt;td>63.87%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>I.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from open and is in a current downtrend&lt;/td>
&lt;td>60.87%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>J.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from the previous day’s close and is in a downtrend&lt;/td>
&lt;td>54.35%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="63-full-results---calculation-3">6.3 Full Results - Calculation 3&lt;/h3>
&lt;p>This calculation find the test results based all stocks in the SP500.&lt;/p>
&lt;p>The final results scanned all stocks in the S&amp;amp;P500 and took roughly 17 minutes (1023 seconds in total) to complete. This was one of the things that originally was surprising as the first scan pointed toward the full calculations taking less time than it did. The total data points scanned totaled 3.7 million. From the results gathered on the full results pull, the changes between the first test scan and the full scan were identified. The calculation result percentages seemed to average out and begin to navigate towards the 50% (random) mark, although there were a few scan results that yielded some potential advantage across the board.&lt;/p>
&lt;p>Caclulated using the code on: &lt;a href="https://github.com/cybertraining-dsc/fa20-523-336/blob/main/project/project.py">https://github.com/cybertraining-dsc/fa20-523-336/blob/main/project/project.py&lt;/a> API key required.&lt;/p>
&lt;ul>
&lt;li>Total Data Points Evaluated: 3,704,001&lt;/li>
&lt;li>Total Beat Earnings: 20,789 (62.3%)&lt;/li>
&lt;li>Total Missed Earnings: 12,577 (37.7%)&lt;/li>
&lt;li>Calculation Total in Seconds: 1023 (17 minutes)&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Calculation&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Result&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>A.&lt;/td>
&lt;td>% of (+) beat earnings when the stock is in an uptrend&lt;/td>
&lt;td>61.6% (-17% difference)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>B.&lt;/td>
&lt;td>% of (-) missed earnings when the stock is in a downtrend&lt;/td>
&lt;td>34.54% (-25.2% difference)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from open&lt;/td>
&lt;td>51.8% (+9.46% difference)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>D.&lt;/td>
&lt;td>% of (+) beat earnings, where price increase from the previous day’s close&lt;/td>
&lt;td>56.74% (-4.52% difference)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>E.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from open&lt;/td>
&lt;td>50.92% (-8.26% difference)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>F.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from the previous day’s close&lt;/td>
&lt;td>56.92% (-3.28% difference)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>G.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from open and is in a current uptrend&lt;/td>
&lt;td>52.84% (+9.38% difference)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>H.&lt;/td>
&lt;td>% of (+) beat earnings, where price increases from the previous day’s close and is in an uptrend&lt;/td>
&lt;td>57.68% (-6.19% difference)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>I.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from open and is in a current downtrend&lt;/td>
&lt;td>53.76% (-7.11% difference)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>J.&lt;/td>
&lt;td>% of (-) missed earnings, where price decreases from the previous day’s close and is in a downtrend&lt;/td>
&lt;td>58.48% (-4.13% difference)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Although these results tend to show more randomness than the 5 scanned in the results of Calculation 1 in Section 6.1, there are a few scans that could yield a profitable and predictive strategy for investors, and/or provide some insight into what the price of a security may do. One area where the software is still able to predict events is in scan A, where we are evaluating the probability that the company will beat earning solely based on what the stock price trend is doing. If we only looked while investing in S&amp;amp;P500 stocks, an investor would be able to assume the company will beat earnings 61.6% of the time if the stock is above the 20 and 50 period moving averages. Of these times, the stock price will increase from the past close 57.68% of the time.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>To conclude, as more and more companies are evaluated in the software&amp;rsquo;s calculations, the chance of unusual and unique events increase. Theoretically, if a company outperforms their expected earnings and shows number better than what financial advisors predict the company to earn, investors should be encouraged to buy more shares in the company and in turn drive the price of the security higher. Sometimes however, stock prices act in the opposite effect during earnings times as large hedge funds and corporations sell off large volume shares of stock to fear other investors into selling. This can snowball the stock price down to where large institutions can repurchase massive amount of those shares again. This is usually refered to as market manipulation. (In a sort of dollar cost averaging method) Because of this, and many other market manipulation activities that occur on stock markets across the globe, positive earnings announcements do not always yield positive buyer sentiment and a price increase. Concluded from this research, it can be said that the strongest correlation to a stock beating earnings estimates, is the price trend of the security. If the stock is in a current uptrend, the chance of that security beating earnings is over 60% (based on SP500 Stock Calculations) With this, earnings announcements are something that many investors should and could look at while investing both short term and long term in companies, however, to develop a truly profitable trading strategy, more work and analysis would need to be conducted. The market moves in ways that few can acurately explain, and as more stocks are scanned and analyzed, the randomness and factor of luck began to show.&lt;/p>
&lt;h2 id="8-acknowledgements">8. Acknowledgements&lt;/h2>
&lt;p>Thank you to Dr. Gregor Von Laszewski, Dr. Geoffrey Fox and all other AI staff that helped with the planning and teaching of the I423 class during the COVID-19 pandemic through the fall 2020 semester. This class helped to better understand areas of big data and the science behind it. Additionally, the class gave participants the ability to learn more about their own chosen topic. Thank you.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2></description></item><item><title>Report: Project: Stock level prediction</title><link>/report/sp21-599-353/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/sp21-599-353/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-353/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-353/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/sp21-599-353/actions">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-353/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Rishabh Agrawal, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-353/">sp21-599-353&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/sp21-599-353/blob/main/project/index.md">Edit&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-353/blob/main/project/code/Stock%20Prediction.ipynb">Stock Prediction.pynb&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/sp21-599-353/blob/main/project/code/Stock%20Prediction.pdf">Stock Prediction.pdf&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>This project includes a deep learning model for stock prediction. It uses LSTM, RNN which is the standart for time series prediction.
It seems to be the right approach. The author really loved this project since he loves stocks.
He invests often, and is also in love with tech, so he finds ways to combine both of them. Most existing models for stock prediction
dont include the volume, and Rishabh intendede to use that as an input, but it didn&amp;rsquo;t go exactly as planned.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-existing-lstm-models">2. Existing LSTM Models&lt;/a>&lt;/li>
&lt;li>&lt;a href="#21-what-is-lstm">2.1. What is LSTM?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-existing-lstm-models-and-why-they-dont-do-great">2.2. Existing LSTM models and why they don&amp;rsquo;t do great&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-datasets">3. Datasets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-results">4. Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-benchmark">6. Benchmark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> tensorflow, LSTM, Time Series prediction, transformers.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Using deep learning for stock level prediction is not a new concept, but this project is trying to address a different issue her. Volume. Almost no model actually uses volume, daily volume or weekly volume.
People that are experts in this field and do a technical analysis(TA) of stocks use volume for their prediction extensively, so why not use it in the model? It could be ground breaking.&lt;/p>
&lt;p>LSTM is the obvious match for this kind of problem, but this project will also try to incorporate transformers in the model. The data set used will be from Yahoo Finance&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="2-existing-lstm-models">2. Existing LSTM Models&lt;/h2>
&lt;p>There are quite a few models out there for stock prediction. But what exactly is LSTM? How does it work, and how is it the obvious option here? There are other models too.
Why the current LSTM models aren&amp;rsquo;t that great? How good/bad do they perform?&lt;/p>
&lt;h2 id="21-what-is-lstm">2.1. What is LSTM?&lt;/h2>
&lt;p>LSTM is short for Long Short Term Memory networks. It comes under the branch of Recurrant Nueral Networks. LSTMs are explicitly designed to avoid the long-term dependency problem.
Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!
All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.
LSTM is also used for other time series forecasting such as weather and climate. It is an area of deep learning that works on considering the last few instances of the time series instead of the entire time series as a whole.
A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/sp21-599-353/raw/main/project/images/lstm.png" alt="LSTM">
&lt;br />Fig 1 LSTM&lt;/p>
&lt;p>This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn.&lt;/p>
&lt;h2 id="22-existing-lstm-models-and-why-they-dont-do-great">2.2. Existing LSTM models and why they don&amp;rsquo;t do great&lt;/h2>
&lt;p>This data corroborates what we can see from Figure 4. The low values in RMSE and decent values in R2 show that the LSTM may be good at predicting the next values for the time series in consideration.&lt;/p>
&lt;p>Figure 5 shows a sample of 100 actual prices compared to predicted ones, from August 13, 2018 to January 4, 2019.
&lt;a href="https://www.blueskycapitalmanagement.com/machine-learning-in-finance-why-you-should-not-use-lstms-to-predict-the-stock-market/">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-353/raw/main/project/images/lstm_pred.png" alt="LSTM prediction">&lt;/a>
&lt;br />Fig 5 LSTM Prediction 1 &lt;a href="https://www.blueskycapitalmanagement.com/machine-learning-in-finance-why-you-should-not-use-lstms-to-predict-the-stock-market/">Image Source&lt;/a>&lt;/p>
&lt;p>This figure makes us draw a different conclusion. While in aggregate it seemed that the LSTM is effective at predicting the next day values,
in reality the prediction made for the next day is very close to the actual value of the previous day.
This can be further seen by Figure 6, which shows the actual prices lagged by 1 day compared to the predicted price&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>&lt;a href="https://www.blueskycapitalmanagement.com/machine-learning-in-finance-why-you-should-not-use-lstms-to-predict-the-stock-market/">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-353/raw/main/project/images/lstm_pred2.png" alt="LSTM prediction2">&lt;/a>
&lt;br />Fig 6 LSTM Prediction 2 &lt;a href="https://www.blueskycapitalmanagement.com/machine-learning-in-finance-why-you-should-not-use-lstms-to-predict-the-stock-market/">Image Source&lt;/a>&lt;/p>
&lt;p>In one other model on Google Colab we see an output like this&lt;/p>
&lt;p>&lt;a href="https://colab.research.google.com/github/Mishaall/Geodemographic-Segmentation-ANN/blob/master/Google_Stock_Price_Prediction_RNN.ipynb#scrollTo=skhdvmCywHrr">&lt;img src="https://github.com/cybertraining-dsc/sp21-599-353/raw/main/project/images/lstm_pred3.png" alt="LSTM prediction3">&lt;/a>
&lt;br />Fig 7 LSTM Prediction 3 &lt;a href="https://colab.research.google.com/github/Mishaall/Geodemographic-Segmentation-ANN/blob/master/Google_Stock_Price_Prediction_RNN.ipynb#scrollTo=skhdvmCywHrr">Image Source&lt;/a>&lt;/p>
&lt;p>Here we can see that clearly the model did not do well. In this model, the LSTM didn&amp;rsquo;t even get the trends correctly. There is definitely need for some changes in the layers, and droupout coeffecient.
My guess is that this model was really overfitted since the first half of the prediction did reletively well&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-datasets">3. Datasets&lt;/h2>
&lt;p>As mentioned above, the Yahoo finance Data set will be used. It is really easy to get. Data can be download from any time stamp to as new as today. There is a download csv button to do so.
Here is an example of &lt;a href="https://finance.yahoo.com/quote/aapl/history?ltr=1">AAPL stock&lt;/a> on Yahoo Finance&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>For this project the Amazon stock was chosen. The all time max historic data was downloaded from Yahoo Finance in csv format. Here is the &lt;a href="https://github.com/cybertraining-dsc/sp21-599-353/blob/main/project/code/AMZN_Stock_Price.csv">link&lt;/a>&lt;/p>
&lt;h2 id="4-results">4. Results&lt;/h2>
&lt;p>Fig 8 shows the result of my model. The results weren&amp;rsquo;t as expected. This project was meant to be for adding volume to the input layer. We tried to do that in the model, but it failed. It gave a straight
line for the prediction. This project did find a unique way to use the LSTM. It played around with the layers, number of layers, the droupout coeffecient to find the most accurate balance. The output shows a more
reliable and believable output, and that is some progress. There might be ways to incorporate volume. Later, maybe it needs better scaling. But the prediction did get the trends pretty accurately, even
though it might not have gotten the exact price correctly. Amazon stock alse soared unbelievabley this year due to COVID-19 and many other external factors that were not incorporated in the model at
all, so it is really common to see an undervalued prediction.&lt;br />
&lt;img src="https://github.com/cybertraining-dsc/sp21-599-353/raw/main/project/images/AMZN_stock_prediction_graph.png" alt="LSTM results">
&lt;br />Fig 8&lt;/p>
&lt;h2 id="6-benchmark">6. Benchmark&lt;/h2>
&lt;p>Cloudmesh was used for the benchmark for this project. According to the documentaion&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, I used the StopWatch.start() and StopWatch.stop() functions. Fig 9 shows the output. Loading the dataset or prediction of the data doesn&amp;rsquo;t take long at all. The majority of the time is taken for the training
wihch is expected. Google Colab was the tool used and utilized the GPU which is why each epoch just took about 2 seconds. When a personal CPU or the default CPU provided by Google Colab was used,
it took about 30-40 seconds for each epoch. You can see the system configuration in Fig 9 too.
&lt;img src="https://github.com/cybertraining-dsc/sp21-599-353/raw/main/project/images/benchmark.png" alt="Cloudmesh Benchmark">
&lt;br />Fig 9 Cloudmesh Benchmark Results&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>Even though the model did not do as expected, we were not able to add volume as an input, we were still able to find some success with changing the number layers and the coeffecient values. We can see
that the model can successfully predict the trend of the stock prices, even with the external factors affecting the prices a little. The next step for this project would be to try to scale the volume
and add it as an input to the model. One other, but rather difficult add-on could be to try to add some external factors as inputs. For an aggregate input of external factors, we could use
sentiment analysis through Twitter tweets as an input to the model too.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;p>Your report must include at least 6 references. Please use customary academic citation and not just URLs. As we will at
one point automatically change the references from superscript to square brackets it is best to introduce a space before
the first square bracket.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Using dataset for stocks, [Online resource]
&lt;a href="https://finance.yahoo.com/">https://finance.yahoo.com/&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Understanding LSTM Networks, [Online Resource]
&lt;a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>AI stock market forecast, [Online resource]
&lt;a href="http://ai-marketers.com/ai-stock-market-forecast/">http://ai-marketers.com/ai-stock-market-forecast/&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Why You Should Not Use LSTM’s to Predict the Stock Market, [Online resource]
&lt;a href="https://www.blueskycapitalmanagement.com/machine-learning-in-finance-why-you-should-not-use-lstms-to-predict-the-stock-market/">https://www.blueskycapitalmanagement.com/machine-learning-in-finance-why-you-should-not-use-lstms-to-predict-the-stock-market/&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Google Stock Price Prediction RNN, [Google Colab]
&lt;a href="https://colab.research.google.com/github/Mishaall/Geodemographic-Segmentation-ANN/blob/master/Google_Stock_Price_Prediction_RNN.ipynb#scrollTo=skhdvmCywHrr">https://colab.research.google.com/github/Mishaall/Geodemographic-Segmentation-ANN/blob/master/Google_Stock_Price_Prediction_RNN.ipynb#scrollTo=skhdvmCywHrr&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Cloudmesh, [Documentation]
&lt;a href="https://cloudmesh.github.io/cloudmesh-manual/autoapi/cloudmeshcommon/cloudmesh/common/StopWatch/index.html#module-cloudmesh-common.cloudmesh.common.StopWatch">https://cloudmesh.github.io/cloudmesh-manual/autoapi/cloudmeshcommon/cloudmesh/common/StopWatch/index.html#module-cloudmesh-common.cloudmesh.common.StopWatch&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Analysis of Financial Markets based on President Trump's Tweets</title><link>/report/fa20-523-307/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-307/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-307/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-307/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-307/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-307/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Alex Baker, fa20-523-307, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-307/blob/master/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>President Trump has utilized the social media platform Twitter as a way to convey his message to the American people. The tweets he has published during his presidency cover a vast array of topics and issues from MAGA rallies to impeachment. This analysis investigates the relationship of the NASDAQ and the sentiment of President Trump&amp;rsquo;s tweets during key events in his presidency. NASDAQ data was gathered though Yahoo Finance&amp;rsquo;s API while President Trump&amp;rsquo;s tweets were gathered from Kaggle. The results observed show that during certain events, a correlation emerges of the NASDAQ data and the sentiment of President Trump&amp;rsquo;s tweets.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-datasets">2. DataSets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-data-cleaning-and-preprocessing">3. Data Cleaning and Preprocessing&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-twitter-data">3.1 Twitter Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-stock-data">3.2 Stock Data&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-methodologyprocess">4. Methodology/Process&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-preliminary-analysis-and-eda">5. Preliminary Analysis and EDA&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#51-twitter-data">5.1 Twitter Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-stock-data">5.2 Stock Data&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-defining-events-during-trumps-presidency">6. Defining events during Trump&amp;rsquo;s presidency&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#61-the-impeachment-of-president-trump">6.1 The Impeachment of President Trump&lt;/a>&lt;/li>
&lt;li>&lt;a href="#62-the-dakota-access-and-keystone-xl-pipelines-approval">6.2 The Dakota Access and Keystone XL pipelines approval&lt;/a>&lt;/li>
&lt;li>&lt;a href="#63-the-government-shutdown">6.3 The Government Shutdown&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> analysis, finance, stock markets, twitter, politics&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Financial markets have been an area of research in both academia and business. Analysis and predictions has been growing in its accuracy with an every increasing amount of data used to test these models. &amp;ldquo;The Efficient Market Hypothesis (EMH) states that stock market prices are largely driven by &lt;em>new&lt;/em> information and follow a random walk pattern&amp;rdquo;&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This shows that prices will follow news rather than previous and present prices. Information is unpredictable in terms of its release/publication showing market prices will follow a random walk pattern and the prediction can not be high.&lt;/p>
&lt;p>There are some problems that arise with EMH. One problem is that &amp;ldquo;stock prices does not follow a random walk pattern and can be predicted to a certain degree&amp;rdquo;&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Another problem associated with EMH is with the information&amp;rsquo;s unpredictability, the unpredictability is called into question with the introduction of social media (Facebook, Twitter, blogs). The rise of social media can be a early indicator for news before it is released/published. This project will analyze the market based on how the President tweets during certain events.&lt;/p>
&lt;h2 id="2-datasets">2. DataSets&lt;/h2>
&lt;p>In this project, two datasets will be used -&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The NASDAQ values from November 2016 to January 2020. This data was obtained through Yahoo! Finance and includes Date, Open, High, Low, Close, Adj Close, and Volume for a given day.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>President Trump&amp;rsquo;s tweets during the periods of November 2016 to January 2020 is over 41,000 tweets. The data includes id, link, content, date, retweets, favorites, mentions, hashtags, and geo for every tweet in the time frame. Since the performance of the analysis is on a daily basis, tweets will be split up by Date. This data is available on Kaggle (&lt;a href="https://www.kaggle.com/austinreese/trump-tweets?select=trumptweets.csv)">https://www.kaggle.com/austinreese/trump-tweets?select=trumptweets.csv)&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>To strengthen the analysis, even more, some code from the 2016 election’s analysis of markets may be utilized but the focus will be on the markets during the Trump administration. Rally data maybe introduced in order to have a deeper sense of some of the tweets when it comes to important news that is announces at President Trump&amp;rsquo;s rallies. In order to have a realistic and strong analysis, the financial data needs to be aligned with the timing of tweets but news that has already started to affect the markets before a tweet has been sent out needs to be taken into account.&lt;/p>
&lt;h2 id="3-data-cleaning-and-preprocessing">3. Data Cleaning and Preprocessing&lt;/h2>
&lt;p>The data required for this project is stock market data and Twitter data from President Trump. Stock market data was collected from Yahoo Finance&amp;rsquo;s API. This data was saved to a CSV file then imported using Pandas. The Twitter data was collected by a Kaggle user and is imported though Kaggle&amp;rsquo;s API or through the use of a local copy saved from the site. The data obtained needs to be cleaned and pre-processed in order to make it reliable for analysis through the use of Pandas, Regex, and Matplotlib.&lt;/p>
&lt;h3 id="31-twitter-data">3.1 Twitter Data&lt;/h3>
&lt;p>When importing the Twitter data, there are several things that are noticed when printing the first five rows. Three of the columns mention, hashtags, and geo are currently showing NaN. After calculating the missing values, all the values in these columns are missing or are zero so we can drop these columns from the dataframe.&lt;/p>
&lt;p>The tweets are one of the last columns needed to be cleaned. The text of the tweets needs to be uniformed in order to conduct analysis. Removing punctuations was the first step followed by removing content specifically seen in tweets. These could be the word retweet, the hashtag symbol(#), the @ symbol followed by a username, and any hyperlinks that could be in a tweet.&lt;/p>
&lt;h3 id="32-stock-data">3.2 Stock Data&lt;/h3>
&lt;p>Stock data has a unique set of challenges when it comes to cleaning. Unlike tweets, stock data is only available Monday through Friday and is not available for holidays that the market is closed. In order to have a complete dataset, several options are available. One option is to drop the tweets that fall on a weekend. This would not be useful since markets can react to news that happens on the weekend. Another option is that &amp;ldquo;if the NASDAQ value on a given day is x and the next available data point is y with n days missing in between, we approximate the missing data by estimating the first day after x to be (y+x)/2 and then following the same method recursively till all gaps are filled&amp;rdquo; &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-methodologyprocess">4. Methodology/Process&lt;/h2>
&lt;p>The collection of finance and Twitter data will be used to visualize the results. Some of Twitter or dataset data will need to be cleaned and classified to build the model. The methodology is composed of the following steps:&lt;/p>
&lt;ul>
&lt;li>Use data from President Trump&amp;rsquo;s personal twitter and data from Yahoo Finance API to help visualize&lt;/li>
&lt;li>Data cleaning and extraction&lt;/li>
&lt;li>Sentiment Analysis&lt;/li>
&lt;/ul>
&lt;p>Sentiment analysis is a key component to categorize President Trump&amp;rsquo;s tweets. Polarity and subjectivity are the two metrics that are used to classify each tweet. Polarity measures the opinion or emotion expressed in a piece of text; the value is returned as a float within the range of -1.0 to 1.0. Subjectivity, on the other hand, reflects the feelings or beliefs in a piece of text; the value is returned as a float within the range 0.0 to 1.0 where 0.0 is very objective and 1.0 is very subjective. TextBlob is the library utilized for processing the tweet&amp;rsquo;s polarity and subjectivity. The sentiment method is called along with the methods for polarity and subjectivity in their own functions. The returned values are added into two columns in the dataframe.&lt;/p>
&lt;p>The plot used for the sentiment analysis is a scatter plot. This will allow for each tweet to be plotted with their respected polarity and subjectivity. A line plot is the best plot to used in order to easily visualize market price verses the sentiment of the tweets. In plotting the line for the sentiment of tweets, several issues arise. The first major issues is that multiple tweets are published on a given day with varying degrees of sentiment. The line graph will display a vertical line for all the points represented. One solution is to take the average of the days tweets and use this new value on the graph. This method can be aligned on the same axes as the stock market data.&lt;/p>
&lt;h2 id="5-preliminary-analysis-and-eda">5. Preliminary Analysis and EDA&lt;/h2>
&lt;h3 id="51-twitter-data">5.1 Twitter Data&lt;/h3>
&lt;p>When starting to conduct preliminary analysis and exploratory data analysis (EDA), it is helpful to first check for any null values in the data and there are no null values in the twitter data.&lt;/p>
&lt;p>The date column is a column that is needed to track the amount of tweets per month and year. In the column, the timestamp and the date are combined so this need to be separated in several ways. The first being separating the date from the timestamp into its own column. This is followed up by separating the date into 4 columns for day, month, year and month-year in order to track tweets based on specified criteria.&lt;/p>
&lt;p>After graphing the amount of tweets per year, the observation is that 2016 and 2020 have a low tweet count. The reminder is that the data starts in November 2016 making 2016 have two months of data compared to 2020 with only one month being January. From 2017 through 2019, we can see that the amount of tweets increases by almost a thousand every year. The tweets per month tell a different story. The amount varies greatly over the years with the greatest amount being near the end of 2016 and the beginning of 2017. The sentiment of the tweets show that a majority of the tweets are a little skewed to the right of the graph. This shows that may of the President tweets are positive in some aspect as well as have a personal opinion, emotion or judgement.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/year_tweets.png" alt="Figure 1: Number of Tweets per Year">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Number of Tweets per Year&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/month_tweets.png" alt="Figure 2: Number of Tweets per Month">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Number of Tweets per Month&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/sentiment.png" alt="Figure 3: Sentiment Analysis of Tweets">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Sentiment Analysis of Tweets&lt;/p>
&lt;h3 id="52-stock-data">5.2 Stock Data&lt;/h3>
&lt;p>Similar to the twitter data, checking for null values is important but since the data is from Yahoo! Finance there are no missing values on the days that the markets are opened.&lt;/p>
&lt;p>Once graphing the open and closed prices of the NASDAQ, there seems to be an general upwards trend in the market over the time period.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/market.png" alt="Figure 4 Open and Close Price of the NASDAQ">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Open and Close Price of the NASDAQ&lt;/p>
&lt;h2 id="6-defining-events-during-trumps-presidency">6. Defining events during Trump&amp;rsquo;s presidency&lt;/h2>
&lt;h3 id="61-the-impeachment-of-president-trump">6.1 The Impeachment of President Trump&lt;/h3>
&lt;p>After weeks of talks among Congress, the House of Representatives have voted to impeach President Trump on two charges: abuse of power and obstruction of Congress on December 18, 2019. Since the country&amp;rsquo;s founding in 1776, only three presidents have faced impeachment from Congress: Andrew Johnson, Bill Clinton and now Donald Trump. This move has been widely advocated for since his election in 2016. &amp;ldquo;In September 2019, news leaked of a phone call between President Trump and Ukrainian President Volodymyr Zelensky regarding an investigation into Hunter Biden, son of then Democratic candidate Joe Biden, for his dealings in Ukraine&amp;rdquo; &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The sentiment analysis preformed on tweets that are published at the start of November though the end of 2019. The graph shows a steady amount of subjectivity and polarity through much of November. In the final week of November, when the House Intelligence Committee&amp;rsquo;s public hearings were concluding, a large spike in subjectivity as well as polarity shows that President Trump was trying to discredit the individuals testifying, declaring the whole impeachment a witch hunt, or making a case of all the good he has done for teh country. The NASDAQ shows that the opening price went down when the Articles of Impeachment were announced but when the House voted to approve the articles started in mid December, the stock price is on an upward trend. The daily change points out that the price of the stock was fluctuating quite a bit during the initial stages of the impeachment hearing as well as the House vote on the Articles of Impeachment.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/impeachment_sentiment.png" alt="Figure 5: Sentiment Analysis during Impeachment">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Sentiment Analysis during Impeachment&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/impeachment_stock.png" alt="Figure 6: Open and Daily Change Price during Impeachment">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Open and Daily Change Price during Impeachment&lt;/p>
&lt;h3 id="62-the-dakota-access-and-keystone-xl-pipelines-approval">6.2 The Dakota Access and Keystone XL pipelines approval&lt;/h3>
&lt;p>One of the first moves President Trump made when arriving into office was to approve the Dakota Access and Keystone XL pipelines. &amp;ldquo;Both of the pipelines were blocked by the Obama administration due to environmental concerns, but President Trump has questioned climate change and promised to expand energy infrastructure and create jobs&amp;rdquo;&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The Keystone pipeline would span 1,200 miles across six states, moving over 800,000 barrels of oil daily from Canada to the Gulf coast. The Dakota Access pipeline would move oil from North Dakota all the way to Illinois. &amp;ldquo;The Standing Rock Sioux tribe, whose reservation is adjacent to the pipeline, staged protests that drew thousands of climate change activists to the rural area of Cannon Ball, North Dakota&amp;rdquo; &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The sentiment analysis shows a high polarity and subjectivity three days before the signing of the presidential memorandum but drops sharply during the event. Subjectivity has a quick resurgence but polarity stays low as time goes on. In the days leading up to the signing of the pipelines on January 24th, the opening price has an upward trend and stays fairly consistent in the following days. Daily change has a big jump at the end of January but falls dramatically and does not reach the level it was once at, this could be due to the protests that followed the approval or companies reevaluating their positions on the pipeline.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/dakota_sentiment.png" alt="Figure 7: Sentiment Analysis during Dakota Approval">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Sentiment Analysis during Dakota Approval&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/dakota_stock.png" alt="Figure 8: Open and Daily Change Price during Dakota Approval">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Open and Daily Change Price during Dakota Approval&lt;/p>
&lt;h3 id="63-the-government-shutdown">6.3 The Government Shutdown&lt;/h3>
&lt;p>On December 21, 2018 the United States Government shutdown. &amp;ldquo;At the heart of the dispute is Trump&amp;rsquo;s demand for just over $5 billion toward a long-promised wall along the US-Mexico border&amp;rdquo; &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The shutdown affected a part of the federal government such as homeland security, transportation, and agriculture. &amp;ldquo;The problems caused by the shutdown are wide-ranging, from waste piling up in national parks to uncertainty for 800,000 federal workers about when their next paycheck will come&amp;rdquo; &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. This shutdown was the longest shutdown in the modern era coming to an end on January 25, 2019 after 35 days. The sentiment analysis tells that the tweets shared during the shutdown are lower in terms of polarity with a majority of tweets being higher in subjectivity. A interesting note is that subjectivity on the day of the shutdown dropped to zero but shot up quickly the next day. There was no significant movement during the whole shutdown until the start of February when polarity soared and subjectivity dropped. A potential reason why there was no significant movement in the analysis was that the president wanted to get his piece of legislation through Congress but Congress was not going to approve his legislation. Prior to the government shutdown, the opening prices fell by 10 dollars with the lowest being around the time the shutdown began. The new year shows stock on a steady increase during the month of January, when the shutdown was lifted. Daily change was on the rise when the shutdown began but swiftly dropped at the start of the new year but recovered and stayed relatively stabled throughout the shutdown.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/shutdown_sentiment.png" alt="Figure 9: Sentiment Analysis during the Government Shutdown">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> Sentiment Analysis during the Government Shutdown&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-307/main/project/images/shutdown_stock.png" alt="Figure 10: Open and Close Price during the Government Shutdown">&lt;/p>
&lt;p>&lt;strong>Figure 10:&lt;/strong> Open and Close Price during the Government Shutdown&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>The investigation showed the relation between President Trump&amp;rsquo;s tweets and the NASDAQ during certain events. The results pointed that a majority of tweets were positive in polarity with subjectivity being higher or sometimes lower depending on the event. The NASDAQ had some interesting reactions based on the events. In highly important events, the stock price tended to have an upward trajectory but leading up to the event the price would go down. These results show that the content of the President&amp;rsquo;s tweets have some impact in terms of the market movements, but many factors go into the price of the market such as foreign relations and how companies are preforming. Finally, it is worth mentioning that the analysis doesn’t take into account some factors. Weekends were a factor that was not included into the stock market data. Tweets from the President&amp;rsquo;s official account were not taken into account in this analysis. All of these remaining areas can be added in future research.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Goel, A. and Mittal, A., 2011. Stock Prediction Using Twitter Sentiment Analysis. [online] cs229.stanford.edu. Available at: &lt;a href="http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf">http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>J. Bollen, H. Mao, and X. Zeng, Twitter mood predicts the stock market. Journal of Computational Science, vol. 2, no. 1, pp. 1–8, 2011. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>President Donald Trump impeached, History.com, 05-Feb-2020. [online]. Available at: &lt;a href="https://www.history.com/this-day-in-history/president-trump-impeached-house-of-representatives">https://www.history.com/this-day-in-history/president-trump-impeached-house-of-representatives&lt;/a>. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>D. Smith and A. Kassam, Trump orders revival of Keystone XL and Dakota Access pipelines, The Guardian, 24-Jan-2017. [online]. Available at: &lt;a href="https://www.theguardian.com/us-news/2017/jan/24/keystone-xl-dakota-access-pipelines-revived-trump-administration">https://www.theguardian.com/us-news/2017/jan/24/keystone-xl-dakota-access-pipelines-revived-trump-administration&lt;/a>. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Bryan, B., The government shutdown is now the longest on record and the fight between Trump and Democrats is only getting uglier. Here&amp;rsquo;s everything you missed. 21-Jan-2019. [online]. Available at: &lt;a href="https://www.businessinsider.com/government-shutdown-timeline-deadline-trump-democrats-2019-1">https://www.businessinsider.com/government-shutdown-timeline-deadline-trump-democrats-2019-1&lt;/a>. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item></channel></rss>