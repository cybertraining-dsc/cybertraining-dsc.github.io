<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cybertraining – sports</title><link>/tags/sports/</link><description>Recent content in sports on Cybertraining</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Mon, 15 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/sports/index.xml" rel="self" type="application/rss+xml"/><item><title>Report: How Big Data has Affected Statistics in Baseball</title><link>/report/fa20-523-328/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-328/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-328/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-328/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-328/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-328/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: report&lt;/p>
&lt;p>Holden Hunt, &lt;a href="mailto:holdhunt@iu.edu">holdhunt@iu.edu&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-328/">fa20-523-328&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-328/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The purpose of this report is to highlight how the inception of big data in baseball has changed the way baseball is played and how it affects the choices managers make before, during, and after a game. It was found that big data analytics can allow baseball teams to make more sound and intelligent decisions when making calls during games and signing contracts with free agent and rookie players. The significance of this project and what was found was that teams that adopt the &lt;em>moneyball mentality&lt;/em> would be able to perform at much higher levels than before with a much lower budget than other teams. The main conclusion from the report was that the use of data analytics in baseball is a fairly new idea, but if implemented on a larger scale than only a couple of teams, it could greatly change the way baseball is played from a managerial standpoint.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-dataset">2. Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-background">3. Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-big-data-in-baseball">4. Big Data in Baseball&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-conclusion">5. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-acknowledgements">6. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-references">7. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> sports, data analysis, baseball, performance&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Whenever people talk about sports, they will always talk about some kind of statistic to show that their team is performing well, or certain player(s) are playing incredibly well. This is due to the fact that statistics has become extremely important in sports, especially in rating an entity’s performance. While essentially every sport has adopted statistics to quantify performance, baseball is the most well-known sport to use it, due to the obscene number of stats that are tracked for each player and team, as well as the sport that uses stats the most in how they play the game. The MLB publishes about 85 different statistics for individual players, including the stats tracked of the teams, there is likely to be about double the amount if tracked statistics for the sport of baseball. The way that all the statistics are calculated, is, of course, by analyzing big data found from the players and teams. This report will mainly talk about the history of big data and data analytics in baseball, what the data is tracking, what we can learn from the data, and how the data is used.&lt;/p>
&lt;h2 id="2-dataset">2. Dataset&lt;/h2>
&lt;p>The dataset that will be analyzed in this report will be the Lahmen Sabermetrics dataset. This dataset is a large dataset curated by Sean Lahmen, which contains baseball data starting from the year 1871, which is when the Major League Baseball association was founded &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The dataset contains data for many different types of statistics, including batting stats, fielding stats, pitching stats, awards stats, player salaries, and games they played in. The data for this dataset has statistics from the last season that occurred (2020 season), but the data that could be accessed for this report is from 1871-2015. I plan to use this dataset for discussing later how the data in sets like this is used for statistical analysis in baseball and how teams can use this to their advantage.&lt;/p>
&lt;h2 id="3-background">3. Background&lt;/h2>
&lt;p>The concept of baseball has been a sport that has existed for centuries, but the actual sport called baseball started in early to mid 1800s. Baseball became popularized in the United States in the 1850s, where a baseball craze hit the New York area, and baseball quickly was named a &lt;em>national pastime.&lt;/em> The first professional baseball team was the Cincinnati Red Stockings, which was established in 1869, and the first professional league was established in 1871, and was called the National Association of Professional Base Ball Players. This league only lasted a few years and was replaced by a more formally structured league called the National League in 1876, and the American League was established in 1901 from the Western League. A vast majority of the modern rules of baseball were in place by 1893, and the last major change was instituted in 1901 where foul balls are counted as strikes. The World Series was inaugurated in the fall of 1903, where the champion of the National League would play against the champion of the American League. During this time, there were many problems with the league, such as strikes due to poor and unequal pay and the discrimination of African Americans. The era in the time of the early 1900s was the first era of baseball, and the second era of baseball started in the 1920s where a plethora of changes to the game caused the sport to move from more of a &lt;em>pitcher’s game&lt;/em> to a &lt;em>hitter’s game,&lt;/em> which was emphasized by the success of the first &lt;em>power hitter&lt;/em> in professional baseball, Babe Ruth. In the 1960s, baseball was losing revenue due to the rising popularity of football, the league had to make changes to combat this drop in revenue and popularity. The changes lead to the salaries of the players getting increased and also an increase in attendance to games, which means increased revenue &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Big data is able to be used in baseball through the use of statistics, which has become a major part in how the sport is played. The use of statistics in baseball has become known as sabermetrics, which can be used by teams to make calls for the game based on numbers. The idea of using sabermetrics started from the book called &lt;em>Moneyball&lt;/em>. The book was published in 2003, and was about the Oakland Athletics baseball team and how they were able to use sabermetric analysis to compete on equal grounds with teams that had much more money and good players than the A&amp;rsquo;s team &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. This book has had a major impact on the way baseball is played today for several teams. For example, teams like the New York Yankees, the St. Louis Cardinals, and the Boston Red Sox have hired full-time sabermetric analysts in attempts to gain an edge over other teams by using these sabermetrics to influence their decisions. Also, the Tampa Bay Rays were able to make the &lt;em>moneyball&lt;/em> idea a reality by making it to the 2020 World Series with a much lower budget team and using lots of sabermetrics in their decision-making &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. This &lt;em>moneyball strategy&lt;/em> is not the perfect strategy however, because the Rays lost the World Series, and the turning point for their loss could be pointed to a decision they made based on what the analytics said they should do, which ended up being the wrong choice, which lost them a crucial game in the series.&lt;/p>
&lt;h2 id="4-big-data-in-baseball">4. Big Data in Baseball&lt;/h2>
&lt;p>The data that is being analyzed for the statistics in baseball are datasets similar to the one this report is looking into, the Lahmen Sabermetrics dataset. Since this dataset contains a vast amount of data for each player and many different tables containing many different kinds of data, many kinds of statistics are able to be tracked for each player. With this large amount of statistics, teams are able to look to numbers and analytics in order to make the best decision on the actions to make during a game. Teams can also use analytic technology to predict the performance of a player based on their previous accomplishments and comparing that to similar players and situations in the past &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>This kind of analysis can be used to gauge the potential performance of a free-agent or rookie player, as well as deciding what player should be in the starting lineup for the upcoming game(s). Since these sabermetric analyses are able to predict the performance of a player in the coming years, they are able to tell if a contract made by a team is likely to not be a smart deal since they tend to make long-term deals for lots of money, even though it is likely that the player will not continue to perform at the same level they are currently at for the entirety of their contract. This is normally due to the inability to play at an incredibly high level consistently for a long period of time and regression of performance from age, which is shown to start occuring at around 32 years old &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This simply means that large contracts have a trend of being a large loss of money in the long run shown from analysis of similar types of contracts in the past. This kind of analysis also allows for a players ranking to be deciding by more areas than before. For example, a player&amp;rsquo;s offensive capabilities can be shown by looking at more categories than the amount of home runs hit and batting average, they can also look at baserunning skill, slugging percentage, and overall baseball intelligence. This ability of looking at a player&amp;rsquo;s overall capabilities in a more analytical manner allows teams to not throw all their budget into one or two top prospected players, but can spread their money across several talented players to have a good and balanced team. Another reason why deciding to not spend lots of money on a long contract for a top prospected player is that the analysis shows that players have started to have shorter lengths of time where they are able to perform at their best, even though other sports have seen the opposite in recent years . However, young players have been performing at a much higher level in recent years and they have had younger players moving from the minor to the major league much faster than before &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The data that is presented in the Lahman Sabermetrics database and other similar databases is able to allow analysts to compare data and statistics of one team with any/every other team with relative ease and in an easy to understand way. For example, the comparative analysis Figure 1 below shows that the payroll of teams and their winning percentage, analysts are able to learn that the New York Yankees have a much higher payroll than all other teams and they have a very good win rate, but there are other teams that do have very high payrolls and have the same good rate rate. Also, Figure 1 shows that there are other teams that have higher payrolls than average, but have a very bad win rate compared to all other teams, including teams that have a much lower payroll &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. This kind of analysis shows us that spending lots of money does not guarantee a strong season, which can strengthen the idea of the &lt;em>moneyball strategy&lt;/em> coined earlier where teams attempt to waste less money by spreading budgets across several players other than spending most of the budget on only one or two players.&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Comparative Analysis of Payroll to Win Percentage &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>
&lt;img src="https://github.com/cybertraining-dsc/fa20-523-328/raw/main/report/images/spending_graph.jpeg" alt="Comparative Analysis of Payroll to Win Percentage Graph">&lt;/p>
&lt;p>This is only one way that the Lahman Sabermetrics dataset can be used, but there are many more ways this data can be used to make league wide analyses and compare a certain team to others. This can be used by teams to possibly learn what they might be doing wrong if they feel as though they should be performing better.&lt;/p>
&lt;h2 id="5-conclusion">5. Conclusion&lt;/h2>
&lt;p>This report discusses the history of baseball and how big data analytics came to be prevalent in the sport, as well as how big data is used in baseball and what can be learned from the use of it so far. Big data is able to be used to make decisions that could greatly benefit a team from saving money on a contract with a player to making a choice during a game. Big data analytics use in baseball is a fairly new occurrence, but due to the advantages a team can gain from using analytics, it is likely that use of it will increase soon in the future.&lt;/p>
&lt;h2 id="6-acknowledgements">6. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the FA20-BL-ENGR-E534-11530: Big Data Applications course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="7-references">7. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Lahman, Sean. &amp;ldquo;Lahman&amp;rsquo;s Baseball Database - Dataset by Bgadoci.&amp;rdquo; Data.world, 5 Oct. 2016. &lt;a href="https://data.world/bgadoci/lahmans-baseball-database">https://data.world/bgadoci/lahmans-baseball-database&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Wikipedia. &amp;ldquo;History of Baseball.&amp;rdquo; Wikipedia, Wikimedia Foundation, 27 Oct. 2020. &lt;a href="https://en.wikipedia.org/wiki/History_of_baseball">https://en.wikipedia.org/wiki/History_of_baseball&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Wikipedia. &amp;ldquo;Moneyball.&amp;rdquo; Wikipedia, Wikimedia Foundation, 28 Oct. 2020. &lt;a href="https://en.wikipedia.org/wiki/Moneyball">https://en.wikipedia.org/wiki/Moneyball&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Wharton University of Pennsylvania. &amp;ldquo;Analytics in Baseball: How More Data Is Changing the Game.&amp;rdquo; Knowledge@Wharton, 21 Feb. 2019. &lt;a href="https://knowledge.wharton.upenn.edu/article/analytics-in-baseball/">https://knowledge.wharton.upenn.edu/article/analytics-in-baseball/&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Tibau, Marcelo. &amp;ldquo;Exploratory data analysis and baseball.&amp;rdquo; Exploratory Data Analysis and Baseball, 3 Jan. 2017. &lt;a href="https://rstudio-pubs-static.s3.amazonaws.com/239462_de94dc54e71f45718aa3a03fc0bcd432.html">https://rstudio-pubs-static.s3.amazonaws.com/239462_de94dc54e71f45718aa3a03fc0bcd432.html&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Predictive Model For Pitches Thrown By Major League Baseball Pitchers</title><link>/report/fa20-523-343/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-343/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-343/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-343/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-343/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-343/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: report&lt;/p>
&lt;p>Bryce Wieczorek, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-343">fa20-523-343&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-343/blob/main/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The topic of this review is how big data analysis is used in a predictive model for classifying what pitches are going to be thrown next. Baseball is a pitcher’s game, as they can control the tempo. Pitchers have to decide what type of pitch they want to throw to the batter based on how their statistics compare to that of the batters. They need to know what the batter struggles to hit against, and where in the strike zone they struggle the most.&lt;/p>
&lt;p>With the introduction of technology into sports, data scientists are sliding headfirst into Major League Baseball. And with the introduction of Statcast in 2015, The MLB has been looking at different ways to use technology in the game. In 2020 alone, the MLB introduce several different types of technologies to keep the fans engaged with the games while not being able to attend them [^3]. In this paper, we will be exploring a predictive model to determine pitches thrown by each pitcher in the MLB. We will be reviewing several predictive models to understand how this can be done with the use of big data.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-and-previous-work">2. Background and Previous Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#21-harvard-college-model">2.1 Harvard College Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#22-north-carolina-state-university-model">2.2 North Carolina State University Model&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-search-and-analysis">4. Search and Analysis&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-limitations">5. Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8 References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Pitch type, release speed, release spin, baseball, pitchers, MLB&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big data is everywhere and has been used in sports for years. Especially in Major League Baseball, where big data has been analyzed for various things. For example, in the movie Moneyball (which is based on true events), the general manager of the Oakland A’s uses analytics to find players that will win them games that other teams overlook. They used many different statistics; batting average, batting average with runners on base, fielding percentage, etc., to find these players. Not only do teams use big data to find the right players for them, but the MLB also uses it for their Statcast technology. Statcast was implemented in 2015 and is an automated tool that analyzes data to deliver accurate statistics in real time &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. This technology tracks many different aspects of the game, including many different pitching statistics.&lt;/p>
&lt;p>The pitching statistics that are recording is actually through the PITCHf/x system &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This system was implemented nine years before Statcast. The system is made up of three cameras that are located throughout the stadium that measure the speed, spin, and trajectory of the baseball as it is thrown. Statcast adopted this technology as the MLB was looking at ways to track more than just pitching statistics.&lt;/p>
&lt;p>And while there is a lot of data about pitching, we will only be focusing on release speed, release spin, pitch type, and the pitcher for our model. And while there is a lot of data about these different pitches and the pitchers, it can still be difficult to predict exactly what type of pitch is thrown according to the pitcher. For instance, the difference between different types of fastballs, a four-seam and two-seam, can be different by less than a mile per hour and forty rotations per minute (as thrown by Aaron Nola of the Philadelphia Phillies). In our model that we try to create, we are use these variables, along with the last three pitches thrown for each pitch type, to try to predict what pitch was just thrown.&lt;/p>
&lt;h2 id="2-background-and-previous-work">2. Background and Previous Work&lt;/h2>
&lt;p>Baseball players are always looking for a way to have an edge over their opponents. Whether that is through training, recovery, use of supplements, and by watching film or statistics to find what your opponent’s struggle with.&lt;/p>
&lt;p>There are several existing models that have been made to predict pitch type. We plan on examining them to determine how this can be done. We are looking for a general blueprint of which multiple models have used and/or have followed. This also allowed us to see what type of datasets would best be used for an analysis of this sort.&lt;/p>
&lt;h3 id="21-harvard-college-model">2.1 Harvard College Model&lt;/h3>
&lt;p>The first prediction model we studied was done through Harvard University &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. This prediction model was very complex and explored many different types of analysis. They first investigated doing a binary classification model. However, they ultimately decided against this because the label of pitches did not accurately represent what they actual were. This led them into multi-class predictive models in which they used. The other types of analysis were boosted trees, classification trees, random forests, linear discriminant analysis, and vector machines. The main point of this report was to see if they could replicate previous work done, but they ultimately failed at each one. This resulted to them in trying to determine if one can correctly predict pitch type selection. There research showed that machine learning cannot correctly predict pitch type due to the many different aspects that need to be analyzed. They hoped that their work could be used as a reference for future work done.&lt;/p>
&lt;p>We found this article to be very useful in our work since it stands as a reference for future work. We found this to be helpful in our review of predictive models for pitch types due to the different models they tried to replicate.&lt;/p>
&lt;h3 id="22-north-carolina-state-university-model">2.2 North Carolina State University Model&lt;/h3>
&lt;p>The prediction model created by North Carolina State University &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> was created to predict the next pitch that was going to be thrown. Their model compared old data to the current live data of a game, they are using many different types of data to predict the next type of pitch. They used a very large database that consisted of 287 MLB pitchers who had an average of 81 different pitch features (pitch type, ball rotation, speed, etc.). They are trying to determine if the next pitch will be a fastball or an off-speed pitch. Like the previously mentioned model, this model is also using trees to give a classification output. The parent node is the first type of pitch thrown, which then leads to if the pitch was a strike or a ball, then it uses this information and compares it to their dataset to predict the next set of nodes.&lt;/p>
&lt;p>We found this to be very useful for our review since their model worked correctly and it used current data from the game. With Statcast today, we find this to be very important since all of this information is recorded and logged as each pitch is thrown.&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>Major League Baseball first started using Statcast in 2015, after a successful trial run in 2014. Statcast &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> provided the MLB with a way to collect and analyze huge amounts of data in real time. Some of the data it collects, which is used in many different models, includes the pitcher’s name, pitch type, release spin, release speed, and the amount of times each pitch was thrown.&lt;/p>
&lt;p>Since we do not need all the data Statcast collects, we found a dataset with the datatypes listed above &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. We chose this dataset because it contains a significant amount of data that can be used for a prediction model. The dataset contains a lot of information about not only the pitchers, but about the batters. The database shows what each batter has done against every type of pitch they have faced, whether they hit the ball, fouled it, swung and missed, etc. We felt as if this data is very important as well since coaches and pitchers will want to know how the batter reacts to different pitches and the different locations of each pitch. This would be a vital part to any pitch prediction model as coaches signal to the pitchers what types of pitches to throw according the strengths and weaknesses of the batters.&lt;/p>
&lt;h2 id="4-search-and-analysis">4. Search and Analysis&lt;/h2>
&lt;p>After conducting our background research, we determined that in order to build an accurate model to predict which pitch was just thrown, you would need to use a similarity analysis tool model with classification trees. This model allows us to take old data and compare it to the live data of the pitch. By comparing the live data to the older data, it will give the most accurate results due to how the players are playing during that game. A batter may struggle with a certain type of pitch that day and not another.&lt;/p>
&lt;p>The similarity analysis tool model would best be used to find instances of the batter in certain pitch counts. For instance, this tool would analyze the different amount of times the batter has faced a count 1 – 2 (1 ball and two strikes), and then find what pitch they saw next and what the outcome was. This information would then be filtered to see what pitch will have the best outcome for the pitcher, it would tell them what pitch to throw and where to throw it. This is where the classification trees would then be used. The trees would classify the recommended pitches to throw and their location to the types of pitches that they throw.&lt;/p>
&lt;h2 id="5-limitations">5. Limitations&lt;/h2>
&lt;p>There are several limitations to predictive models for pitch types &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. The biggest is pitch classification itself. Pitchers ultimately pick what types of pitches they throw and as some pitches behave like others, different pitchers can classify the same pitch type as something else. For example, the traditional curve ball and a knuckle curve react the same. Other classification challenges that can be faced can be related to how the pitcher is playing. As the game goes on, pitcher’s velocities traditionally decrease due to their arms tiring, this could result in misidentification between different types of fastballs. The last limitation that could affect the pitch classification could be the PITCHf/x system malfunctioning or interference with the system.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>This report discusses the use of predictive model in baseball and how they can be used to predict the next pitch thrown. By reviewing previous models down by Harvard &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> and by North Carolina State University &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>, we determined that the best way to build a predictive model would be by using a similarity analysis tool model with classification trees. With the Carolina State University’s successful model, we also concluded that there were limitations to it, as different pitchers classify their pitches differently and telling different pitches apart due to the similar behaviors. The use of predictive models in baseball could change the game as it gives the pitchers an advantage over the batters. We hope that this report can show how big analytics can affect the game of baseball.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8 References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>[2020. About Statcast. MLB Advanced Media. &lt;a href="http://m.mlb.com/glossary/statcast">http://m.mlb.com/glossary/statcast&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Nathan, Alan M. Tracking Baseballs Using Video Technology: The PITCHf/x, HITf/x, and FIELDf/x Systems. &lt;a href="http://baseball.physics.illinois.edu/pitchtracker.html">http://baseball.physics.illinois.edu/pitchtracker.html&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Plunkett, Ryan. 2019. Pitch Type Prediction in Major League Baseball. Bachelor&amp;rsquo;s thesis, Harvard College. &lt;a href="https://dash.harvard.edu/handle/1/37364634">https://dash.harvard.edu/handle/1/37364634&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Sidle, Glenn. 2017. Using Multi-Class Classification Methods to Predict Baseball Pitch Types. North Carolina State University. &lt;a href="https://projects.ncsu.edu/crsc/reports/ftp/pdf/crsc-tr17-10.pdf">https://projects.ncsu.edu/crsc/reports/ftp/pdf/crsc-tr17-10.pdf&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Schale, Paul. 2020. MLB Pitch Data 2015-2018. Kaggle &lt;a href="https://www.kaggle.com/pschale/mlb-pitch-data-20152018">https://www.kaggle.com/pschale/mlb-pitch-data-20152018&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Sharpe, Sam. 2020. MLB Pitch Classification. Medium. &lt;a href="https://technology.mlblogs.com/mlb-pitch-classification-64a1e32ee079">https://technology.mlblogs.com/mlb-pitch-classification-64a1e32ee079&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data Analytics in the National Basketball Association</title><link>/report/fa20-523-317/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-317/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-317/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-317/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-317/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-317/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Igue Khaleel, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-317/">fa20-523-317&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-317/blob/master/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The National Basketball Association and the deciding factors in understanding how the game should be played in terms of coaching styles, positions of players, and understanding the efficiencies of shooting certain shots is something that is prevalent in why analytics is used. Analytics is a topic space within basketball that has been growing and emerging as something that can make a big difference in the outcomes of gameplay. With the small analytic departments that have been incorporated within teams, results have already started coming in with the teams that use the analytics showing more advantages and dominance over opponents who don&amp;rsquo;t. We will analyze positions on the court of players and how big data and analytics can further take those positions and their game statistics and transform them into useful strategies against opponents.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#11-point-guard">1.1 Point Guard&lt;/a>&lt;/li>
&lt;li>&lt;a href="#12-shooting-guard">1.2 Shooting Guard&lt;/a>&lt;/li>
&lt;li>&lt;a href="#13-small-forward">1.3 Small Forward&lt;/a>&lt;/li>
&lt;li>&lt;a href="#14-power-forward">1.4 Power Forward&lt;/a>&lt;/li>
&lt;li>&lt;a href="#15-center">1.5 Center&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#2-era-of-analytics">2. Era of Analytics&lt;/a>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#31-the-houston-rockets">3.1 The Houston Rockets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#32-tools">3.2 Tools&lt;/a>&lt;/li>
&lt;li>&lt;a href="#33-draft-philosophy">3.3 Draft Philosophy&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-background-work-and-advanced-analytics-in-basketball">4. Background Work and Advanced Analytics in Basketball&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-algorithims-associated-with-nba">5. Algorithims associated with NBA&lt;/a>
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#51-k-means">5.1 K-Means&lt;/a>&lt;/li>
&lt;li>&lt;a href="#52-linear-regression">5.2 Linear Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#53-logistic-regression">5.3 Logistic Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#54-support-vector-machines">5.4 Support Vector Machines&lt;/a>&lt;/li>
&lt;li>&lt;a href="#55-artificial-neural-networks">5.5 Artificial Neural Networks&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgment">7. Acknowledgment&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> basketball, sports, team, analytics , statistics, positions&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The National Basketball Association was first created in the year of 1946 with the name of BAA (Basketball Association of America). However, in 1949 the name was changed to the NBA with a total of 17 teams&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. As time progressed the league started picking up steam and more and more teams began to join and it wasn’t until the 90’s that we see the total amount of NBA teams be produced.This league consists of professional basketball players from both national and international spaces of the world. As there are 16 roster spots per team and 32 teams in total, only the very most athletic, skillfull, and colossal individuals are chosen to represent this league. Now, knowing the special skillsets of individual players, the founder of basketball, James Naismith, created positions to maximize these individual players for team success. On the court there are 5 positions : point guard, shooting guard, small forward, power forward, and center&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="11-point-guard">1.1 Point Guard&lt;/h4>
&lt;p>Starting with the point guard, generally these individuals are the smallest players on the court with an average height around 6'2 tall. With what these player lack in height they make up for in skillset in terms of quickness, passing, agility, ball handling, and natural shooting ability. Point guards are generally looked at to be the floor general of the team and take up the job of setting up the coach&amp;rsquo;s gameplan and teamates.&lt;/p>
&lt;h4 id="12-shooting-guard">1.2 Shooting Guard&lt;/h4>
&lt;p>The shooting guard is a generally a slightly taller player than the point guard and like the name suggests they are generally the player known for their indiviualisitc shooting prowess whehter if it is beyond the 3 point line or in the mid-range. Shooting guards are known to be positioned in the perimeter(outside the arc) as a partner to the point guard. On occasion, the role of the shooting guard is expanded in the case that the point guard is pressured so the role may be for the shooting guard to be better at defense or a player that can help in the playmaking duties of the point guard.&lt;/p>
&lt;h4 id="13-small-forward">1.3 Small Forward&lt;/h4>
&lt;p>The small forward is where things change in terms of roles when comparing to the guards of that were previously mentioned above. They can be considered hybrids in the sense that they can both operate on the perimeter like guards and can go down low like power forwards and centers which will be discussed later. Noramlly with wings(another name for small forward) with an average height around 6'7, there are a plethora of responsibilites in order to be considered effective. The reason for this is because generally speaking, small forwards are the most athletic player on the court. They basically have most the agility and ball handling of guards and most of the physicallity and power of power forwards/centers. Understandibly, there are tasked with big defensive assignments and are usually looked at to be a decent to above-average producer on offense.&lt;/p>
&lt;h4 id="14-power-forward">1.4 Power Forward&lt;/h4>
&lt;p>The power forward position is where the physicallity of players matters more. Generally these players are around 6'9 to 6'11 and are heavier than most players. Becuase of this they give up speed and shooting which is why they operate around the free throw line and basket. They are looked at to protect the interior with the center from smaller players and small forwards driving in the lane to the basket.&lt;/p>
&lt;h4 id="15-center">1.5 Center&lt;/h4>
&lt;p>The center is considered mostly the point guard of the defense of the team. They are generally the anchor that protects the rim primarily and takes up defensive assignemtns and calls. Without a competent center, a team can see their defense take a hit. Along with defense, centers are good options to go to when the team has offensive lulls since the easiest shot to make in the nba is a hook shot or layup and the center operates 3 feet from the basket. Centers generally range from 6'11 to as high as 7'6 in height. On rare occasions you can see 6'9 to 6'10 centers take the court and that is generally because of play-style or above-average defense.&lt;/p>
&lt;h2 id="2-era-of-analytics">2. Era of Analytics&lt;/h2>
&lt;p>The National Basketball Association continues to not only grow in the sense of continued personnel but an increase of cap(cash flow) amongst teams as well. Within the scope of this prosperous cap situation that the NBA has accumulated over the years through merchandising, tickets, and tv deals, teams have found flexibility in the ability to create the optimal situation for whatever version of basketball the General Manager sees fit for the vision of the team. In terms of better understanding how this can be accomplished it is best to understand what spurred this action of finding styles to lead to the best team success.&lt;/p>
&lt;p>That particular action is players such as Stephen Curry, a 6-3 NBA point guard, that led to the change in utilizing analytics. The year Steph Curry broke through as an MVP, his team; the Golden State Warriors broke the former Chicago Bulls record of 72-9. This in big part was due to Steph Curry breaking the 3pt record as well as Golden State adopting the small ball philosophy. This particular year gave birth to the era of analytics because of how dominate those two approaches were.&lt;/p>
&lt;h4 id="31-the-houston-rockets">3.1 The Houston Rockets&lt;/h4>
&lt;p>This has then inspired teams to introduce analytics departments to measure ways to beat the game and exploit mismatches in defensive schemes and height within players. An example of a team that spearheaded this change in strategy is the Houston Rockets. Their GM(General Manager) Daryl Morey was a MIT graduate who advocated for a team that primarily shoots three point shots as their main forte&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The science behind this concept was that 33% shooting from the three point line measure to 50% from the two point line respectively. This was in the works in the year of 2017 just two years removed from Steph Curry&amp;rsquo;s three point dominance in his MVP season. In terms of numbers representing the change, the 2018 Houston Rockets attempted approximately 82% of their shot attempts around the three point line and the restricted area(the circle around ~5 feet in diameter surrounding the rim)&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The next best team in that department was eleven percent down at 71% in terms of attempts. In this year, the Rockets won their conference at a record of 65 wins - 17 losses as well as break the NBA record in three pointers attemted and made.&lt;/p>
&lt;h4 id="32-tools">3.2 Tools&lt;/h4>
&lt;p>In order to evaluate these players and acquire the data necessary for analyization, the NBA partnered with a company name STATS to provide the necessary tools for data collection. STATS worked with the NBA by installing six cameras in each basketball arena in order to, &amp;ldquo;track player and referee movements at 25 frames per second to get the most analytical data for teams and the NBA to analyze&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&amp;rdquo; This is very effective in terms of showing the play-by-play moves of players in a system as well as even how referees move. With players, these tools can serve as a chess board where the coach is able to watch pieces move and can determine where certain positions could be optimized to its maximum efficiency. This allows for film sessions to be more productive and helpful for players to better see where they fit and even improve in. In terms of referees, throughout sports it is known that referees have cost some games due to missed calls or questionable decisions. This technology can help in terms of understanding: 1) how a specific referee calls certain fouls and 2) if there seems to be a number count of fouls depending on what team the referee is reffing historically. Understanding both the tendencies of players and refs alike gives coaching staffs a direction to go in when preparing for opponents on a game-by-game basis&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h4 id="33-draft-philosophy">3.3 Draft Philosophy&lt;/h4>
&lt;p>Another facet of the game that is likewise impacted by the tools and techniques described in 3.2 is the NBA draft. The NBA draft consists a total of 60 players selected in two rounds combined. The general consensus before this analytics era was to choose the best player avaible most of the time. Teams back then usually drafted big men(e.g. forwards and centers) because it was considered a safe pick and known to help your team better in more areas. As time passed, we&amp;rsquo;ve seen a shift to more guards that are drafted instead to fit the narrative the analytics presents to teams regarding the best path to success. For example, earlier Stephen Curry was mentioned to be one of the foundational reasons that the analytics movement was largely adapted. The year Curry got drafted, the #1 pick in the draft was Blake Griffin who at the time was considered the best Power Forward in the draft while Curry was drafted at 8th overall and even James Harden of the Houston Rockets was drafted 3rd&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. As we fast forward to 2020, both Curry and Harden are looked at as the two best players from their draft class with Curry revolutionizing the three point shot and Harden being the ultimate analytics player with his ability to manipulate the defense and draw free throws from fouls like no player has ever done. As years passed, there has been a shift in drafting players with the mindset of that particular players' potential over fit in the sense that teams look for the best available player that fits the teams system the most efficiently&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. An example is the upcoming 2020 NBA draft where there is a question of who will become the #1 and 2 pick respectively. The Golden State Warriors have the 2nd pick in the draft because of a year of injuries for all of their star players. So, they typically aren&amp;rsquo;t looking for a player like most losing teams are doing in the draft. In the eyes of many scouts, some view a player like Lamelo Ball, a 6'7 point guard as the best player or at least second best and others see players like Anthony Edwards(SG), James Wiseman(C) and Deni Advija(SG) as potentially better fits and safer picks. However, for the warriors rumors over social media from notable sources have shown that they aren&amp;rsquo;t interested in drafting Lamelo Ball as he is a point guard and they have Steph Curry already. They instead prefer to choose a Small Forward or Center that can help their defensive potential and style of play. Years ago, that may not have been the case as the best player available would usually come off the board and the team would figure it out after that. Thus, this shows how analytics has not only persuaded teams to change their play styles and system but also the players that come with it whether they are veterans or incoming rookies.&lt;/p>
&lt;h2 id="4-background-work-and-advanced-analytics-in-basketball">4. Background Work and Advanced Analytics in Basketball&lt;/h2>
&lt;p>Considering how the impacts of how implemented analytics has aided the NBA atmosphere as mentioned above, we look to learn technologies and work that help bring this about. This begins with camera systems that have been implemented by a company named SportVU who&amp;rsquo;ve helped bring about change in NBA arenas since 2013 that track player and basketball movement across the arenas&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. This system goes further in the analysis of collecting data in the context of individual player statistics being captured as well as their positioning on the court and speed in particular instances.&lt;/p>
&lt;p>Thus by capturing the basic statistics such as points, assists, rebounds, steals and blocks. Analytical tools such as Player Efficiency Ratings and Defensive Metrics were better used to analyze players and their individualistic impacts on the basketball court. The impacts of these analytical/computational metrics are represented in many organizations abilities to understand the scope of player&amp;rsquo;s salaries and positioning on the court, who to draft, and helps sports analyst on TV shows such as FS1 and ESPN to easily break down the game of Basketball.&lt;/p>
&lt;p>This is where algorithims come to play as an algorithim needs a dataset from which it can train itself and develop statistical patterns to help in predictive analysis and representation for coaches and teams to utilize respectively. An example of this is show through students named Panna Felsen and Lucy from the University of California Berkely, who are developing a software name Bhostgusters that helped analyze the body positions of players and further the response and movements of a team to certain plays run by the opposition&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The end goal of this for coaches to be able to draw up a play on a tablet and see potential conflicts, results, and how opponents may counteract that particular play.&lt;/p>
&lt;p>Other technologies that are being developed and implemented are things like, CourtVision, which is technology that shows the statistics of a player making a shot based on that players' past statistics and position on the court. As the player is moving through the court the numbers change to reflect his efficiency on certain areas on the court based on this. As stated by Marcus Woo, the author of this article, these technologies aren&amp;rsquo;t meant to replace the systems in place but instead are there the help in efficiency and effectiveness&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="5-algorithims-associated-with-nba">5. Algorithims associated with NBA&lt;/h2>
&lt;p>When it comes to the variety of algorithims used in the National Basketball Association, we will be analyzing
the range of algorithims discussed through articles and papers on google scholar. We looked at a total of five
algorithims that were commonolu shown to be used of the most searches when it came to predictive and learning
analysis within NBA analytics departements and outside agencies. The algorithims as presented are: K-means,
Artificial Neural Networks, Linear Regression, Logistic Regression, and Support Vector Machines&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Linear
Progression was by far the most written on topic within the five algorithims listed above with a total of 11,000
searches. It is followed by the Support Vector Machines with 5,240, Logistic Regression with 4,500, Artificial
Neural Networks with 4,300, and K-Means with 1,590 search results(*all results via google scholar search bar).&lt;/p>
&lt;h4 id="51-k-means">5.1 K-Means&lt;/h4>
&lt;p>The first algorithim we&amp;rsquo;ll look at is K-Means which is classified as generally the &amp;ldquo;clustering algorithim&amp;rdquo; which takes the form of initializing a single point of k or the mean and organizing the data towards that particular mean&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. This is then repeated over and over until the appropriate results are found and compiled. Now as National Basketball Association statistics are inserted this can be used to cluster players together than fit the criteria on certain outcomes of points, rebounds, assists, and blocks.&lt;/p>
&lt;h4 id="52-linear-regression">5.2 Linear Regression&lt;/h4>
&lt;p>Linear Regression, which is very commonly used in machine learning is very effective as a predictor tool. It works by forming &amp;ldquo;regression coefficients&amp;rdquo; that stems from pitting together independent variables which help in predictions within a game&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. So, throught the input and output variables taht are presented predictive measurements can be performed to highlight potential productivety. An example is the &amp;ldquo;Box-Plus-Minus&amp;rdquo;. This was created to show a basketball player&amp;rsquo;s overall court production and effect through their statistics, what position they play on the court and the wins and losses that team incurs because of this&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. This was built through linear regression and shows through charts based on statistics how productive a player is or potentially can be given the system and oppurtunities.&lt;/p>
&lt;h4 id="53-logistic-regression">5.3 Logistic Regression&lt;/h4>
&lt;p>Similarly to Linear Regression, Logistic Regression shares a lot of features in terms of the formula used for prediction except it utilizes a sigmoid as opposed to a linear function when performing calculations. Weight values are the main form of predictions in whatever form of scenario or situation in which that analyst wants to produce&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. An example of this is shown through a logistical regression analysis performed by Oklahoma State University on clutch and non-clutch shots by players in the National Basketball Association. The premise of this is taking the data of an individual player based on their shooting percentages in spots on the floor relative to the distance of the defender on them and using that to figure out the potential of a player making a shot in the clutch(universally known as the last two minutes in a close game)&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. This then shows how a predictive algorithm can be utilized not only based on solely percentages and efficiencies but also with the inclusion of situation on a basketball floor.&lt;/p>
&lt;h4 id="54-support-vector-machines">5.4 Support Vector Machines&lt;/h4>
&lt;p>Support Vector Machines are considered to be a very formidable tool when it comes to measuring classification issues. This modeled machine creates a decision-making tree that helps in the predictions of basketball games and thus can help coaches form strategies and gameplans around what the model predicts can happen. Additional advantages that come with this tool is its ability to operate in high dimensions, the ability to identify kernels, and its memory efficiency&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. The minor issue with this machine is the lack of rule generation but as it is more of an emerging tool overtime this is something that is relatively fixable&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. The advantages&lt;/p>
&lt;h4 id="55-artificial-neural-networks">5.5 Artificial Neural Networks&lt;/h4>
&lt;p>With Artificial Neural Networks the use of the Multi-Layer Perceptron is prevalent and it is highlighted by the vertices of a group in correlation to input varables and comes out with the output&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. This tool according the Beckler is also considered to be, &amp;ldquo;an adaptive system that changes its structure based on external and internal information flows during the network training phase&amp;rdquo;&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. With this, the Artificial Neural Network is considered to be one of the most accurate predictive tools when it comes to basketball and can predict patterns as more data is inputed&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>As time progresses, we will continue to see the use of analytics as well as the expanision of analytics departments in not only the National Basketball Association but other professional sports as well. The impacts of analytics have been highlighted through recent years as mentioned above with the change to styles of play, and the way coaches approach gameplans before each respective game is played. As Adam Silver, the commissioner of the National Basketball Association stated, &amp;ldquo;Analytics have become front and center with precisely when players are rested, how many minutes they get, who they’re matched up against&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>.&amp;rdquo; Through this, Silver explains not only to technical aspect of basketball that analytics supports but the physical aspect which can aid in preventing things like player injuries and rest. Understandibly, this highlights how analytics can help the league now and in the future; especially when more sophisticated machine learning tools and algorithims are produced for this purpose.&lt;/p>
&lt;h2 id="7-acknowledgment">7. Acknowledgment&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Online, N., 2020. NBA History. [online] Nbahoopsonline.com. Available at: &lt;a href="https://nbahoopsonline.com/History/#:~:text=The%20NBA%20began%20life%20as,start%20of%20the%20next%20season">https://nbahoopsonline.com/History/#:~:text=The%20NBA%20began%20life%20as,start%20of%20the%20next%20season&lt;/a>. [ Accessed 20 October 2020]. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Editor, M., 2020. How NBA Analytics Is Changing Basketball | Merrimack College. [online] Merrimack College Data Science Degrees. Available at: &lt;a href="https://onlinedsa.merrimack.edu/nba-analytics-changing-basketball/">https://onlinedsa.merrimack.edu/nba-analytics-changing-basketball/&lt;/a> [Accessed 16 November 2020]. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>N. M. Abbas, &amp;ldquo;NBA Data Analytics: Changing the Game,&amp;rdquo; Medium, 21-Aug-2019. [Online]. Available: &lt;a href="https://towardsdatascience.com/nba-data-analytics-changing-the-game-a9ad59d1f116">https://towardsdatascience.com/nba-data-analytics-changing-the-game-a9ad59d1f116&lt;/a>. [Accessed: 17-Nov-2020]. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>C. Ford, &amp;ldquo;NBA Draft 2009,&amp;rdquo; ESPN. [Online]. Available: &lt;a href="http://www.espn.com/nba/draft2009/index?topId=4279081">http://www.espn.com/nba/draft2009/index?topId=4279081&lt;/a>. [Accessed: 17-Nov-2020]. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>M. Woo, &amp;ldquo;Artificial Intelligence in NBA Basketball,&amp;rdquo; Inside Science, 21-Dec-2018. [Online]. Available: &lt;a href="https://insidescience.org/news/artificial-intelligence-nba-basketball">https://insidescience.org/news/artificial-intelligence-nba-basketball&lt;/a>. [Accessed: 07-Dec-2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>M. Beckler and M. Papamichael, &amp;ldquo;NBA Oracle,&amp;rdquo; 10701 Report, 2008. [Online]. Available: &lt;a href="https://www.mbeckler.org/coursework/2008-2009/10701_report.pdf">https://www.mbeckler.org/coursework/2008-2009/10701_report.pdf&lt;/a>. [Accessed: 06-Dec-2020]. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>R. Anderson, &amp;ldquo;NBA Data Analysis Using Python &amp;amp; Machine Learning,&amp;rdquo; Medium, 02-Sep-2020. [Online]. Available: &lt;a href="https://randerson112358.medium.com/nba-data-analysis-exploration-9293f311e0e8">https://randerson112358.medium.com/nba-data-analysis-exploration-9293f311e0e8&lt;/a>. [Accessed: 07-Dec-2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>J. P. Hwang, &amp;ldquo;Learn linear regression using scikit-learn and NBA data: Data science with sports,&amp;rdquo; Medium, 18-Sep-2020. [Online]. Available: &lt;a href="https://towardsdatascience.com/learn-linear-regression-using-scikit-learn-and-nba-data-data-science-with-sports-9908b0f6a031">https://towardsdatascience.com/learn-linear-regression-using-scikit-learn-and-nba-data-data-science-with-sports-9908b0f6a031&lt;/a>. [Accessed: 07-Dec-2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>J. Perricone, I. Shaw, and W. Swie¸chowicz, &amp;ldquo;Predicting Results for Professional Basketball Using NBA API Data,&amp;rdquo; Stanford.edu, 2016. [Online]. Available: &lt;a href="http://cs229.stanford.edu/proj2016/report/PerriconeShawSwiechowicz-PredictingResultsforProfessionalBasketballUsingNBAAPIData.pdf">http://cs229.stanford.edu/proj2016/report/PerriconeShawSwiechowicz-PredictingResultsforProfessionalBasketballUsingNBAAPIData.pdf&lt;/a>. [Accessed: 06-Dec-2020]. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>A. P. B. N. Barakat, J. H. F. L. Breiman, M. T. R. Burbidge, K.-S. S. T. Chen, J. L. R. WW. Cooper, V. N. V. C. Cortes, E. F. M. Hall, J. Holland, R. C. E. J. Kennedy, K. J. Kim, K. H. T. K. Kirchner, J. S. S. P. Kvan, A. C. W. BL. Lee, B. B. D. Martens, J. Mercer, J. K. B. Min, O. B. K. Muata, J. S. L. IS. Oh, P. M. M. M. Pal, J. R. Quinlan, F. P.-C. FJR. Ruiz, W. H. C. JY. Shih, H. M. E. I.-D. MBA. Snousy, P. V. E. Štrumbelj, L. C. FEH. Tay, V. V. S. S. Tripathi, G. Valentini, V. N. Vapnik, G. D. N. Vlastakis, J. N. Wang, E. Y. K. A. Widodo, C. F. H. TA. Zak, and J. S. J. Zhou, &amp;ldquo;Analyzing basketball games by a support vector machines with decision tree model,&amp;rdquo; Neural Computing and Applications, 01-Jan-1970. [Online]. Available: &lt;a href="https://link.springer.com/article/10.1007/s00521-016-2321-9">https://link.springer.com/article/10.1007/s00521-016-2321-9&lt;/a>. [Accessed: 07-Dec-2020]. &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>2017 A. S. M. N. A. Jun 01, &amp;ldquo;The NBA&amp;rsquo;s Adam Silver: How Analytics Is Transforming Basketball,&amp;rdquo; Knowledge@Wharton. [Online]. Available: &lt;a href="https://knowledge.wharton.upenn.edu/article/nbas-adam-silver-analytics-transforming-basketball/">https://knowledge.wharton.upenn.edu/article/nbas-adam-silver-analytics-transforming-basketball/&lt;/a>. [Accessed: 07-Dec-2020]. &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Rank Forecasting in Car Racing</title><link>/report/fa20-523-349/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-349/project/project/</guid><description>
&lt;h1 id="rank-forecasting-in-car-racing">Rank Forecasting in Car Racing&lt;/h1>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-349/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-349/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-349/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-349/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;ul>
&lt;li>Jiayu Li, fa20-523-349&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-349/blob/master/project/project.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The IndyCar Series is the premier level of open-wheel racing in North America. Computing System and Data analytics is critical to the game, both in improving the performance of the team to make it faster and in helping the race control to make it safer. IndyCar ranking prediction is a practical application of time series problems. We will use the LSTM model to analyze the state of the car, and then predict the future ranking of the car. Rank forecasting in car racing is a challenging problem, which is featured with highly complex global dependency among the cars, with uncertainty resulted from existing exogenous factors, and as a sparse data problem. Existing methods, including statistical models, machine learning regression models, and several state-of-the-art deep forecasting models all perform not well on this problem. In this project, we apply deep learning methods to racing telemetry data. And compare deep learning with traditional statistical methods (SVM, XGBoost).&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#structure-of-the-log-file">Structure of the log file&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#data-preprocessing">Data preprocessing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#feature-selection">Feature selection&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-inference">5. Inference&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> Time series forecasting, deep learning.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Indy500 is the premier event of the IndyCar series&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Each year, 33 cars compete on a 2.5-mile oval track for 200 laps. The track is split into several sections or timeline. E.g., SF/SFP indicate the start and finish line on the track or on the pit lane, respectively. A local communication network broadcasts race information to all the teams, following a general data exchange protocol.
We aim to predict the leading car in the future through telemetry data generated in real time during the race. Given a prediction step t_p, and a time point t_0 in the game, we predict the following two events:&lt;/p>
&lt;ol>
&lt;li>Whether the currently leading car continue to lead at time t_0 + t_p.&lt;/li>
&lt;li>Which car is the leading car at time t_0 + t_p.&lt;/li>
&lt;/ol>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>In many real-world applications, data is captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them.
The traditional statistical learning model (Naive Bayes, SVM, Simple Neural Networks) is difficult to deal with the problem of time series prediction, since the model is unable to understand the time-series dependence of data. Traditional time series prediction models such as autoregressive integrated moving average (ARIMA) can only deal with linear time series with certain periodicity. The anomaly events and human strategies in the racing competition make these methods no longer applicable. Therefore, time series prediction models (RNN, GRU, LSTM, etc.) based on deep learning are more suitable for solving such problems.
Previous racing prediction attempts such as &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> could not make real-time predictions because the data they used was based on Lap, that is, new data would only be generated when the car passed a specific position. And we will try to use high-frequency telemetry data to make predictions.&lt;/p>
&lt;p>This article is different from &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> in at least three points:&lt;/p>
&lt;ol>
&lt;li>The resolution of the data is different. This article uses telemetry data. The telemetry data generates 7 to 8 data points per second. After preprocessing, we sample the data to 1 data point per second. The data used in &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> is based on &amp;ldquo;Lap&amp;rdquo;, that is, the new data will only be recorded when the car passes the starting point.&lt;/li>
&lt;li>The prediction model is different. This article uses LSTM for prediction, while &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> uses DeepAR-based models for prediction.&lt;/li>
&lt;li>The definition of ranking is different. What this article predicts is: Given a certain time t, predict which car will lead at t+tp. And the output of &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> is: predict the rank of each car to complete the nth lap. One is to predict the position of the car in space at a given time; the other is to give a spatial position (usually the starting point of the track), and then predict the time for the car to pass that position.&lt;/li>
&lt;/ol>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>There are two main sources of data: One is the game record from 2013 to 2019. The other is telemetry data for 2017 and 2018.&lt;/p>
&lt;p>The race record only includes the time spent in each section and does not include the precise location of every two cars at a certain point in time.
Telemetry data is a high-resolution data, each car will produce about 7 records per second, we use telemetry data to estimate the position of each car at any time.
In order to expand the training data set, we used interpolation to convert ordinary race records into a time series of car positions.
If we assume that the speed of the car within each section does not change, then the position of the car at time T can be calculated as follows:
LapDistance(T) \approx L \frac{T-T_1}{T_2 - T_1} . T_1 and T_2 are the start and end time of the current section. L=2.5 miles is the length of the section.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled6.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Table 1&lt;/strong> : Indy 500 data sets&lt;/p>
&lt;h3 id="structure-of-the-log-file">Structure of the log file&lt;/h3>
&lt;p>The Multi-Loop Protocol is designed to deliver specific dynamic and static data that is set up and produced by the INDYCAR timing system. This is accomplished by serially streaming data that is broken down into different record sets. This information includes but is not limited to the following:&lt;/p>
&lt;ul>
&lt;li>Completed lap results&lt;/li>
&lt;li>Time line passing or crossing results&lt;/li>
&lt;li>Completed section results&lt;/li>
&lt;li>Current run or session information&lt;/li>
&lt;li>Flag information&lt;/li>
&lt;li>Track set up information including segment definitions&lt;/li>
&lt;li>Competitor information&lt;/li>
&lt;li>Announcement information&lt;/li>
&lt;/ul>
&lt;p>The INDYCAR MLP is based on the AMB Multi-Loop Protocol version 1.3. This document contains the
INDYCAR formats for specific fields not defined in the AMB document.&lt;/p>
&lt;p>Record Description:&lt;/p>
&lt;p>Every record starts with a header and ends with a CR/LF. Inside the record, the fields are separated by a &amp;ldquo;broken bar&amp;rdquo; symbol 0xA6 (not to be confused with the pipe symbol 0x7C). The length of a record is not defined and can therefore be more than 256 characters. The data specific to each record Command is contained between the header and CR/LF.
&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled8.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Figure 1&lt;/strong> : Indy 500 track map&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;h3 id="data-preprocessing">Data preprocessing&lt;/h3>
&lt;p>There are two main sources of data: One is the game record from 2013 to 2019. The other is telemetry data for 2017 and 2018.&lt;/p>
&lt;p>The race record only includes the time spent in each section and does not include the precise location of every two cars at a certain point in time.
Telemetry data is a high-resolution data, each car will produce about 7 records per second, we use telemetry data to estimate the position of each car at any time.
In order to expand the training data set, we used interpolation to convert ordinary race records into a time series of car positions.
If we assume that the speed of the car within each section does not change, then the position of the car at time T can be calculated as follows:
LapDistance(T) \approx L \frac{T-T_1}{T_2 - T_1} . T_1 and T_2 are the start and end time of the current section. L=2.5 miles is the length of the section.&lt;/p>
&lt;p>The preprocessing mainly includes 3 operations.&lt;/p>
&lt;ol>
&lt;li>Stream data interpolation. In order to expand the training data set, we used interpolation to convert ordinary race records into a time series of car positions.&lt;/li>
&lt;li>Data normalization, scale the input data to the range of -1 to 1.&lt;/li>
&lt;li>Data sorting. Due to the symmetry of the input data, that is, any data exchanged between two cars can still get a legal data set. Therefore, a model with more parameters is required to learn this symmetry. In order to avoid unnecessary complexity, we sort the data according to the position of the car. That is, the data of the current leading car is placed in the first column, the data of the currently ranked second car is placed in the second column, and so on. This helps to compress the model and improve performance.&lt;/li>
&lt;/ol>
&lt;h3 id="feature-selection">Feature selection&lt;/h3>
&lt;p>The future ranking of a car is mainly affected by two factors:
One is its current position: the car that is currently leading has a greater probability of being ahead in the future; the other is the time of the last pit stop: because the fuel tank of each car is limited, Entering pit stop, the possibility of it leading in the future will be reduced.&lt;/p>
&lt;p>Therefore, we choose the following characteristics to predict ranking:&lt;/p>
&lt;ul>
&lt;li>The current position of each car (Lap and Lap Distance)&lt;/li>
&lt;li>Time of the last Pit Stop of each car&lt;/li>
&lt;/ul>
&lt;p>Time series prediction problems are a difficult type of predictive modeling problem. Unlike regression predictive modeling, time series also adds the complexity of a sequence dependence among the input variables. A powerful type of neural network designed to handle sequence dependence is called recurrent neural networks. The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.
&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled5.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong> : Work flow and model structure&lt;/p>
&lt;h2 id="5-inference">5. Inference&lt;/h2>
&lt;p>Table shows the experimental results, which verify our hypothesis that the time series prediction model based on deep learning obtained the highest accuracy. Although the LSTM model achieves the highest accuracy, its advantages are not as obvious as RankNet. This is because the telemetry data of racing cars is non-public, and the data available for training are limited.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled4.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Table 2&lt;/strong> : LSTM model parameters&lt;/p>
&lt;p>According to the experimental results in Table 6, we draw the following conclusions:&lt;/p>
&lt;ol>
&lt;li>The LSTM model has higher accuracy in time series forecasting.&lt;/li>
&lt;li>Limited by the size of the training data set (only the telemetry data for 2 games is available), the accuracy improvement obtained by LSTM is not as obvious as RankNet.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://raw.githubusercontent.com/cybertraining-dsc/fa20-523-349/main/project/images/Untitled7.png" alt="alt text">&lt;/p>
&lt;p>&lt;strong>Table 3&lt;/strong> : Model accuracy comparison&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>The prediction problem of racing cars has the characteristics of non-linearity, non-periodicity, randomness, and timing dependence. The traditional statistical learning model (Naive Bayes, SVM, Simple Neural Networks) is difficult to deal with the problem of time series prediction, since the model is unable to understand the time-series dependence of data. Traditional time series prediction models such as ARMA / ARIMA can only deal with linear time series with certain periodicity. The anomaly events and human strategies in the racing competition make these methods no longer applicable. Therefore, time series prediction models (RNN, GRU, LSTM, etc.) based on deep learning are more suitable for solving such problems.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The author would like to thank Dr. Gregor Von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>IndyCar Dataset. &lt;a href="https://racetools.com/logfiles/IndyCar/">https://racetools.com/logfiles/IndyCar/&lt;/a>. visited on 04/15/2020 &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>M4 Competition. &lt;a href="https://forecasters.org/resources/time-series-data/m4-competition/">https://forecasters.org/resources/time-series-data/m4-competition/&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>C. L. W. Choo. Real-time decision making in motorsports: analytics forimproving professional car race strategy, PhD Thesis, MassachusettsInstitute of Technology, 2015. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>T. Tulabandhula.Interactions between learning and decision making.PhD Thesis, Massachusetts Institute of Technology, 2014. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Peng B, Li J, Akkas S, Wang F, Araki T, Yoshiyuki O, Qiu J. Rank Position Forecasting in Car Racing. arXiv preprint &lt;a href="https://arxiv.org/abs/2010.01707/">https://arxiv.org/abs/2010.01707/&lt;/a>. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: NBA Performance and Injury</title><link>/report/fa20-523-301/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-301/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;ul>
&lt;li>Gavin Hemmerlein, fa20-523-301&lt;/li>
&lt;li>Chelsea Gorius, fa20-523-344&lt;/li>
&lt;li>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-301/blob/main/project/project.md">Edit&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Sports Medicine will be a $7.2 billion dollar industry by 2025. The NBA has a vested interest in predicting performance of players as they return from injury. The authors evaluated datasets available to the public within the 2010 decade to build machine and deep learning models to expect results. The team utilized Gradient Based Regressor, Light GBM, and Keras Deep Learning models. The results showed that the coefficient of determination for the deep learning model was approximately 98.5%. The team recommends future work to predicting individual player performance utilizing the Keras model.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-dataset">3. Dataset&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#31-data-transformations-and-calculations">3.1 Data Transformations and Calculations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#41-development-of-models">4.1 Development of Models&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#411-evaluation-metrics">4.1.1 Evaluation Metrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-gradient-boost-regression">4.1.2 Gradient Boost Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#412-lightgbm-regression">4.1.2 LightGBM Regression&lt;/a>&lt;/li>
&lt;li>&lt;a href="#413-keras-deep-learning-models">4.1.3 Keras Deep Learning Models&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#5-inference">5. Inference&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-conclusion">6. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#61-limitations">6.1 Limitations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-acknowledgements">7. Acknowledgements&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#71-work-breakdown">7.1 Work Breakdown&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> basketball, NBA, injury, performance, salary, rehabilitation, artificial intelligence, convolutional neural network, lightGBM, deep learning, gradient based regressor.&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>The topic to be investigated is basketball player performance as it relates to injury. The topic of injury and recovery is a multi-billion dollar industry. The Sports Medicine field is expected to reach $7.2 billion dollars by 2025 &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The scope of this effort is to explore National Basketball Association(NBA) teams, but the additional uses of a topic such as this could expand into other realms such as the National Football League, Major League Baseball, the Olympic Committees, and many other avenues. For leagues with salaries, projecting an expected return on the investment can assist in contract negotiations and cater expectations. Competing at such a high level of intensity puts these players at a greater risk to injury than the average athlete because of the intense and constant strain on their bodies. The overall valuation of the NBA in recent years is over 2 billion dollars, meaning each team is spending millions of dollars in the pursuit of a championship every season. Injuries to players can cost teams not only wins but also significant profits. Ticket sales alone for a single NBA finals game have reported greater than 10 million dollars in profit for the home team, if a team&amp;rsquo;s star player gets injured just before the playoffs and the team does not succeed, that is a lot of money lost. These injuries can have an effect no matter the time of year, regular season ticket sales have been known to fluctuate with injuries from the team&amp;rsquo;s top performers. Besides ticket sales these injuries can also influence viewership, TV or streaming, and potentially lead to a greater loss in profits. With the health of the players and so much money at stake NBA team organizations as a whole do their best to take care of their players and keep them injury free.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>The assumptions were made based on current literature as well. The injury return and limitations upon return of Anterior Cruciate Ligament (ACL) rupture (ACLR) are well documented and known. Interesting enough, forty percent of the players in the study occurred during the fourth quarter &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. This leads some credence to the idea that fatigue is a major factor in the occurrence of these injuries.&lt;/p>
&lt;p>The current literature also shows that a second or third injury can occur more frequently due to minor injuries. &lt;em>&amp;ldquo;When an athlete is recovering from an injury or surgery, tissue is already compromised and thus requires far more attention despite the recovery of joint motion and strength. Moreover, injuries and surgical procedures can create detraining issues that increase the likelihood of further injury&amp;rdquo;&lt;/em> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="3-dataset">3. Dataset&lt;/h2>
&lt;p>To compare performance and injury, a minimum of two datasets will be needed. The first is a dataset of injuries for players &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. This dataset created the samples necessary for review.&lt;/p>
&lt;p>Once the controls for injuries were established, the next requirement was to establish pre-injury performance parameters and post-injury parameters. These areas were where the feature engineering took place. The datasets needed had to include appropriate basketball performance stats to establish a metric to encompass a player&amp;rsquo;s performance. One example that ESPN has tried in the past is the Player Efficiency Rating (PER). To accomplish this, it was important to review player performance within games such as in the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> dataset because of how it allowed the team to evaluate the player performance throughout the season, and not just the average stats across the year. In addition to that the data from the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> dataset was valuable in order to compare the calculated performance metrics just before an injury or after recovery to the player&amp;rsquo;s overall performance that season or in seasons prior. That comparison provided a solid baseline to understand how injuries can effect a player&amp;rsquo;s performance. With in depth information about each game of the season, and not just the teams and players aggregated stats, added to the data provided from the injury dataset &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> the team was be able to compose new metrics to understand how these injuries are actually affecting the players performance.&lt;/p>
&lt;p>Along the way attempted to discover if there is also a causal relationship to the severity of some of the injuries, based on how the player was performing just before the injury. The term &lt;em>load management&lt;/em> has become popular in recent years to describe players taking rest periodically throughout the season in order to prevent injury from overplaying. This new practice has received both support for the player safety it provides and also criticism around players taking too much time off. Of course not all injuries are entirely based on the recent strain under the players body, but a better understanding about how that affects the injury as a whole could give better insight into avoiding more injuries. It is important to remember though that any pattern identification would not lead to an elimination of all injuries, any contact sport will continue to have injuries, especially one as high impact as the NBA. There is value to learn from why some players are able to return from certain injuries more quickly and why some return to almost equivalent or better playing performance than before the injury. This comparison of performance was attempted by deriving metrics based on varying ranges of games immediately leading up to injury and then immediately after returning from injury. In addition to that performed comparisons to the players known peak performance to better understand how the injury affected them. Another factor that was important to include is the length of time recovering from the injury. Different players take differing amounts of time off, sometimes even with similar injuries. Something will be said about the player’s dedication to recovery and determination to remain at peak performance, even through injury, when looking at how severe their injury was, how much time was taken for recovery, and how they performed upon returning.&lt;/p>
&lt;p>These datasets were chosen because they allow for a review of individual game performance, for each team, throughout each season in the recent decade. Aggregate statistics such as points per game (ppg) can be deceptive because duration of the metric is such a large period of time. The large sample of 82 games can lead to a perception issue when reviewing the data. These datasets include more variables to help the team determine effects to player injury, such as minutes per game (mpg) to understand how strenuous the pre-injury performance or how fatigue may have played a factor in the injury. Understanding more of the variables such as fouls given or drawn can help determine if the player or other team seemed to be the primary aggressor before any injury.&lt;/p>
&lt;h3 id="31-data-transformations-and-calculations">3.1 Data Transformations and Calculations&lt;/h3>
&lt;p>Using the Kaggle package the datasets were downloaded direct from the website and unzipped to a directory accessible by the ‘project_dateEngineering.ipynb’ notebook. The 7 unzipped datasets are then loaded into the notebook as pandas data frames using the ‘.read_csv()’ function. The data engineering performed in the notebook includes removal of excess data and data type transformations across almost all the data frames loaded. This data transformation includes transforming the games details column ‘MIN’, meaning minutes played, from a timestamp format to a numerical format that could have calculations like summation or average performed on it. This was a crucial transformation since minutes played have a direct correlation to player fatigue, which can increase a player’s chance of injury.&lt;/p>
&lt;p>One of the more difficult tasks was transforming the Injury dataset into something that would provide more information through machine learning and analysis. The dataset is loaded as one data set where 2 columns ‘Relinquished’ and ‘Acquired’ defined if the row in questions was a player leaving the roster due to injury or returning from injury, respectively. In this case for each for one of those two columns contained a players name and the other was blank. Besides that the data frame contained information like the date, notes, and the team name. In order to appropriately understand each injury as whole the data frame needs to be transformed into one where each row contains the player, the start date of the injury, and the end date of the injury. In order to do this first the original Injury dataset was separated into rows marking the start of an injury and those marking the end of an injury. Data frames from the &lt;em>NBA games data&lt;/em> &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> data set were used to join TeamID and PlayerID columns to the Injury datasets. An ‘iterrows():’ loop was then used on the data frame marking the start of an injury to specifically locate the corresponding row in the Injury End data frame with the same PlayerID and where the return date was the closest date after the injury date. As this new data frame was being transformed, it was noted that sometimes a Player would have multiple rows with the same Injury ending date but different injury start dates, this can happen if an injury worsens or the player did not play due to last minute decision. In order to solve this the table was grouped by the PlayerID and InjuryEnd Date while keeping the oldest Injury Start date, since the model will want to see the full length of the injury. From there it was simple to calculate the difference in days for each row between the Injury start and end dates. This data frame is called ‘df_Injury_length’ in the notebook and is much easier to use for improved understanding of NBA injuries than the original format of the Injury data set.&lt;/p>
&lt;p>Once created, the ‘df_Injury_length’ data frame was copied and built upon. Using ‘iterrows():’ loop again to filter down the games details data frame rows with the same PlayerId, over 60 calculated columns are created to produce the ‘df_Injury_stats’ data frame. The data frame includes performance statistics specifically from the game the player was injured and the game the player returned from that injury. In addition to this aggregate performance metrics were calculated based on the 5 games prior to the injury and the 5 games post returning from injury. At this time the season of when the injury occurred and when the player returned is also stored in the dataframe. This will allow comparisons between the ‘df_Injury_stats’ data frame and the ‘df_Season_stats’ data frame which contains the players average performance metrics for entire seasons.&lt;/p>
&lt;p>A few interesting figures were generated within the Exploratory Data Analysis (EDA) stage. &lt;strong>Figure 1&lt;/strong> gave a view of the load of the player returning from injury. The load to the player will show how recovered the player is upon completion of rehab. Many teams decide to slowly work a returning player in. Additionally, the amount of time for an injury can be seen on this graph. The longer the injury, the more unlikely the player will return to action.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/avg_min_played_post5.png" alt="Average Minutes Played in First Five Games Upon Return over Injury Length in Days">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Average Minutes Played in First Five Games Upon Return over Injury Length in Days*&lt;/p>
&lt;p>&lt;strong>Figure 2&lt;/strong> shows the frequency in which a player is injured. The idea behind this graph is to see a relationship between the time leading up to the injury. Interesting enough, there is no key indication of where injury is more likely to occur. It can be assumed that there is a rarity of players who see playing time greater than 30 minutes. The histogram only shows a near flat relationship; which was surprising.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/frequencies_by_average_minutes.png" alt="Frequency of Injuries by Average Minutes Played in Prior Five Games">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Frequency of Injuries by Average Minutes Played in Prior Five Games*&lt;/p>
&lt;p>&lt;strong>Figure 3&lt;/strong> shows the length of injury over number of injuries. By reviewing this data, it can be seen that most injuries occur fewer rather than more often. A player that is deemed injury prone will be a lot more likely to be cut from the team. This data makes sense.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length.png" alt="Injury Length in Days over Number of Injuries">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Injury Length in Days over Number of Injuries&lt;/p>
&lt;p>&lt;strong>Figure 4&lt;/strong> shows the injury length over average minutes played in the five games before injury. This graph attempts to show all of the previous games and the impacts to the players injury. The data looks evenly distributed, but the majority of plaers do not play close to 40 minutes per game. By looking at this data, it shows that minutes played does likely contribute to the injury severity.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injury_length_over_avg_min.png" alt="Injury Length in Days over Avg Minutes Played in Prior 5 Games">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> Injury Length in Days over Avg Minutes Played in Prior 5 Games&lt;/p>
&lt;p>&lt;strong>Figure 5&lt;/strong> shows that in general the number of games played does not have a significant relationship to the length of the injury. There is a darker cluster between 500-1000 days injured that exists over the 40-82 games played, this could suggest that as more games are played there is likeliness for more severe injury.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_gamesplayed.png" alt="Injury Length in Days over Player Games Played that Season">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Injury Length in Days over Player Games Played that Season&lt;/p>
&lt;p>&lt;strong>Figures 6&lt;/strong>, &lt;strong>Figure 7&lt;/strong>, and &lt;strong>Figure 8&lt;/strong> attempt to demonstrate if any relationship exists visually between a player&amp;rsquo;s injury length and their age, weight, or height. For the most part &lt;strong>Figure 6&lt;/strong> shows most severe injuries occurring to younger players, which could make sense considering they can perform more difficult moves or have more stamina than older players. Some severe injuries still exist among the older players, this also makes sense considering their bodies have been under stress for many years and are more prone to injury. It should be noted that there are more players in the league that fall into the younger age bucket than the older ages. It is difficult to identify any pattern on &lt;strong>Figure 7&lt;/strong>. If anything the graph is somewhat normally shaped similar to the heights of players across the league. Suprisingly the injuries on &lt;strong>Figure 8&lt;/strong> are clustered a bit towards the left, being the lighter players. This could be explained through the fact that the lighter players are often more athletic and perform more strenuous moves than heavier players. It is also somewhat surprising since the argument that heavier players are putting more strain on their bodies could be used as a reason why heavier players would have worse injuries. One possible explanation could be the musculature adding more of the dense body mass could add protection to weakened joints. More investigation would be needed to identify an exact reason.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerage.png" alt="Injury Length in Days over Player Age that Season">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Injury Length in Days over Player Age that Season&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerHeight.png" alt="Injury Length in Days over Player Height in Inches">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Injury Length in Days over Player Height in Inches&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/injurylength_playerWeight.png" alt="Injury Length in Days over Player Weight in Kilograms">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> Injury Length in Days over Player Weight in Kilograms&lt;/p>
&lt;p>Finally, the team decided to use the z-score to normalize all of the data. By using the Z-score from the individual data in a column of df_Injury_stats, the team was able to limit variability of multiple metrics across the dataframe. A player&amp;rsquo;s blocks and steals should be a miniscule amount compared to minutes or points of some players. The same can be said of assists, technical fouls, or any other statistic in the course of an NBA game. The Z-score, by nature of the metric from the mean, allows for much less variability across the columns.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>The objective of this project was to develop performance indicators for injured players returning to basketball in the NBA. It is unreasonable to expect a player to return to the same level of play post injury immediately upon starting back up after recovery. It often takes a player months if not years to return to the same level of play as pre-injury, especially considering the severity of the injuries. In order to successfully analyze this information from the datasets, a predictive model will need to be created using a large set of the data to train.&lt;/p>
&lt;p>From this point, a test run was used to gauge the validity and accuracy of the model compared to some of the data set aside. The model created was able to provide feature importance to give a better understanding of which specific features are the most crucial when it comes to determining how bad the effects of an injury may or may not be on player performance. Feature engineering was performed prior to training the model in order to improve the chances of higher accuracy from the predictions. This model could be used to keep an eye out for how a player&amp;rsquo;s performance intensity and the engineered features could affect how long a player takes to recover from injury, if there are any warning signs prior to an injury, and even how well they perform when returning.&lt;/p>
&lt;h3 id="41-development-of-models">4.1 Development of Models&lt;/h3>
&lt;p>To help with review of the data, conditioned data was used to save resources on Google Colab. By conditioning the data and saving the files as a .CSV, the team was able to create a streamlined process. Additionally, the team found benefit by uploading these files to Google Drive to quickly import data near real time. After operating in this fashion for some time, the team was able to load the datasets into Github and utilize that feature. By loading the datasets up to Github, a url could be used to link the files directly to the files saved on Github without using a token like with Kaggle or Google Drive. The files saved were the following:&lt;/p>
&lt;p>&lt;strong>Table 1:&lt;/strong> Datasets Imported&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Dataframe&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Title&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">1.&lt;/td>
&lt;td style="text-align:center">df_Injury_stats&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">df_Injury_length&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">df_Season_stats&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">games&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">df_Games_gamesDetails&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">injuries_2010-2018&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">7.&lt;/td>
&lt;td style="text-align:center">players&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">8.&lt;/td>
&lt;td style="text-align:center">ranking&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">9.&lt;/td>
&lt;td style="text-align:center">teams&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Every time Google Colab loads data, it takes time and resources. The team was able to utilize the cross platform connectivity of the Google utilities. The team could then focus on building models as opposed to conditioning data every time the code was ran.&lt;/p>
&lt;h4 id="411-evaluation-metrics">4.1.1 Evaluation Metrics&lt;/h4>
&lt;p>The metrics chosen were designed to give results on Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and the Explained Variance (EV) Score. MAE is a measure of errors between paired observations experiencing the same expression. RMSE is the standard deviation of the prediction errors for our dataset. EV is the relationship between the train data and the test data. By using these metrics, the team is capable of reviewing the data in a statistical manner.&lt;/p>
&lt;h4 id="412-gradient-boost-regression">4.1.2 Gradient Boost Regression&lt;/h4>
&lt;p>The initial model that was used was a Gradient Boosting Regressor (GBR) model. This model produced the results shown in Table 2. The GBR model builds in a stage-wise fashion; similarly to other boosting methods. GBR also generalizes the data and attempts to optimize the results utilizing a loss function. An example of the algorithm can be seen in &lt;strong>Figure 5&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/gbr.png" alt="Gradient Boosting Regressor">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Gradient Boosting Regressor &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The team saw a relationship given the data. &lt;strong>Table 2&lt;/strong> shows the results of that model. The results were promising given the speed and utility of a GBR model. The team reviewed the data multiple times after multiple stages of conditioning the data.&lt;/p>
&lt;p>&lt;strong>Table 2:&lt;/strong> GBR Results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Category&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Value&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MAE Mean&lt;/td>
&lt;td style="text-align:center">-10.787&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MAE STD&lt;/td>
&lt;td style="text-align:center">0.687&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE Mean&lt;/td>
&lt;td style="text-align:center">-115.929&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE STD&lt;/td>
&lt;td style="text-align:center">96.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV Mean&lt;/td>
&lt;td style="text-align:center">1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV STD&lt;/td>
&lt;td style="text-align:center">0.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>After running a GBR model, the decision was made to try multiple models to see what gives the best results. The team settled on LightGBM and a Deep Learning model utilizing Keras built on the TensorFlow platform. These results will be seen in &lt;em>4.1.2&lt;/em> and &lt;em>4.1.3&lt;/em>.&lt;/p>
&lt;h4 id="412-lightgbm-regression">4.1.2 LightGBM Regression&lt;/h4>
&lt;p>Another algorithm chosen was a Light Gradient Boost Machine (LightGBM) model. LightGBM is known for its lightweight and resource sparse abilities. The model is built from decision tree algorithms and used for ranking, classification, and other machine learning tasks. By choosing LightGBM data scientists are able to analyze larger data a faster approach. LightGBM can often over fit a model if the data is too small, but fortunately for the purpose of this assignment the data available for NBA injuries and stats is extremely large. Availability of data allowed for smooth operation of the LightGBM model. Mandot explains the model really well in The Medium. Mandot said, &lt;em>&amp;ldquo;Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development&amp;rdquo;&lt;/em> &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. There are a lot of benefits available to this algorithm.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/lightGBM_regressor.png" alt="LightGBM Algorithm: Leafwise searching">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> LightGBM Algorithm: Leafwise searching &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>&lt;/p>
&lt;p>When running the model &lt;strong>Table 3&lt;/strong> was generated. This table uses the same metrics as the GBR Results Table (&lt;strong>Table 2&lt;/strong>). After reviewing the results, the GBR model still appeared to be a viable avenue. The Keras model will be evaluated next to see most optimal model to use for repeatable fresults.&lt;/p>
&lt;p>&lt;strong>Table 3:&lt;/strong> LightGBM Results&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Category&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Value&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">MAE Mean&lt;/td>
&lt;td style="text-align:center">-0.011&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">MAE STD&lt;/td>
&lt;td style="text-align:center">0.001&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE Mean&lt;/td>
&lt;td style="text-align:center">-0.128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">RMSE STD&lt;/td>
&lt;td style="text-align:center">0.046&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV Mean&lt;/td>
&lt;td style="text-align:center">0.982&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">EV STD&lt;/td>
&lt;td style="text-align:center">0.013&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="413-keras-deep-learning-models">4.1.3 Keras Deep Learning Models&lt;/h4>
&lt;p>The final model attempted was a Deep Learning model. A few runs of different layers and epochs were chosen. They can be seen in &lt;strong>Table 4&lt;/strong> (shown later). The model was sequentially ran through the test layers to refine the model. When this is done, each predecessor layer acts as an input to the next layer&amp;rsquo;s input for the model. The results can produce accurate results while using unsupervised learning. The visualization for this model can be seen in the following figure:&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-301/raw/main/project/images/simple_neural_network_vs_deep_learning.jpg" alt="Neural Network">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> Neural Network &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>&lt;/p>
&lt;p>When the team ran the Neural Networks, the data went through three layers. Each layer was built upon the previous similarly to the figure. This allowed for the team to capture information from the processing. &lt;strong>Table 4&lt;/strong> shows the results for the deep learning model.&lt;/p>
&lt;p>&lt;strong>Table 4:&lt;/strong> Epochs and Batch Sizes Chosen&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;strong>Number&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Regressor Epoch&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Regressor Batch Sizes&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>KFolds&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>Model Epochs&lt;/strong>&lt;/th>
&lt;th style="text-align:center">&lt;strong>R2&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">&lt;em>1.&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>25&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>25&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>10&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>10&lt;/em>&lt;/td>
&lt;td style="text-align:center">&lt;em>0.985&lt;/em>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">2.&lt;/td>
&lt;td style="text-align:center">40&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.894&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">3.&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.966&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">4.&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">0.707&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">5.&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">5&lt;/td>
&lt;td style="text-align:center">0.611&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">6.&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">25&lt;/td>
&lt;td style="text-align:center">10&lt;/td>
&lt;td style="text-align:center">20&lt;/td>
&lt;td style="text-align:center">0.982&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The team has decided that the results for the Deep Learning are the most desirable. This model would be the one that the team would recommend based on the results from the metrics available. The parameters the team recommends are italicized in &lt;em>Line 1&lt;/em> of &lt;strong>Table 4&lt;/strong>.&lt;/p>
&lt;h2 id="5-inference">5. Inference&lt;/h2>
&lt;p>With the data available, some conclusions can be made. Not all injuries are of the same severity. By treating an ACL tear in the same manner as a bruise, the team doctors would take terrible approaches to rehab. The severity of the injury is a part of the approach to therapy. This detail is nearly impossible to capture in the model.&lt;/p>
&lt;p>Another aspect to come to a conclusion is that not every player recovers in the same timetable as another. Genetics, diet, effort, and mental health can all harm or reinforce the efforts from the medical staff. These areas are hard to capture in the data and cannot be appropriately reviewed with this model.&lt;/p>
&lt;p>It is also difficult to indicate where a previous injury may have contributed to a current injury. The kinetic chain is a structure of the musculoskeletal system that moves the body using the muscles and bones. If one portion of the chain is compromised, the entire chain will need to be modified to continue movement. This modification can result in more injuries. The data cannot provide this information. It is important to remember these possible confounding variables when interpreting the results of the model.&lt;/p>
&lt;h2 id="6-conclusion">6. Conclusion&lt;/h2>
&lt;p>After reviewing the results, the team created a robust model to predict the performance of a player after an injury. The coefficient of determination for the deep learning model shows a strong relationship between the training and test sets. After conditioning the data, the results can be seen in &lt;strong>Table 2&lt;/strong>, &lt;strong>Table 3&lt;/strong>, and &lt;strong>Table 5&lt;/strong>. The team had an objective to find this correlation and build it to the point where injury and performance can be modeled. The team was able to accomplish this goal.&lt;/p>
&lt;p>Additionally, these results are consistent with the current scientific literature &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The biological community has been able to record these results for decades. By leveraging this effort, the scientific community could move to a more proactive approach as opposed to reactive with respect to injury controls. This data will also allow for proper contract negotiations to take place in the NBA, considering potential decisions to avoid injury may include less playing time. The negotiations are pivotal to ensuring that expectations are met in the future seasons; especially when injury occurs in the final year of a player&amp;rsquo;s contract. Teams with an improved understanding of how players can or will return from injury have an opportunity to make the best of scenarios where other teams may be hesitant to sign an injured player. These different opportunities for a team&amp;rsquo;s front office could be the difference between a championship ring and missing the playoffs entirely.&lt;/p>
&lt;h2 id="61-limitations">6.1 Limitations&lt;/h2>
&lt;p>With respect to the current work, the models could be continued to be refined. Currently the results are to the original intentions of the team, but improvements can be made. Feature Engineering is always an area where the models can improve. Some valuable features to be created in the future are the calculations for the player&amp;rsquo;s efficiency overall, as well as offensinve and defensive efficiencies in each game. The team would also like to develop a model to use the stats of a player in pre-injury and apply that to the post-injury set of metrics. Also, the team would like to move to where the same could be applied given the length of the injury to the player while considering the severity of the injury. Longer and more severe injury will lead to different future results than say a long not severe injury, or a short injury that was somewhat severe. The number of varaibles that could provide more valuable information to the model are endless.&lt;/p>
&lt;h2 id="7-acknowledgements">7. Acknowledgements&lt;/h2>
&lt;p>The authors would like to thank Dr. Gregor von Laszewski, Dr. Geoffrey Fox, and the associate instructors in the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course (offered in the Fall 2020 semester at Indiana University, Bloomington) for their continued assistance and suggestions with regard to exploring this idea and also for their aid with preparing the various drafts of this article. In addition to that the community of students from the &lt;em>FA20-BL-ENGR-E534-11530: Big Data Applications&lt;/em> course also deserve a thanks from the author for the support, continued engagement, and valuable discussions through Piazza.&lt;/p>
&lt;h3 id="71-work-breakdown">7.1 Work Breakdown&lt;/h3>
&lt;p>For the effort developed, the team split tasks between each other to cover more ground. The requirements for the investigation required a more extensive effort for the teams in the ENGR-E 534 class. To accomplish the requirements, the task was expanded by addressing multiple datasets within the semester and building in multiple models to display the results. The team members were responsible for committing in Github multiple times throughout the semester. The tasks were divided as follows:&lt;/p>
&lt;ol>
&lt;li>Chelsea Gorius
&lt;ul>
&lt;li>Exploratory Data Analysis&lt;/li>
&lt;li>Feature Engineering&lt;/li>
&lt;li>Keras Deep Learning Model&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Gavin Hemmerlein
&lt;ul>
&lt;li>Organization of Items&lt;/li>
&lt;li>Model Development&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Both
&lt;ul>
&lt;li>Report&lt;/li>
&lt;li>All Outstanding Items&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>A. Mehra, &lt;em>Sports Medicine Market worth $7.2 billion by 2025&lt;/em>, [online] Markets and Markets.
&lt;a href="https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp">https://www.marketsandmarkets.com/PressReleases/sports-medicine-devices.asp&lt;/a> [Accessed Oct. 15, 2020]. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>J. Harris, B. Erickson, B. Bach Jr, G. Abrams, G. Cvetanovich, B. Forsythe, F. McCormick, A. Gupta, B. Cole,
&lt;em>Return-to-Sport and Performance After Anterior Cruciate Ligament Reconstruction in National Basketball Association Players&lt;/em>, Sports Health. 2013 Nov;5(6):562-8. doi: 10.1177/1941738113495788. [Online serial]. Available: &lt;a href="https://pubmed.ncbi.nlm.nih.gov/24427434">https://pubmed.ncbi.nlm.nih.gov/24427434&lt;/a> [Accessed Oct. 24, 2020]. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>W. Kraemer, C. Denegar, and S. Flanagan, &lt;em>Recovery From Injury in Sport: Considerations in the Transition From Medical Care to Performance Care&lt;/em>, Sports Health.
2009 Sep; 1(5): 392–395.[Online serial]. Available: &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445177&lt;/a> [Accessed Oct. 24, 2020]. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>R. Hopkins, &lt;em>NBA Injuries from 2010-2020&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/ghopkins/nba-injuries-2010-2018">https://www.kaggle.com/ghopkins/nba-injuries-2010-2018&lt;/a> [Accessed Oct. 9, 2020]. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>N. Lauga, &lt;em>NBA games data&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv">https://www.kaggle.com/nathanlauga/nba-games?select=games_details.csv&lt;/a> [Accessed Oct. 9, 2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>J. Cirtautas, &lt;em>NBA Players&lt;/em>, [online] Kaggle. &lt;a href="https://www.kaggle.com/justinas/nba-players-data">https://www.kaggle.com/justinas/nba-players-data&lt;/a> [Accessed Oct. 9, 2020]. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>V. Aliyev, &lt;em>A hands-on explanation of Gradient Boosting Regression&lt;/em>, [online] Medium. &lt;a href="https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e">https://medium.com/@vagifaliyev/a-hands-on-explanation-of-gradient-boosting-regression-4cfe7cfdf9e&lt;/a> [Accessed Nov., 9 2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>P. Mandon, &lt;em>What is LightGBM, How to implement it? How to fine tune the parameters?&lt;/em>, [online] Medium. &lt;a href="https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc">https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc&lt;/a> [Accessed Nov., 9 2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>The Data Scientist, &lt;em>What deep learning is and isn’t&lt;/em>, [online] The Data Scientist. &lt;a href="https://thedatascientist.com/what-deep-learning-is-and-isnt">https://thedatascientist.com/what-deep-learning-is-and-isnt&lt;/a> [Accessed Nov., 9 2020]. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: NFL Regular Season Skilled Position Player Performance as a Predictor of Playoff Appearance Overtime</title><link>/report/fa20-523-308/project/project/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-308/project/project/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-308/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-308/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Project&lt;/p>
&lt;p>Travis Whitaker, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-308">fa20-523-308&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-308/blob/main/project/project.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>The present research investigates the value of in-game performance metrics for NFL skill position players (i.e., Quarterback, Wide Receiver, Tight End, Running Back and Full Back) in predicting post-season qualification. Utilizing nflscrapR-data that collects all regular season in-game performance metrics between 2009-2018, we are able to analyze the value of each of these in-game metrics by including them in a regression model that explores each variables strength in predicting post-season qualification. We also explore a comparative analysis between two time periods in the NFL (2009-2011 vs 2016-2018) to see if there is a shift in the critical metrics that predict post-season qualification for NFL teams. Theoretically, this could help inform the debate as to whether there has been a shift in the style of play in the NFL across the previous decade and where those changes may be taking place according to the data. Implications and future research are discussed.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-choice-of-data-sets">3. Choice of Data-sets&lt;/a>&lt;/li>
&lt;li>&lt;a href="#4-methodology">4. Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-results">5. Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#inference">Inference&lt;/a>&lt;/li>
&lt;li>&lt;a href="#preliminary-results">Preliminary Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#2009-2011-skill-position-player-performance-as-playoff-predictor">2009-2011 Skill Position Player Performance as Playoff Predictor&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2016-2018-skill-position-player-performance-as-playoff-predictor">2016-2018 Skill Position Player Performance as Playoff Predictor&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#comparative-results">Comparative Results&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#6-discussion">6. Discussion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#limitations-and-future-research">Limitations and Future Research&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-acknowledgements">8. Acknowledgements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#9-references">9. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> ANOVA, Comparative Analysis, Exploratory Analysis, Football, Sports, Metrics&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>In the modern NFL the biggest negotiating tools for players in signing a new contract is their on-field performance. Many players choose to &amp;ldquo;hold-out&amp;rdquo; of pre-season practice or regular season games as a negotiating tool in their attempt to sign a more lucrative contract. In this situation players feel as though their exceptional performance on the field is not reflected in the monetary compensation structure of their contract. This is most often reflected in skill position players such as wide receivers or running backs whose play on the field is most often celebrated (e.g., touchdowns) and discussed by fans of the game. While these positions are no doubt important to a team’s success, the question remains how important is one players contribution to a team’s overall success? The current project will attempt to evaluate the importance of skill position players' (i.e., Quarterback (QB), Wide Receiver (WR), Running Back (RB), and Tight End (TE)) performance during the regular season and use in-game performance metrics as a predictive factor for their team making the playoffs. This is an attempt to answer the question can qualifying for the post-season be predicted by skill position metrics measured during the regular season? If so, then which metrics are most crucial to predicting post-season qualification?&lt;/p>
&lt;p>A secondary analysis in this project will look at a comparison between 2009-2011 vs. 2016-2018 regular season metrics and build separate models for each three year span to investigate whether a shift in performance metric importance has occurred over the past decade. NFL insiders and journalists have noted the shift in play-calling over the past decade in the NFL as well as the change at the quarterback position to more &amp;ldquo;dual-threat&amp;rdquo; (running and throwing) quarterbacks that have transitioned the game to a more &amp;ldquo;aggressive&amp;rdquo; play-style &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Yet punditry and reporting have not always been supported by performance metrics and this specific claim of a transition over the past decade needs some exploring. Therefore, we will be investigating whether there has been a shift in the performance metrics that are important in predicting team success. Again, team success will be measured by making the post-season.&lt;/p>
&lt;h2 id="2-background-research-and-previous-work">2. Background Research and Previous Work&lt;/h2>
&lt;p>There are many playoff predictor models that focus on team performance or team wins vs losses as a predictor of making the playoffs &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. However, few take into consideration individual player performance as an indicator of their team making the post-season playoffs in the NFL. The most famous model that takes into consideration player performance is ELO rating &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The first ELO rating was a straightforward model that took head-to-head results and player-vs-team model to predict win probability in an NFL game. However, in 2019 Silver and his team at FiveThirtyEight updated their ELO model to give a value rating to the quarterback position for each team. This quarterback value included metrics such as pass attempts, completions, passing yards, passing touchdowns, interceptions, sacks, rush attempts, rushing yards, and rushing touchdowns. Taking these metrics along with the defensive quality metrics, which is an adjustment of quarterback value based on the opposing defense ranking, gives you an overall value for your quarterback. Thus, this widely accepted model takes head-to-head team comparisons on a week-to-week basis and includes the quarterback value in predicting the winners of these head-to-head matchups. However, no model has taken just player performance and tried to predict team success for an entire regular season based on each of their individual players. These previous models primarily look at offensive vs. defensive units and try to predict win/loss records based off each of these units.&lt;/p>
&lt;p>The goal of the present research is not to compare our model vs previous models, as these standing models are not meant for playoff prediction, rather these previous models are used for a game-by-game matchup comparison. The present research investigates whether looking at position players at each of the skilled positions, maps onto predicting the post-season qualifying teams. Further, how does this predictive model change over time? Looking at 2009-2011 NFL skill player performance vs 2016-2018 skill player performance we will investigate if there are differences in metrics that predict post-season appearance. This will show us whether there are shifts in skill position play that impact predicting post-season playoff appearance. Specifically, by comparing the two models which use two different periods of time (i.e., 2009-2011 vs 2016-2018) we will be able to better investigate specific metrics at each position that are important for predicting success. For example, if the wide receiver position is important, what is most important about that position, yards-after-catch or number of receptions? Further, how might the importance of those metrics shift over time? We hope to explore and understand better the impact of skill players and the metrics that they are measured on in terms of making the post-season.&lt;/p>
&lt;h2 id="3-choice-of-data-sets">3. Choice of Data-sets&lt;/h2>
&lt;p>The datasets used are in a github folder that holds nflscrapR-data that originates from NFL.com &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The folder includes play-by-play data, including performance measures, for all regular season games from 2009 to 2019. This file was paired with week-by-week regular season roster data for each team in the NFL. This allowed us to track skilled position player performance during the regular season and then compare this regular season file with the files that contain playoff teams for each year from 2009-2019. Supplemental data was pulled from Pro-Football-Reference.com &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="4-methodology">4. Methodology&lt;/h2>
&lt;p>The first step we took to understand the data was to use various slices of the data put into scatterplots and bar charts to find trends, as well as various time series charts. This was an exploratory step in understanding the data.&lt;/p>
&lt;p>Then each metric from player performance during the regular season was included in the analysis or models that were built to predict post-season appearance. Post-season appearance is a designation for a team qualifying for the post-season or playoffs. We think it is important to engineer some new features to potentially provide insights. For instance, it is possible to determine whether a play was during the final two minutes of a half and if a play was in the red zone. During these critical points of a game a win or lose is often determined. Our thought is by weighing these moments and performance metrics with more importance the model will better predict a team’s likelihood of making the playoffs. Another secondary metric that may strengthen the predictive ability of the model would be to use Football Outsider’s Success Rate, which is a determination of a play’s success rate for the offense that is on the field &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. This can also provide me with the down and distance to go for the offense and players that are on the field. We will also use college position designations as way to normalize the positions performance across teams. Many NFL teams utilize different player sets. Thus, it is important to use a standard, which college football uses across all teams. Since we are only interested in skill position players this will include Wide Receiver (WR), Running Back (RB), Full Back (FB), Quarterback (QB), and Tight End (TE). These designations will allow the model to compare player performance by position.&lt;/p>
&lt;p>After breaking down the data into key categorical variables to see if there was an impact for these performance variables in making the playoffs for the NFL teams. These individual position statistics were analyzed as a group and then separated into &amp;ldquo;Post Season&amp;rdquo; meaning the player&amp;rsquo;s team qualified for the playoffs, or &amp;ldquo;No Post&amp;rdquo; meaning the player&amp;rsquo;s team did not qualify for the playoffs. By doing this we were able to verify that a reasonable number of players fell into the &amp;ldquo;Post Season&amp;rdquo; group, as only 12 out of 32 teams qualify for the post-season. Further we were able to use these designations in the next step of modeling, where the data was analyzed in an ANOVA to see how important each metric was in predicting post-season appearance for each player.&lt;/p>
&lt;p>Metric measurement needs to be consistent across years. A comparison of year-to-year metrics was completed comparing each years measurements from 2009-2011 and 2016-2018 in order to make sure that the measurement techniques were stable and do not vary across time. If there were changes in the way metrics are measured than either that year will need to be dropped from the model or adjustments will need to be made to the metric to balance it with the other years included in the model. Luckily, there were no variation in metric measurement across years, so all measurements initially included in the model were kept.&lt;/p>
&lt;p>Finally, once all metrics were balanced and the team performance metrics had been aggregated. The ANOVA model was built to analyze metric measurement as a predictor of a player making post-season play. This ANOVA model was created twice, once for the 2009-2011 players and then again for the 2016-2018 players. Once this analysis was run, we were able to see the strength of the model in predicting playoff appearance by player, based on metric measurement.&lt;/p>
&lt;h2 id="5-results">5. Results&lt;/h2>
&lt;h3 id="inference">Inference&lt;/h3>
&lt;p>An individual player-performance model for NFL skill position players (i.e., quarterback, wide receiver, tight end, and running back) will perform better than chance level (50%) of identifying playoff teams from the NFL during the season&amp;rsquo;s of 2009-2011 and 2016-2018. Further, an exploratory analysis of time periods (2009-2011 vs 2016-2018) will expose differences in the key metrics that are used to predict playoff appearance over time. This will give us a better understanding of the shifts in performance in the NFL at each skill position.&lt;/p>
&lt;h3 id="preliminary-results">Preliminary Results&lt;/h3>
&lt;p>The descriptive statistics for the player performance revealed no issues with normality or different metric standards across seasons for player performance measurements. Figure 1 represents a count check for players qualifying for the post-season in the seasons of 2009-2011. Based on the NFL post-season structure 12 out of 32 teams qualify for the post-season, or 37.5%. Figure 1 shows that roughly 1/3 of players qualify for the post season at each position. However, it is important to note that each team structure and roster is different. For example, one team may carry 7 receivers, 2 running backs, 2, quarter backs, 2 tight ends, and 0 full backs, where another team may carry 4 receivers, 4 running backs, 3 quarter backs, 4 tight ends, and 1 full back. This is an important distinction to make because the &amp;ldquo;Post Season&amp;rdquo; players shown in figure 1 are not at an equal percentage across position.&lt;/p>
&lt;h4 id="2009-2011-skill-position-player-performance-as-playoff-predictor">2009-2011 Skill Position Player Performance as Playoff Predictor&lt;/h4>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Player_summary_2009_2011.png" alt="Player Qualifying for Post-Season">&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> Breakdown of Players by Skill Position That Qualified for Post-Season Play (2009-2011)&lt;/p>
&lt;p>We also wanted to investigate whether play-count at each position was balanced across post-season players and players who did not qualify for the post-season. Figure 2 shows that players on teams who did qualify for the post-season were involved in more plays at their position than players at their position who did not qualify for the playoffs. Thinking about this finding as a result of the regular season, players in skilled positions on post-season qualifying teams play on offenses that won more games than teams who did not qualify for the playoffs. While we did not look at time of possession for players by position, it seems fairly reasonable through logic and the results in figure 2 that play count is higher because these teams are more successful and the players on post-season qualifying teams are on the field more than teams with more losses who did not qualify for the post-season.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Play_count_position_2009.png" alt="Count of Play-type by Post-Season Qualification category">&lt;/p>
&lt;p>&lt;strong>Figure 2:&lt;/strong> Count of Play-type by Post-Season Qualification category (2009-2011)&lt;/p>
&lt;p>Figure 3 below is a reference table for the features included in the ANOVA regression model determining the key features that predict post-season qualification. These features are used for reference in figures 4 and 7.&lt;/p>
&lt;p>Using the player performance metrics for the regular season, an ANOVA was run to see if these metrics placed together would be a successful predictor of post-season qualification. Figure 4 shows the top 10 metrics that had the highest f-values for predicting post-season qualification, all of which were all significant (p&amp;lt;.05) in the model. Reviewing the model, the most significant factors for predicting post-season qualification for teams in order were; 1. Successful Reception (wide receiver or tight end), 2. Total Receiving Yards (Wide Receiver or Tight End), 3. Yards After Catch (wide receiver or tight end), 4. Total Receiving Touchdowns (Wide Receiver or Tight End), 5. Total Touchdowns (All positions), 6. Receiver Plays (Wide Receiver), 7. Redzone Plays (All positions), 8. Successful Plays (All positions), 9. Yards Gained (All positions), 10. Total Offensive Plays in the 3rd Quarter (All positions). The model accounted for 78.2% of variance. The model successfully accounted for predicting post-season qualifying teams in 78.2% of instances.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/figure_description_table.png" alt="Feature Descriptions">&lt;/p>
&lt;p>&lt;strong>Figure 3:&lt;/strong> Description for top features included in ANOVA regression model&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_pvalue_table_2009.png" alt="ANOVA for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 4:&lt;/strong> ANOVA Table for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2009-2011)&lt;/p>
&lt;h4 id="2016-2018-skill-position-player-performance-as-playoff-predictor">2016-2018 Skill Position Player Performance as Playoff Predictor&lt;/h4>
&lt;p>We paralleled our analysis from the 2009-2011 analysis above in completing the 2016-2018 analysis represented in Figures 5-7. Figure 5 represents the player count that qualified for post-season play (orange) and the non-post-season players (blue). Again, player count was compared to the roughly 37.5% rate that should be expected for 12 teams out of 32 qualifying for post-season play. However, the rates were a bit below the 37.5% rate. This can be explained by the number of injuries and roster changes that occur throughout the season. Teams shuffle in-and-out players at each position based on injury or performance. Teams will not have a static roster throughout the season, this includes post-season teams who cut players or put players on IR. These players, even though they played for post-season qualifying teams, would be in blue because they are not on the post-season rosters for the teams who qualify for post-season play. This along with the roster structure described for figure 1 explains the lower than 1/3 rate of players qualifying for post-season play.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Player_summary_2016-2018.png" alt="Player Qualifying for Post-Season">&lt;/p>
&lt;p>&lt;strong>Figure 5:&lt;/strong> Breakdown of Players by Skill Position That Qualified for Post-Season Play (2016-2018)&lt;/p>
&lt;p>Figure 6 investigates play-count at each position, like figure 2, but this time for the seasons of 2016-2018. Again, the analysis was balanced across post-season players and players who did not qualify for the post-season. Figure 6 shows that players on teams who did qualify for the post-season were involved in more plays at their position than players at their position who did not qualify for the playoffs. Figure 6 shows a consistent pattern with figure 2. Descriptive statistic comparison between the 2009-2011 seasons and the 2016-2018 seasons will be revisited in the discussion section.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Play_Count_Position_2016.png" alt="Count of Play-type by Post-Season Qualification category">&lt;/p>
&lt;p>&lt;strong>Figure 6:&lt;/strong> Count of Play-type by Post-Season Qualification category (20016-2018)&lt;/p>
&lt;p>Using the player performance metrics for the regular season, an ANOVA was run to see if these metrics placed together would be a successful predictor of post-season qualification. Figure 7 shows the top 10 metrics that had the highest f-values for predicting post-season qualification, all of which were significant (p&amp;lt;.05) in the model. The model successfully accounted for predicting post-season qualifying teams in 77.8% of instances.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_pvalue_table_2016.png" alt="ANOVA for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 7:&lt;/strong> ANOVA Table for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2016-2018)&lt;/p>
&lt;h3 id="comparative-results">Comparative Results&lt;/h3>
&lt;p>Comparing the 2016-2018 with the 2009-2011 season model, certain shifts have occurred from the 2009-2011 seasons model. Namely, yards-after-catch has become the strongest predictor of post-season qualification, flipping positions with successful reception in the 2009-2011 model. Another notable shift is the importance of number of plays run in the second quarter in the 2016-2018 model, overtaking number of plays run in the third quarter from the 2009-2011. The models themselves also shift in their strength of prediction. The 2009-2011 model shows stronger predictive capability (78.2% vs 77.8%), which is reflected in the f-values for the top 10 metrics of the model. The 2009-2011 model has four variables with f-values above 50 and one above 60. The 2016-2018 model only has one variable with an f-value above 50. These values represent the variance accounted for in the model by a variable. The higher the f-value the more variance accounted for in the model by that specific variable. Since the f-values were so high for many of the top 10 variables listed in each model, the p-values showed highly significant far exceeding the p=.05 level that was needed. The f-values were high because they accounted for so much of the variance in the model, meaning the predictive nature of the model was due in large part to many of the variables in the top 10. Another way to state this is that each of these top 10 variables were significantly better at predicting post-season qualification than would be expected due to chance.&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_sig_features_2009.png" alt="ANOVA Chart for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 8:&lt;/strong> ANOVA Chart for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2009-2011)&lt;/p>
&lt;p>&lt;img src="https://github.com/cybertraining-dsc/fa20-523-308/raw/main/project/images/Anova_sig_features_2016.png" alt="ANOVA Chart for Metric Importance in Model">&lt;/p>
&lt;p>&lt;strong>Figure 9:&lt;/strong> ANOVA Chart for Metrics Measured as Predictors for Teams Qualifying for Post-Season Play (2016-2018)&lt;/p>
&lt;p>Cloudmesh Comon &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup> is used to create the benchmark.&lt;/p>
&lt;h2 id="6-discussion">6. Discussion&lt;/h2>
&lt;p>The first inference of this project was investigating the possibility of using in-game performance metrics as a competent and better-than-chance predictor of selecting skill position players making the NFL post-season. Both the 2009-2011 and the 2016-2018 season models were able to predict player post-season qualification at 78.2% and 77.8% levels of success, both above chance level. This success highlights the critical nature of skill performance players and provides confidence to the modern metric model of NFL players as a useful and qualified tool to evaluate player performance as a measure of success. This also gives clout to the skill position players who believe their contributions on the field are deserving of top dollar compensation in the NFL. According to our models, wide receivers are deserving of high compensation as their game play impacts the likelihood of their team making the playoff more than running backs. However, it is hard to discriminate whether quarterback play is also key to the success of wide receivers. It could well be that these two positions work hand-in-hand.&lt;/p>
&lt;p>Investigating the second inference regarding changes in the predictive model across time. In comparing the descriptive statistic models (figs. 1, 2 vs. 5, 6). There are some noticeable, but not significant differences in the two-time ranges. First, there are more receivers in the 2016-2018 time range, which reflects the NFL’s shift towards a more pass prone league. Since there was not an increase in roster size between the two-time ranges, the increase in receivers lead to a decrease in the number of quarterbacks and fullbacks on a roster, but these additional receivers carried probably took the roster spots of non-skill positions players that are not accounted for here. Both models show the importance of pass plays, successful pass plays, receiving touchdowns and yards, yards after catch, and other passing variables that highlight the importance of wide receivers and tight ends. The NFL has shifted towards a more pass-friendly league &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, and the models built here highlight the reasons why that occurred. Receiver plays are significantly more important in predicting post-season qualification than any other skill position metrics. It is likely that the shift towards receivers and away from running backs has taken place over time. It is possible that we have pulled two time periods that are too close together to reflect the shift in NFL play, and if we had pulled data from the 1990’s or 1980’s (unfortunately this data is not available in the needed metrics) we would see more running back heavy metrics at the tops of our models and significant changes in the two time periods.&lt;/p>
&lt;h3 id="limitations-and-future-research">Limitations and Future Research&lt;/h3>
&lt;p>Metrics are not provided for non-skill position players who could be critical in predicting playoff qualification. For instance, if we could include offensive linemen metrics, we would have a stronger model that would be better able to predict post-season qualification. Further, the NFL data we had access to does not measure defensive player metrics that we believe are critical in being able to predict post-season qualification for NFL teams. Future work should look to include defensive player metrics into their model, as well as non-skill position players to improve on this model.&lt;/p>
&lt;p>Though we were able to build a model to predict player qualification for the post-season, future research can build on this model by making a composite of players on a team to then predict a team making the playoffs. The present study is a nice first step in understanding the capabilities of game performance for predicting player success, but NFL teams are equally interested in a team&amp;rsquo;s success, not just individual skill players. Therefore, future research can build on this project by incorporating defensive player metrics, non-skill position offensive metrics, and composites of players on one team to predict a team&amp;rsquo;s projected chances of making the playoffs.&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>Utilizing skill position performance metrics shows to be a successful predictor of player post-season qualification above chance level (50%). Further, there are slight shifts in which metrics are best at predicting post-season qualification between the 2009-2011 and 2016-2018 time periods. However, the key metrics that were significant in both models from the two time periods (2009-2011 and 2016-2018) did not change. Therefore, we cannot say definitively that there has been a shift in style of play from 2009 to 2018.&lt;/p>
&lt;h2 id="8-acknowledgements">8. Acknowledgements&lt;/h2>
&lt;p>Thank you to my friends and family who supported me through working on this project.&lt;/p>
&lt;h2 id="9-references">9. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Seifert, K. (2020, June 18). How pro football has changed in the past 10 years: 12 ways the NFL evolved this decade. Retrieved November 17, 2020, from &lt;a href="https://www.espn.com/nfl/story/_/id/28373283/how-pro-football-changed-10-years-12-ways-nfl-evolved-decade">https://www.espn.com/nfl/story/_/id/28373283/how-pro-football-changed-10-years-12-ways-nfl-evolved-decade&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Zita, C. (2020, September 16). Improving a Famous NFL Prediction Model. Retrieved November 02, 2020, from &lt;a href="https://towardsdatascience.com/improving-a-famous-nfl-prediction-model-1295a7022859">https://towardsdatascience.com/improving-a-famous-nfl-prediction-model-1295a7022859&lt;/a> &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Silver, N. (2018, September 05). How Our NFL Predictions Work. Retrieved November 02, 2020, from &lt;a href="https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/">https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>Ryurko. Ryurko/NflscrapR-Data. 2 Mar. 2020, &lt;a href="https://github.com/ryurko/nflscrapR-data">https://github.com/ryurko/nflscrapR-data&lt;/a>. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>Sports Reference, LLC. Pro Football Statistics and History. Retrieved October 09, 2020. &lt;a href="https://www.pro-football-reference.com/">https://www.pro-football-reference.com/&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>Gregor von Laszewski, Cloudmesh StopWatch and Benchmark from the Cloudmesh Common Library, &lt;a href="https://github.com/cloudmesh/cloudmesh-common">https://github.com/cloudmesh/cloudmesh-common&lt;/a> &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Report: Big Data in Sports Game Predictions and How It is Used in Sports Gambling</title><link>/report/fa20-523-331/report/report/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>/report/fa20-523-331/report/report/</guid><description>
&lt;p>&lt;a href="https://github.com/cybertraining-dsc/fa20-523-331/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-331/workflows/Check%20Report/badge.svg" alt="Check Report">&lt;/a>
&lt;a href="https://github.com/cybertraining-dsc/fa20-523-331/actions">&lt;img src="https://github.com/cybertraining-dsc/fa20-523-331/workflows/Status/badge.svg" alt="Status">&lt;/a>
Status: final, Type: Report&lt;/p>
&lt;p>Mansukh Kandhari, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-331/">fa20-523-331&lt;/a>, &lt;a href="https://github.com/cybertraining-dsc/fa20-523-331/blob/master/report/report.md">Edit&lt;/a>&lt;/p>
&lt;div class="pageinfo pageinfo-primary">
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Big data in sports is being used more and more as technology advances and this has a very big impact, especially when it comes to sports gambling. Sports gambling has been around for a while and it is gaining popularity with it being legalized in more places across the world. It is a very lucrative industry and the bookmakers use everything they can to make sure the overall odds are in their favor so they can reduce the risk of paying out to the betters and ensure a steady return. Sports statistics and data is more important than ever for bookmakers to come up with the odds they put out to the public. Odds are no longer just determined by expert analyzers for a specific sport. The compilation of odds uses a lot of historical data about team and player performance and looks at the most intricate details in order to ensure accuracy. Bookmakers spend a lot of money to employ the best statisticians and the best algorithms. There are also many companies that solely focus on sports data analysis, who often work with bookmakers around the world. On the other hand, big data for sports game analysis is also used by gamblers to gain a competitive edge. Many different algorithms have been created by researchers and gamblers to try to beat the bookmakers, some more successful than others. Oftentimes these not only involve examining sports data, but also analysing data from different bookmakers odds in order to determine the best bets to place. Overall, big data is very important in this field and this research paper aims to show the various techniques that are used by different stakeholders.&lt;/p>
&lt;p>Contents&lt;/p>
&lt;div class="toc">
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#2-background">2. Background&lt;/a>&lt;/li>
&lt;li>&lt;a href="#3-how-bookmakes-determine-odds">3. How bookmakes Determine odds&lt;/a>&lt;/li>
&lt;li>&lt;a href="#5-poisson-model">5. Poisson Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#6-algorithms-and-prediction-models">6. Algorithms and prediction models&lt;/a>&lt;/li>
&lt;li>&lt;a href="#7-conclusion">7. Conclusion&lt;/a>&lt;/li>
&lt;li>&lt;a href="#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>Keywords:&lt;/strong> sports, sportsbook, betting, gambling, data analysis, machine learning, punter(British word for gambler)&lt;/p>
&lt;h2 id="1-introduction">1. Introduction&lt;/h2>
&lt;p>Big Data in sports has been used for years by various stakeholders in this industry to do everything from predicting game outcomes to injury prevention. It is also becoming very prevalent in the area of sports gambling. Ever since the Supreme court decision in Murphy v. National Collegiate Athletic Association that overturned a ban on sports betting, the majority of states in the US have passed legislation to allow sports gambling &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. In 2019, the global sports betting market was valued at 85.047 US Dollars so this is an already very big industry that is expanding &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. There are various platforms that allow betting in this industry including tangible sports books, casinos, racetracks, and many online and mobile gambling apps. The interesting thing about big data in sports betting is that it is being used on both sides in this market. It is used by bookmakers to create game models and come up with different spreads and odds, but big data analysis is also being used by gamblers to gain a competetive advantage and place more accurate bets. Various prediction models using machine learning and big data analytics have been created and they can sometimes be very accurate. For example, Google correctly predicted 14 out of the 16 matches in the 2014 world cup and Microsoft did even better by correctly predicting 15 out of the 16 matches during that year &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. Many big companies have spent a lot of time gathering lots of data and creating prediction algorithms, inlcuding ESPN&amp;rsquo;s Football Power index that gives the probility of one team beating another, Analytics Powerhouse 538 that determines scores of games using their ELO method, and Accuscore which runs Mone Carlo Simulations on worldwide sporting events &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Bookmakers use all their possible tools and algorithms to put out the best odds that will give them a return. They often employ teams of statisticians that use the most advanced prediction models and information from data analysis companies to come up with their odds. If sports data analysis is vastly being used by people other than bookies and prediction models can often be very accurate, one might wonder how people haven&amp;rsquo;t made millions off sports betting and how bookmakers are still in business? This report analyzes how big data analytics are used by bookmakers to come up with the odds they put out for games while also examining how it is used on the gamblers side. It aims to analysize various prediction models created by sports betters, researchers, and AI companies, and see how they compare to the way big data is used by bookmakers. Besides giving an analysis of how big data is used in this field, it will show if betting guided by prediction models can give a consistent return.&lt;/p>
&lt;h2 id="2-background">2. Background&lt;/h2>
&lt;p>Many people with an interest in sports betting prediction have created models, some that involve machine learning and AI. A few of these projects have had some very interesting results using different types of data and analysis techniques. Jordan Bailey created an NBA prediction model based on the over under bets &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. For context, bookmakers will set a point total for a game and bettors can bet on whether the actual score will be over or under the point total set by the book. This type of betting is offered for many types of sports. Using NBA box scores for 5 previous seasons and data on historical betting lines created by various bookmakers, Bailey created a logistical regression model that would return a prediction on if the score was over or under a point total set by a bookmaker. Two models were created, one that predicted if a game would be over a set line and one that predicted if it would be under a set line. The datasets for these models were structured in a way where each specific game was represented as the box scores for the 3 previous games for each team, so 6 previous games &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. When creating the model, the first four seasons were used as the training set and the fifth season was used as the testing data to make predictions on &lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. In order to determine the significance of results, Bailey set up a &amp;ldquo;confidence threshold&amp;rdquo; of 62 percent for the probability his model returned on a game being over or under. The prediction was &amp;ldquo;confident&amp;rdquo; if the probability of the prediction was above the set threshold. When testing the models, the over model predicted 88 games confidently and 52 games correctly, with an accuracy of 59.09 percent. The under model predicted 96 games confidently and correctly predicted 52 games with an accuracy of 54.16 percent &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. To simulate how the model would perform on betting with 10,000 dollars, a bet was made every time the model predicted a confident bet for the 2018 NBA season. The accuracy on the bets were 52.52 percent and the total after the simulation was 11,880 dollars.&lt;/p>
&lt;h2 id="3-how-bookmakes-determine-odds">3. How bookmakes Determine odds&lt;/h2>
&lt;p>It is no secret that bookmakers use a lot of data and apply various statistical techniques to come up with betting odds. The statistical techniques used and the data that bookmakers look at vary from sport to sport, for example, a popular method for modeling soccer uses the Poisson distribution since it can be very accurate but also because it makes it easy to add time decay to the inputs &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Big data and data accuracy plays such a big part for bookmakers that many companies in the sports betting market have multi million dollar contracts with big leagues like the NFL &lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>. The NBA also recently extended their contracts with Sportradar and Genius Sports group that will have the rights to distribute official NBA data to licenced sports betting operators in the United States &lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup>. Companies like Sportradar collect and analyze official data and provide services to various bookmakers. The accuracy of data can be very important, a difference of something as little as one yard can make such a big difference; therefore, the industry values the accuracy of data that the leagues itself can provide. Bookmakers employ various mathematicians to analyze historical sports data to come up with odds; however, sports data isn’t the only thing that bookmakers look at when determining how they will make odds for a game. At the end of the day, the gambling industry is a numbers game that thrives on ensuring the probability is in the houses favor. Bookmakers use various techniques involving big data and factors such as public opinion to do so.&lt;/p>
&lt;p>Big data is being used more and more in various industries, and in the gambling industry, it isn&amp;rsquo;t just used to come up with odds. One major way it is used is by gathering data about user demographics &lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>. When using an online sportsbook, the casino can gather data about a users age, location, gender, excetera which can provide valuable insights that can be used for product development and marketing purposes. By using user demographic data to provide targeted advertising, casinos and bookmakers can attract more betters which will increase revenue. As said by former odds compiler Matthew Trenhaile, &amp;ldquo;Their [bookmakers] product is entertainment and not the selling of an intellectual contest between punter and bookmaker. It is foolish to think this has ever been the product that bookmakers have sold.They sell an adrenaline rush and anyone who thinks great characters pitting themselves against the punters and taking anyone on in horse racing betting rings is what betting used to be about is kidding himself or herself.&amp;rdquo; Due to the probability of making money in sports betting, and really every type of gambling, being in the houses favor, online casinos and sportsbooks use big data to increase the number of bets placed by customers; nevertheless, using models to come up with odds is the heart of this industry which makes it the most important way big data analytics is used by bookmakers.&lt;/p>
&lt;p>When odds are being made for a sports book, a lot of things are taken into consideration in the process. Bookmakers have teams of statisticians that analyze historical data of the teams in order to come up with game prediction models, often using machine learning based algorithms&lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. When bookmakers are actually making the odds, these statistical models created from large amounts of data aren&amp;rsquo;t the only thing they use. When bookmakers are creating odds, their goal isn&amp;rsquo;t to come up with an accurate game prediction, it is to have the lowest probability of paying out, so they will add a margin in order to statistically ensure a profit regardless of the outcome. Some times public opinion is used by bookmakers to sway their odds. For example, if a team has been on an unexpected winning streak, the bookmakers will often overestimate their odds, even against a team that will statistically do better than them, since people will be more inclined to take that bet resulting in the bookmaker reducing their probability of paying out &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Furthermore, bookmakers will often &amp;ldquo;hedge&amp;rdquo; bets to cover potential losses if an unexpected outcome occurs. For example, if a large amount of people are betting on a team regardless of the odds, the bookmakers will have a large payout if that outcome occurs, so they will start offering more favorable odds on the opposite outcome so they can bring in bets that would cover their losses &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. At the end of the day, when bookmakers set out their odds they will always make sure they are statistically in their favor. Even though bookmakers heavily analyze sports data in order to come up with prediction models, the odds put out don&amp;rsquo;t exactly reflect the true probability of a game outcome. Game prediction models is used to come up with the most probable event occurring but bookmakers add a margin that is skews the actual probability in order to statistically ensure a profit &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. An example of a coin toss can show how these margins work &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. If one were to bet on a coin toss, there is a 50 percent change of heads winning and a 50 percent change of tails winning so that means neither side is favored and the market of this bet is 100 percent. As a bookmaker is trying to ensure a profit, they will add a margin to the actual game winning probability in order to mitigate risk and ensure that the odds are in their favor. The margins that the bookmakers put on the actual probability is determined by many factors such as public opinion and perception of a team &lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup>. Gamblers are actually able to calculate the margins that the bookmakers put on a bet using a relatively simple formula. This formula varies for the type of sports the gambler is trying to calculate the odds for. In a two way market such as tennis or basketball, a person can figure out the bookmakers margin using the decimal odds places for both sides &lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup>. This formula is 100(1/decimal odds) + 1000(1/other decimal odds). The amount the market percentage is over 100 is the margin the bookmaker has on the better; therefore, the margin percentage the bookmaker has over the gambler can be determined by subtracting 100 from that formula.&lt;/p>
&lt;h2 id="5-poisson-model">5. Poisson Model&lt;/h2>
&lt;p>One of the most popular models for soccer game predictions is the Poisson distribution model. According to former odds compiler Matthew Trenhaile, the Poisson distribution model for soccer prediction can be very accurate and is very useful since it is easy to add time decay to the inputs &lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. Refinements can easily be made as a game progresses and goal input changes to easily re calculate the odds, which is useful for bookmakers. The Poisson distribution for soccer game predictions is not only used on a large scale by bookmakers to calculate odds, but it is also often used by even small time bettors to determine how they will bet. A Poisson model for soccer games can even be created on Excel for betters who want to place their bets more accurately. This process works by using historical data of how many goals a team scored and how many goals they let in and comparing it to a leagues average in order to determine the number of goals each team is likely to score in a game. It starts with calculating the average number of goals scored for home games and away games for the whole league and determining a team&amp;rsquo;s &amp;ldquo;attack strength&amp;rdquo; and &amp;ldquo;defense strength&amp;rdquo; &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>. The attack strength is a team&amp;rsquo;s average number of goals per game divided by the league average of goals per game. Similarly, a teams defense strength is determined by dividing a teams average number of goals conceded by the leagues average number of goals conceded. The goal expectancy for the home team is calculated by multiplying the team&amp;rsquo;s attack strength with the away team&amp;rsquo;s defense strength and the league&amp;rsquo;s average number of home goals. The goal expectancy for the away team is calculated by multiplying the away teams attack strength with the home teams defense strength and multiplying it by the leagues average number of away goals &lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>. With this information, one can determine the probability for the range of goal outcomes on both sides using a formula created by the French mathematician Simeon Denis Poisson. The Poisson distribution indicates the probability of a given number of events occurring over a fixed interval, so it can be used to determine the probability of the number of goals scored in a soccer game. The formula for this for soccer prediction is P(x events in interval) = (e-μ) (μx) / x! . This formula determines the probability of the number of goals being scored (x) using Euler’s number (e) and the goal expectancy (μ). With this formula, we can see the probability each team has for scoring a number of goals in a game. Usually the distribution is done for 0-5 goals to see the percentages of each team scoring on the goal interval. This can be used by bookmakers to determine odds and by gamblers to make well educated bets.&lt;/p>
&lt;h2 id="6-algorithms-and-prediction-models">6. Algorithms and prediction models&lt;/h2>
&lt;p>Gamblers often use various models, driven by big data, in order to help them place more accurate bets. Many different models have been created in the sports field that use factors such as historical sports data in order to come up with game prediction models. A lot of people who have come up with good prediction models will not share how they work and sometimes offer a subscription service where eager gamblers can pay to receive game picks. When it comes to making the best sports betting algorithms, it isn&amp;rsquo;t just about the amount of data a person can acquire. Creating algorithms that predict well takes understanding the sport and the meaning behind each type of data. In regards to creating sports prediction algorithms and the data that goes behind it, Micheal Beuoy, an actuary and popular sports data analyst said, &amp;ldquo;I think it takes discipline combined with a solid understanding of the sport you’re trying to analyze. If you don’t understand the context behind your numbers, no amount of advanced analysis and complicated algorithms is going to help you make sense of them. You need discipline because it is very easy to lock in on a particular theory and only pay attention to the data points that confirm that theory. A good practice is to always set aside a portion of your data before you start analysing. Once you’ve built what you think is your &amp;ldquo;best&amp;rdquo; model or theory, you can then test it against this alternative dataset.&amp;rdquo; Creating sports prediction algorithms requires a lot of different types of analysis and the methods that yield the best results are always changing.&lt;/p>
&lt;p>Creating good models requires understanding the sport well and using specific types of data in the algorithms. A creator of an NBA game prediction algorithm who runs a website called Fast Break Bets, which sells a game prediction service, primarily uses NBA game statistics known as efficiency metrics to come up with his model &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>. As the creator of the algorithm is profiting off eager gamblers, the exacts of how it works have not been released but the creator explains the type of data he uses to make his algorithm work. The NBA is a game of efficiency since there is a shot clock and possessions are changed very quickly, so the score total of games can greatly vary by how fast or slow paced a team is. The creator of this algorithm uses an offensive and defensive rating that measures how many points a team scores and allows per 100 possessions, since 100 possessions is close to the NBA average of possessions per game &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>. The algorithm also uses effective field goal percentage, turnover rate, and rebounding rate with offensive and defensive rating to optimize the efficiency metrics. Another major factor that this creator uses in the algorithm is the NBA season schedule and how often a team plays games in a time span &lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup>. This is due to the fact that players get fatigued playing games close to each other and coaches will therefore limit the amount of time some of the players will be on the court in order to give them a rest. This is important since player statistics can greatly vary from person to person on a team. Using this information of efficiency metrics, player performance, and the frequency of games played, the creator was able to create a prediction model that works well enough for people to pay for his game picks.&lt;/p>
&lt;p>In research done by Manuel Silvero, he studied 5 famous algorithms that used Neural Network and Machine learning and concluded that their accuracy varies from 50-70 percent, depending on the sport &lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup>. Purucker in 1996 was one of the first computational sports prediction model and used an Artificial Neural Network with backward propagation &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>. It was 61 percent accurate. In 2003, Khan expanded Puruckers work and was more acurate. Data on 208 matches was collected and the elements used were total yardage differential, rushing yardage differential, turnover differential, away team indicator and home team indicator &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup>. The first 192 matches of the season were used as the training data set for the model. When tested on the remaining games of the season, the models predicted at a 75 percent accuracy. This was compared to predictions created by 8 ESPN sportscasters for the same games and they only predicted 63 percent of those matches correctly.&lt;/p>
&lt;p>One of the most accurate models created, in terms of receiving a good return on a bet, was created by Lisandro Kaunitz of the University of Tokyo, and relied on data from odds that bookmakers put out rather than historical game data &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. When it comes to statistical models for sports, big data from historical sports games are often analyzed in order to come up with game predictions and to gain insight on things like team, player, and position performance. In the market of sports betting, these models are used to come up with odds and also by bettors to place bets. Gamblers have came up with different game prediction models in order to beat the books, mainly comprising of historical sports data while sometimes also using historical betting data &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Kaunitz created a model that mainly focused on analysing data of the odds created by bookmakers, rather than sports team data, to predict good bets to place. The basis of his model relied on a technique bookmakers use to reduce their payout risk, known as hedging. This concept and the way bookmakers use it to reduce their risk of payout is covered in section 3. Kaunitz betting model worked by gathering the odds for a game created by various bookmakers and determining the average odds available. Using statistical analysis of odds offered, Kaunitz was able to determine any outliers from the average odds for a game &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Using these outliers, Kaunitz could determine if a bet would favor them or not. After various simulations and models, Kaunitz&amp;rsquo;s and his team took their strategy into the real world, and their bets payed out 47.2 percent of the time. They received an 8.5 percent return and profited $957.50 in 265 bets &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>. Due to their impressive returns, bookmakers caught on and started to limit the amount that they could bet&lt;/p>
&lt;h2 id="7-conclusion">7. Conclusion&lt;/h2>
&lt;p>Overall, big data plays a very important role in the sports betting industry and it is used by various stakeholders. Bookmakers use it to come up with odds and gamblers use it for a competitive advantage. Although data analysis is very important on both ends, this research shows that it is very hard to receive a consistent return as a gambler. From 1989 to 2000 for NFl betting, the bookmakers favorite won 66.7 percent of the time and from 2001 and 2012, the bookmakers favorite won 66.9 percent of the time &lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup>. Even though technology has advanced and people use the most sophisticated algorithms to come up with prediction models, the bookmaker seems to have the advantage. This is due to the fact that bookmakers spend tons of money gathering the most accurate data and employ some of the best statisticians and sports analysing firms, but also due to the way they hedge bets and use public opinion to modify odds in order prevent potential losses. Bookmakers adjust for a margin when they are compiling their odds, because just like everything in the gambling industry, the probability is set up so that the house will always win in the long run. Gamblers have created various algorithms in order to make the most educated sports bet. These use historical team data but sometimes also use data from betting odds. Some of the best betting algorithms work by analyzing bookmakers' odds and determining where the odds are significantly different from the expected outcome of the game. As seen with Lisandro Kaunitz from the University of Tokyo, when bookmakers see that gamblers are beating the system they can start to limit a person&amp;rsquo;s bets. Overall, big data plays a huge role in the sports gambling industry. Even though what happens on the field or the court is often based on chance, there are significant trends that can be seen when statistically analyzing sports data. At the end of the day, big data plays a big role in this industry for bookmakers and gamblers alike.&lt;/p>
&lt;h2 id="8-references">8. References&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>INSIGHT: Sports Betting in States Races on a Year After SCOTUS Overturns Ban,&amp;quot; Bloomberg Law, 04-Jun-2019. [Online]. Available: &lt;a href="https://news.bloomberglaw.com/us-law-week/insight-sports-betting-in-states-races-on-a-year-after-scotus-overturns-ban">https://news.bloomberglaw.com/us-law-week/insight-sports-betting-in-states-races-on-a-year-after-scotus-overturns-ban&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Research and Markets, &amp;ldquo;Global Sports Betting Market Worth $85 Billion in 2019 - Industry Assessment and Forecasts Throughout 2020-2025,&amp;rdquo; GlobeNewswire News Room, 31-Aug-2020. [Online]. Available: &lt;a href="https://www.globenewswire.com/news-release/2020/08/31/2086041/0/en/Global-Sports-Betting-Market-Worth-85-Billion-in-2019-Industry-Assessment-and-Forecasts-Throughout-2020-2025.html">https://www.globenewswire.com/news-release/2020/08/31/2086041/0/en/Global-Sports-Betting-Market-Worth-85-Billion-in-2019-Industry-Assessment-and-Forecasts-Throughout-2020-2025.html&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>R. Delgado, &amp;ldquo;How Big Data is Changing the Gambling World: Articles: Chief Data Officer,&amp;rdquo; Articles | Chief Data Officer | Innovation Enterprise, 01-Sep-2016. [Online]. Available: &lt;a href="https://channels.theinnovationenterprise.com/articles/how-big-data-is-changing-the-gambling-world">https://channels.theinnovationenterprise.com/articles/how-big-data-is-changing-the-gambling-world&lt;/a>. [Accessed: 29-Nov-2020]. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>J. Zalcman, &amp;ldquo;HOW TO CREATE A SPORTS BETTING ALGORITHM,&amp;rdquo; Oddsfactory, 16-Nov-2020. [Online]. Available: &lt;a href="https://theoddsfactory.com/how-to-create-a-sports-betting-algorithm/">https://theoddsfactory.com/how-to-create-a-sports-betting-algorithm/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>J. Bailey, &amp;ldquo;Applying Data Science to Sports Betting,&amp;rdquo; Medium, 18-Sep-2018. [Online]. Available: &lt;a href="https://medium.com/@jxbailey23/applying-data-science-to-sports-betting-1856ac0b2cab">https://medium.com/@jxbailey23/applying-data-science-to-sports-betting-1856ac0b2cab&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6" role="doc-endnote">
&lt;p>J. Bailey, &amp;ldquo;Jordan-Bailey/DSI_Capstone_Project,&amp;rdquo; DSI_Capstone_Project, 14-Sep-2018. [Online]. Available: &lt;a href="https://github.com/Jordan-Bailey/DSI_Capstone_Project/blob/master/Technical_Report.md">https://github.com/Jordan-Bailey/DSI_Capstone_Project/blob/master/Technical_Report.md&lt;/a>. [Accessed: 01-Dec-2020]. &lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7" role="doc-endnote">
&lt;p>M. Trenhaile, &amp;ldquo;How Bookmakers Create their Odds, from a Former Odds Compiler,&amp;rdquo; Medium, 29-Jun-2017. [Online]. Available: &lt;a href="https://medium.com/@TrademateSports/how-bookmakers-create-their-odds-from-a-former-odds-compiler-5b36b4937439">https://medium.com/@TrademateSports/how-bookmakers-create-their-odds-from-a-former-odds-compiler-5b36b4937439&lt;/a>. [Accessed: Nov-2020]. &lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8" role="doc-endnote">
&lt;p>K. J. Brooks, &amp;ldquo;The new game in town for pro sports leagues: Selling stats,&amp;rdquo; CBS News, 09-Jan-2020. [Online]. Available: &lt;a href="https://www.cbsnews.com/news/nba-nfl-sports-nascar-leagues-selling-stats-to-gambling-companies/">https://www.cbsnews.com/news/nba-nfl-sports-nascar-leagues-selling-stats-to-gambling-companies/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9" role="doc-endnote">
&lt;p>C. Murphy, &amp;ldquo;NBA extends data partnerships with Sportradar and Genius Sports Group,&amp;rdquo; SBC Americas, 29-Oct-2020. [Online]. Available: &lt;a href="https://sbcamericas.com/2020/10/29/nba-extends-data-partnerships-with-sportradar-and-genius-sports-group/">https://sbcamericas.com/2020/10/29/nba-extends-data-partnerships-with-sportradar-and-genius-sports-group/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10" role="doc-endnote">
&lt;p>&amp;ldquo;How Big Data Analytics Are Transforming the Global Gambling Industry,&amp;rdquo; Analytics Insight, 17-Jan-2020. [Online]. Available: &lt;a href="https://www.analyticsinsight.net/how-big-data-analytics-are-transforming-the-global-gambling-industry/">https://www.analyticsinsight.net/how-big-data-analytics-are-transforming-the-global-gambling-industry/&lt;/a>. [Accessed: Oct-2020]. &lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11" role="doc-endnote">
&lt;p>arXiv, &amp;ldquo;The Secret Betting Strategy That Beats Online Bookmakers,&amp;rdquo; MIT Technology Review, 19-Oct-2017. [Online]. Available: &lt;a href="https://www.technologyreview.com/2017/10/19/67760/the-secret-betting-strategy-that-beats-online-bookmakers/">https://www.technologyreview.com/2017/10/19/67760/the-secret-betting-strategy-that-beats-online-bookmakers/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12" role="doc-endnote">
&lt;p>A. Dörr, &amp;ldquo;How to apply predictive analytics to Premiership football to beat the bookies,&amp;rdquo; Dataconomy, 19-Mar-2019. [Online]. Available: &lt;a href="https://dataconomy.com/2019/03/how-to-apply-predictive-analytics-to-premiership-football-to-beat-the-bookies%EF%BB%BF/">https://dataconomy.com/2019/03/how-to-apply-predictive-analytics-to-premiership-football-to-beat-the-bookies%EF%BB%BF/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13" role="doc-endnote">
&lt;p>S. Hubbard, &amp;ldquo;Betting Margins Explained: How to Calculate Sports Margins,&amp;rdquo; BettingLounge, 24-Sep-2020. [Online]. Available: &lt;a href="https://bettinglounge.co.uk/guides/sports-betting-explained/betting-margins/">https://bettinglounge.co.uk/guides/sports-betting-explained/betting-margins/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14" role="doc-endnote">
&lt;p>&amp;ldquo;Sportsbook Profit Margins,&amp;rdquo; Sports Insights, 18-Sep-2015. [Online]. Available: &lt;a href="https://www.sportsinsights.com/betting-tools/sportsbook-profit-margins/">https://www.sportsinsights.com/betting-tools/sportsbook-profit-margins/&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15" role="doc-endnote">
&lt;p>B. Cronin, &amp;ldquo;Poisson Distribution: Predict the score in soccer betting,&amp;rdquo; Pinnacle, 27-Apr-2017. [Online]. Available: &lt;a href="https://www.pinnacle.com/en/betting-articles/Soccer/how-to-calculate-poisson-distribution/MD62MLXUMKMXZ6A8">https://www.pinnacle.com/en/betting-articles/Soccer/how-to-calculate-poisson-distribution/MD62MLXUMKMXZ6A8&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16" role="doc-endnote">
&lt;p>Stephen, &amp;ldquo;NBA Betting Model Explained: Sports Betting Picks, Tips, and Blog,&amp;rdquo; FAST BREAK BETS, 11-Nov-2017. [Online]. Available: &lt;a href="https://www.fastbreakbets.com/nba-picks/nba-betting-model-explained/">https://www.fastbreakbets.com/nba-picks/nba-betting-model-explained/&lt;/a>. [Accessed: 05-Dec-2020]. &lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17" role="doc-endnote">
&lt;p>M. Silverio, &amp;ldquo;My findings on using machine learning for sports betting: Do bookmakers always win?,&amp;rdquo; Medium, 26-Aug-2020. [Online]. Available: &lt;a href="https://towardsdatascience.com/my-findings-on-using-machine-learning-for-sports-betting-do-bookmakers-always-win-6bc8684baa8c">https://towardsdatascience.com/my-findings-on-using-machine-learning-for-sports-betting-do-bookmakers-always-win-6bc8684baa8c&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18" role="doc-endnote">
&lt;p>R. P. Bunker and F. Thabtah, &amp;ldquo;A machine learning framework for sport result prediction,&amp;rdquo; Applied Computing and Informatics, 19-Sep-2017. [Online]. Available: &lt;a href="https://www.sciencedirect.com/science/article/pii/S2210832717301485">https://www.sciencedirect.com/science/article/pii/S2210832717301485&lt;/a>. [Accessed: 2020]. &lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19" role="doc-endnote">
&lt;p>M. Beouy, &amp;ldquo;BGO - The Casino of the Future,&amp;rdquo; bgo Online Casino. [Online]. Available: &amp;lt;https://www.bgo.com/casino-of-the-future/the-future-of-sports-betting/. [Accessed: 05-Dec-2020]&amp;gt;. &lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item></channel></rss>